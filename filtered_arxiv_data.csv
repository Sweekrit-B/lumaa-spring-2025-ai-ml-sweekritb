,terms,titles,abstracts
0,['cs.LG'],Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities,"Graph neural networks (GNNs) have been widely used to learn vector
representation of graph-structured data and achieved better task performance
than conventional methods. The foundation of GNNs is the message passing
procedure, which propagates the information in a node to its neighbors. Since
this procedure proceeds one step per layer, the range of the information
propagation among nodes is small in the lower layers, and it expands toward the
higher layers. Therefore, a GNN model has to be deep enough to capture global
structural information in a graph. On the other hand, it is known that deep GNN
models suffer from performance degradation because they lose nodes' local
information, which would be essential for good model performance, through many
message passing steps. In this study, we propose multi-level attention pooling
(MLAP) for graph-level classification tasks, which can adapt to both local and
global structural information in a graph. It has an attention pooling layer for
each message passing step and computes the final graph representation by
unifying the layer-wise graph representations. The MLAP architecture allows
models to utilize the structural information of graphs with multiple levels of
localities because it preserves layer-wise information before losing them due
to oversmoothing. Results of our experiments show that the MLAP architecture
improves the graph classification performance compared to the baseline
architectures. In addition, analyses on the layer-wise graph representations
suggest that aggregating information from multiple levels of localities indeed
has the potential to improve the discriminability of learned graph
representations."
1,"['cs.LG', 'cs.AI']",Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes,"Deep networks and decision forests (such as random forests and gradient
boosted trees) are the leading machine learning methods for structured and
tabular data, respectively. Many papers have empirically compared large numbers
of classifiers on one or two different domains (e.g., on 100 different tabular
data settings). However, a careful conceptual and empirical comparison of these
two strategies using the most contemporary best practices has yet to be
performed. Conceptually, we illustrate that both can be profitably viewed as
""partition and vote"" schemes. Specifically, the representation space that they
both learn is a partitioning of feature space into a union of convex polytopes.
For inference, each decides on the basis of votes from the activated nodes.
This formulation allows for a unified basic understanding of the relationship
between these methods. Empirically, we compare these two strategies on hundreds
of tabular data settings, as well as several vision and auditory settings. Our
focus is on datasets with at most 10,000 samples, which represent a large
fraction of scientific and biomedical datasets. In general, we found forests to
excel at tabular and structured data (vision and audition) with small sample
sizes, whereas deep nets performed better on structured data with larger sample
sizes. This suggests that further gains in both scenarios may be realized via
further combining aspects of forests and networks. We will continue revising
this technical report in the coming months with updated results."
2,"['cs.LG', 'cs.CR', 'stat.ML']",Power up! Robust Graph Convolutional Network via Graph Powering,"Graph convolutional networks (GCNs) are powerful tools for graph-structured
data. However, they have been recently shown to be vulnerable to topological
attacks. To enhance adversarial robustness, we go beyond spectral graph theory
to robust graph theory. By challenging the classical graph Laplacian, we
propose a new convolution operator that is provably robust in the spectral
domain and is incorporated in the GCN architecture to improve expressivity and
interpretability. By extending the original graph to a sequence of graphs, we
also propose a robust training paradigm that encourages transferability across
graphs that span a range of spatial and spectral characteristics. The proposed
approaches are demonstrated in extensive experiments to simultaneously improve
performance in both benign and adversarial situations."
3,"['cs.LG', 'cs.CR']",Releasing Graph Neural Networks with Differential Privacy Guarantees,"With the increasing popularity of Graph Neural Networks (GNNs) in several
sensitive applications like healthcare and medicine, concerns have been raised
over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to
privacy attacks, such as membership inference attacks, even if only blackbox
access to the trained model is granted. To build defenses, differential privacy
has emerged as a mechanism to disguise the sensitive data in training datasets.
Following the strategy of Private Aggregation of Teacher Ensembles (PATE),
recent methods leverage a large ensemble of teacher models. These teachers are
trained on disjoint subsets of private data and are employed to transfer
knowledge to a student model, which is then released with privacy guarantees.
However, splitting graph data into many disjoint training sets may destroy the
structural information and adversely affect accuracy. We propose a new
graph-specific scheme of releasing a student GNN, which avoids splitting
private training data altogether. The student GNN is trained using public data,
partly labeled privately using the teacher GNN models trained exclusively for
each query node. We theoretically analyze our approach in the R\`{e}nyi
differential privacy framework and provide privacy guarantees. Besides, we show
the solid experimental performance of our method compared to several baselines,
including the PATE baseline adapted for graph-structured data. Our anonymized
code is available."
4,['cs.LG'],Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification,"Machine learning solutions for pattern classification problems are nowadays
widely deployed in society and industry. However, the lack of transparency and
accountability of most accurate models often hinders their safe use. Thus,
there is a clear need for developing explainable artificial intelligence
mechanisms. There exist model-agnostic methods that summarize feature
contributions, but their interpretability is limited to predictions made by
black-box models. An open challenge is to develop models that have intrinsic
interpretability and produce their own explanations, even for classes of models
that are traditionally considered black boxes like (recurrent) neural networks.
In this paper, we propose a Long-Term Cognitive Network for interpretable
pattern classification of structured data. Our method brings its own mechanism
for providing explanations by quantifying the relevance of each feature in the
decision process. For supporting the interpretability without affecting the
performance, the model incorporates more flexibility through a quasi-nonlinear
reasoning rule that allows controlling nonlinearity. Besides, we propose a
recurrence-aware decision model that evades the issues posed by unique fixed
points while introducing a deterministic learning method to compute the tunable
parameters. The simulations show that our interpretable model obtains
competitive results when compared to the state-of-the-art white and black-box
models."
5,"['cs.LG', 'stat.ML']",Lifelong Graph Learning,"Graph neural networks (GNNs) are powerful models for many graph-structured
tasks. Existing models often assume that a complete structure of a graph is
available during training, however, in practice, graph-structured data is
usually formed in a streaming fashion, so that learning a graph continuously is
often necessary. In this paper, we aim to bridge GNN to lifelong learning by
converting a graph problem to a regular learning problem, so that GNN is able
to inherit the lifelong learning techniques developed for convolutional neural
networks (CNNs). To this end, we propose a new graph topology based on feature
cross-correlation, called the feature graph. It takes features as new nodes and
turns nodes into independent graphs. This successfully converts the original
problem of node classification to graph classification, in which the increasing
nodes are turned into independent training samples. In the experiments, we
demonstrate the efficiency and effectiveness of feature graph networks (FGN) by
continuously learning a sequence of classical graph datasets. We also show that
FGN achieves superior performance in human action recognition with distributed
streaming signals for wearable devices."
6,['cs.LG'],Bayesian graph convolutional neural networks via tempered MCMC,"Deep learning models, such as convolutional neural networks, have long been
applied to image and multi-media tasks, particularly those with structured
data. More recently, there has been more attention to unstructured data that
can be represented via graphs. These types of data are often found in health
and medicine, social networks, and research data repositories. Graph
convolutional neural networks have recently gained attention in the field of
deep learning that takes advantage of graph-based data representation with
automatic feature extraction via convolutions. Given the popularity of these
methods in a wide range of applications, robust uncertainty quantification is
vital. This remains a challenge for large models and unstructured datasets.
Bayesian inference provides a principled approach to uncertainty quantification
of model parameters for deep learning models. Although Bayesian inference has
been used extensively elsewhere, its application to deep learning remains
limited due to the computational requirements of the Markov Chain Monte Carlo
(MCMC) methods. Recent advances in parallel computing and advanced proposal
schemes in MCMC sampling methods has opened the path for Bayesian deep
learning. In this paper, we present Bayesian graph convolutional neural
networks that employ tempered MCMC sampling with Langevin-gradient proposal
distribution implemented via parallel computing. Our results show that the
proposed method can provide accuracy similar to advanced optimisers while
providing uncertainty quantification for key benchmark problems."
7,"['cs.LG', 'stat.ML']",Understanding and Resolving Performance Degradation in Graph Convolutional Networks,"A Graph Convolutional Network (GCN) stacks several layers and in each layer
performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN)
for learning node representations over graph-structured data. Though powerful,
GCNs tend to suffer performance drop when the model gets deep. Previous works
focus on PROPs to study and mitigate this issue, but the role of TRANs is
barely investigated. In this work, we study performance degradation of GCNs by
experimentally examining how stacking only TRANs or PROPs works. We find that
TRANs contribute significantly, or even more than PROPs, to declining
performance, and moreover that they tend to amplify node-wise feature variance
in GCNs, causing variance inflammation that we identify as a key factor for
causing performance drop. Motivated by such observations, we propose a
variance-controlling technique termed Node Normalization (NodeNorm), which
scales each node's features using its own standard deviation. Experimental
results validate the effectiveness of NodeNorm on addressing performance
degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow
ones in cases where deep models are needed, and to achieve comparable results
with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and
can well generalize to other GNN architectures. Code is publicly available at
https://github.com/miafei/NodeNorm."
8,['cs.LG'],Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs,"Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty of
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. It allows us to use global
self-attention, the key element of transformers, directly for graphs and comes
with the benefit of long-range interaction among nodes. Moreover, the edge
channels allow the structural information to evolve from layer to layer, and
prediction tasks on edges/links can be performed directly from the output
embeddings of these channels. In addition, we introduce a generalized
positional encoding scheme for graphs based on Singular Value Decomposition
which can improve the performance of EGT. Our framework, which relies on global
node feature aggregation, achieves better performance compared to
Convolutional/Message-Passing Graph Neural Networks, which rely on local
feature aggregation within a neighborhood. We verify the performance of EGT in
a supervised learning setting on a wide range of experiments on benchmark
datasets. Our findings indicate that convolutional aggregation is not an
essential inductive bias for graphs and global self-attention can serve as a
flexible and adaptive alternative."
9,"['cs.LG', 'cs.AI']",Variational Graph Normalized Auto-Encoders,"Link prediction is one of the key problems for graph-structured data. With
the advancement of graph neural networks, graph autoencoders (GAEs) and
variational graph autoencoders (VGAEs) have been proposed to learn graph
embeddings in an unsupervised way. It has been shown that these methods are
effective for link prediction tasks. However, they do not work well in link
predictions when a node whose degree is zero (i.g., isolated node) is involved.
We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero
regardless of their content features. In this paper, we propose a novel
Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization
to derive better embeddings for isolated nodes. We show that our VGNAEs
outperform the existing state-of-the-art models for link prediction tasks. The
code is available at https://github.com/SeongJinAhn/VGNAE."
10,['cs.LG'],Local Augmentation for Graph Neural Networks,"Data augmentation has been widely used in image data and linguistic data but
remains under-explored on graph-structured data. Existing methods focus on
augmenting the graph data from a global perspective and largely fall into two
genres: structural manipulation and adversarial training with feature noise
injection. However, the structural manipulation approach suffers information
loss issues while the adversarial training approach may downgrade the feature
quality by injecting noise. In this work, we introduce the local augmentation,
which enhances node features by its local subgraph structures. Specifically, we
model the data argumentation as a feature generation process. Given the central
node's feature, our local augmentation approach learns the conditional
distribution of its neighbors' features and generates the neighbors' optimal
feature to boost the performance of downstream tasks. Based on the local
augmentation, we further design a novel framework: LA-GNN, which can apply to
any GNN models in a plug-and-play manner. Extensive experiments and analyses
show that local augmentation consistently yields performance improvement for
various GNN architectures across a diverse set of benchmarks. Code is available
at https://github.com/Soughing0823/LAGNN."
11,"['cs.LG', 'cs.AI', 'cs.DC']",FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks,"Graph Neural Network (GNN) research is rapidly growing thanks to the capacity
of GNNs in learning distributed representations from graph-structured data.
However, centralizing a massive amount of real-world graph data for GNN
training is prohibitive due to privacy concerns, regulation restrictions, and
commercial competitions. Federated learning (FL), a trending distributed
learning paradigm, provides possibilities to solve this challenge while
preserving data privacy. Despite recent advances in vision and language
domains, there is no suitable platform for the FL of GNNs. To this end, we
introduce FedGraphNN, an open FL benchmark system that can facilitate research
on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and
contains a wide range of datasets from different domains, popular GNN models,
and FL algorithms, with secure and efficient system support. Particularly for
the datasets, we collect, preprocess, and partition 36 datasets from 7 domains,
including both publicly available ones and specifically obtained ones such as
hERG and Tencent. Our empirical analysis showcases the utility of our benchmark
system, while exposing significant challenges in graph FL: federated GNNs
perform worse in most datasets with a non-IID split than centralized GNNs; the
GNN model that attains the best result in the centralized setting may not
maintain its advantage in the FL setting. These results imply that more
research efforts are needed to unravel the mystery behind federated GNNs.
Moreover, our system performance analysis demonstrates that the FedGraphNN
system is computationally efficient and secure to large-scale graphs datasets.
We maintain the source code at https://github.com/FedML-AI/FedGraphNN."
12,['cs.LG'],Diff-ResNets for Few-shot Learning -- an ODE Perspective,"Interpreting deep neural networks from the ordinary differential equations
(ODEs) perspective has inspired many efficient and robust network
architectures. However, existing ODE based approaches ignore the relationship
among data points, which is a critical component in many problems including
few-shot learning and semi-supervised learning. In this paper, inspired by the
diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to
strengthen the interactions among data points. Under the structured data
assumption, it is proved that the diffusion mechanism can decrease the
distance-diameter ratio that improves the separability of inter-class points
and reduces the distance among local intra-class points. This property can be
easily adopted by the residual networks for constructing the separable
hyperplanes. The synthetic binary classification experiments demonstrate the
effectiveness of the proposed diffusion mechanism. Moreover, extensive
experiments of few-shot image classification and semi-supervised graph node
classification in various datasets validate the advantages of the proposed
Diff-ResNet over existing few-shot learning methods."
13,"['cs.LG', 'cs.IT', 'math.IT', 'physics.data-an']",Scale-invariant representation of machine learning,"The success of machine learning stems from its structured data
representation. Similar data have close representation as compressed codes for
classification or emerged labels for clustering. We observe that the frequency
of the internal representation follows power laws in both supervised and
unsupervised learning. The scale-invariant distribution implies that machine
learning largely compresses frequent typical data, and at the same time,
differentiates many atypical data as outliers. In this study, we derive how the
power laws can naturally arise in machine learning. In terms of information
theory, the scale-invariant representation corresponds to a maximally uncertain
data grouping among possible representations that guarantee pre-specified
learning accuracy."
14,['cs.LG'],Generation of Synthetic Electronic Health Records Using a Federated GAN,"Sensitive medical data is often subject to strict usage constraints. In this
paper, we trained a generative adversarial network (GAN) on real-world
electronic health records (EHR). It was then used to create a data-set of
""fake"" patients through synthetic data generation (SDG) to circumvent usage
constraints. This real-world data was tabular, binary, intensive care unit
(ICU) patient diagnosis data. The entire data-set was split into separate data
silos to mimic real-world scenarios where multiple ICU units across different
hospitals may have similarly structured data-sets within their own
organisations but do not have access to each other's data-sets. We implemented
federated learning (FL) to train separate GANs locally at each organisation,
using their unique data silo and then combining the GANs into a single central
GAN, without any siloed data ever being exposed. This global, central GAN was
then used to generate the synthetic patients data-set. We performed an
evaluation of these synthetic patients with statistical measures and through a
structured review by a group of medical professionals. It was shown that there
was no significant reduction in the quality of the synthetic EHR when we moved
between training a single central model and training on separate data silos
with individual models before combining them into a central model. This was
true for both the statistical evaluation (Root Mean Square Error (RMSE) of
0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also
for the medical professionals' evaluation (no quality difference between EHR
generated from a single source and EHR generated from multiple sources)."
15,"['cs.LG', 'cs.AI', 'cs.SI']",Sparsifying the Update Step in Graph Neural Networks,"Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural
Network (GNN) framework, celebrate much success in the analysis of
graph-structured data. Concurrently, the sparsification of Neural Network
models attracts a great amount of academic and industrial interest. In this
paper, we conduct a structured study of the effect of sparsification on the
trainable part of MPNNs known as the Update step. To this end, we design a
series of models to successively sparsify the linear transform in the Update
step. Specifically, we propose the ExpanderGNN model with a tuneable
sparsification rate and the Activation-Only GNN, which has no linear transform
in the Update step. In agreement with a growing trend in the literature, the
sparsification paradigm is changed by initialising sparse neural network
architectures rather than expensively sparsifying already trained
architectures. Our novel benchmark models enable a better understanding of the
influence of the Update step on model performance and outperform existing
simplified benchmark models such as the Simple Graph Convolution. The
ExpanderGNNs, and in some cases the Activation-Only models, achieve performance
on par with their vanilla counterparts on several downstream tasks while
containing significantly fewer trainable parameters. In experiments with
matching parameter numbers, our benchmark models outperform the
state-of-the-art GNN models. Our code is publicly available at:
https://github.com/ChangminWu/ExpanderGNN."
16,"['cs.LG', 'cs.AI', 'cs.DS']",Computing Graph Descriptors on Edge Streams,"Graph feature extraction is a fundamental task in graphs analytics. Using
feature vectors (graph descriptors) in tandem with data mining algorithms that
operate on Euclidean data, one can solve problems such as classification,
clustering, and anomaly detection on graph-structured data. This idea has
proved fruitful in the past, with spectral-based graph descriptors providing
state-of-the-art classification accuracy on benchmark datasets. However, these
algorithms do not scale to large graphs since: 1) they require storing the
entire graph in memory, and 2) the end-user has no control over the algorithm's
runtime. In this paper, we present single-pass streaming algorithms to
approximate structural features of graphs (counts of subgraphs of order $k \geq
4$). Operating on edge streams allows us to avoid keeping the entire graph in
memory, and controlling the sample size enables us to control the time taken by
the algorithm. We demonstrate the efficacy of our descriptors by analyzing the
approximation error, classification accuracy, and scalability to massive
graphs. Our experiments showcase the effect of the sample size on approximation
error and predictive accuracy. The proposed descriptors are applicable on
graphs with millions of edges within minutes and outperform the
state-of-the-art descriptors in classification accuracy."
17,"['cs.LG', '68T30', 'I.5.4']",Deep Dual Support Vector Data Description for Anomaly Detection on Attributed Networks,"Networks are ubiquitous in the real world such as social networks and
communication networks, and anomaly detection on networks aims at finding nodes
whose structural or attributed patterns deviate significantly from the majority
of reference nodes. However, most of the traditional anomaly detection methods
neglect the relation structure information among data points and therefore
cannot effectively generalize to the graph structure data. In this paper, we
propose an end-to-end model of Deep Dual Support Vector Data description based
Autoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which
considers both the structure and attribute for attributed networks.
Specifically, Dual-SVDAE consists of a structure autoencoder and an attribute
autoencoder to learn the latent representation of the node in the structure
space and attribute space respectively. Then, a dual-hypersphere learning
mechanism is imposed on them to learn two hyperspheres of normal nodes from the
structure and attribute perspectives respectively. Moreover, to achieve joint
learning between the structure and attribute of the network, we fuse the
structure embedding and attribute embedding as the final input of the feature
decoder to generate the node attribute. Finally, abnormal nodes can be detected
by measuring the distance of nodes to the learned center of each hypersphere in
the latent structure space and attribute space respectively. Extensive
experiments on the real-world attributed networks show that Dual-SVDAE
consistently outperforms the state-of-the-arts, which demonstrates the
effectiveness of the proposed method."
18,['cs.LG'],Learning Fair Graph Neural Networks with Limited and Private Sensitive Attribute Information,"Graph neural networks (GNNs) have shown great power in modeling graph
structured data. However, similar to other machine learning models, GNNs may
make biased predictions w.r.t protected sensitive attributes, e.g., skin color
and gender. This is because the training data often contains historical bias
towards sensitive attributes. In addition, we empirically show that the
discrimination in GNNs can be magnified by graph structures and the
message-passing mechanism of GNNs. As a result, the applications of GNNs in
high-stake domains such as crime rate prediction would be largely limited.
Though extensive studies of fair classification have been conducted on i.i.d
data, methods to address the problem of discrimination on non-i.i.d data are
rather limited. Generally, learning fair models require abundant sensitive
attributes to regularize the model. However, for many graphs such as social
networks, users are reluctant to share sensitive attributes. Thus, only limited
sensitive attributes are available for fair GNN training in practice. Moreover,
directly collecting and applying the sensitive attributes in fair model
training may cause privacy issues, because the sensitive information can be
leaked in data breach or attacks on the trained model. Therefore, we study a
novel and crucial problem of learning fair GNNs with limited and private
sensitive attribute information. In an attempt to address these problems,
FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high
accuracy by leveraging graph structures and limited sensitive information. We
further extend FairGNN to NT-FairGNN which can achieve both fairness and
privacy on sensitive attributes by using limited and private sensitive
attributes. Theoretical analysis and extensive experiments on real-world
datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving
fair and high-accurate classification."
19,['cs.LG'],Adversarial Stein Training for Graph Energy Models,"Learning distributions over graph-structured data is a challenging task with
many applications in biology and chemistry. In this work we use an energy-based
model (EBM) based on multi-channel graph neural networks (GNN) to learn
permutation invariant unnormalized density functions on graphs. Unlike standard
EBM training methods our approach is to learn the model via minimizing
adversarial stein discrepancy. Samples from the model can be obtained via
Langevin dynamics based MCMC. We find that this approach achieves competitive
results on graph generation compared to benchmark models."
20,"['cs.LG', 'cs.CG', 'q-bio.QM', 'stat.ML']",Parametric UMAP embeddings for representation and semi-supervised learning,"UMAP is a non-parametric graph-based dimensionality reduction algorithm using
applied Riemannian geometry and algebraic topology to find low-dimensional
embeddings of structured data. The UMAP algorithm consists of two steps: (1)
Compute a graphical representation of a dataset (fuzzy simplicial complex), and
(2) Through stochastic gradient descent, optimize a low-dimensional embedding
of the graph. Here, we extend the second step of UMAP to a parametric
optimization over neural network weights, learning a parametric relationship
between data and embedding. We first demonstrate that Parametric UMAP performs
comparably to its non-parametric counterpart while conferring the benefit of a
learned parametric mapping (e.g. fast online embeddings for new data). We then
explore UMAP as a regularization, constraining the latent distribution of
autoencoders, parametrically varying global structure preservation, and
improving classifier accuracy for semi-supervised learning by capturing
structure in unlabeled data. Google Colab walkthrough:
https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing"
21,['cs.LG'],Towards Self-Explainable Graph Neural Network,"Graph Neural Networks (GNNs), which generalize the deep neural networks to
graph-structured data, have achieved great success in modeling graphs. However,
as an extension of deep learning for graphs, GNNs lack explainability, which
largely limits their adoption in scenarios that demand the transparency of
models. Though many efforts are taken to improve the explainability of deep
learning, they mainly focus on i.i.d data, which cannot be directly applied to
explain the predictions of GNNs because GNNs utilize both node features and
graph topology to make predictions. There are only very few work on the
explainability of GNNs and they focus on post-hoc explanations. Since post-hoc
explanations are not directly obtained from the GNNs, they can be biased and
misrepresent the true explanations. Therefore, in this paper, we study a novel
problem of self-explainable GNNs which can simultaneously give predictions and
explanations. We propose a new framework which can find $K$-nearest labeled
nodes for each unlabeled node to give explainable node classification, where
nearest labeled nodes are found by interpretable similarity module in terms of
both node similarity and local structure similarity. Extensive experiments on
real-world and synthetic datasets demonstrate the effectiveness of the proposed
framework for explainable node classification."
22,['cs.LG'],Temporal Network Embedding via Tensor Factorization,"Representation learning on static graph-structured data has shown a
significant impact on many real-world applications. However, less attention has
been paid to the evolving nature of temporal networks, in which the edges are
often changing over time. The embeddings of such temporal networks should
encode both graph-structured information and the temporally evolving pattern.
Existing approaches in learning temporally evolving network representations
fail to capture the temporal interdependence. In this paper, we propose Toffee,
a novel approach for temporal network representation learning based on tensor
decomposition. Our method exploits the tensor-tensor product operator to encode
the cross-time information, so that the periodic changes in the evolving
networks can be captured. Experimental results demonstrate that Toffee
outperforms existing methods on multiple real-world temporal networks in
generating effective embeddings for the link prediction tasks."
23,['cs.LG'],LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis,"Graph structured data have enabled several successful applications such as
recommendation systems and traffic prediction, given the rich node features and
edges information. However, these high-dimensional features and high-order
adjacency information are usually heterogeneous and held by different data
holders in practice. Given such vertical data partition (e.g., one data holder
will only own either the node features or edge information), different data
holders have to develop efficient joint training protocols rather than directly
transfer data to each other due to privacy concerns. In this paper, we focus on
the edge privacy, and consider a training scenario where Bob with node features
will first send training node features to Alice who owns the adjacency
information. Alice will then train a graph neural network (GNN) with the joint
information and release an inference API. During inference, Bob is able to
provide test node features and query the API to obtain the predictions for test
nodes. Under this setting, we first propose a privacy attack LinkTeller via
influence analysis to infer the private edge information held by Alice via
designing adversarial queries for Bob. We then empirically show that LinkTeller
is able to recover a significant amount of private edges, outperforming
existing baselines. To further evaluate the privacy leakage, we adapt an
existing algorithm for differentially private graph convolutional network (DP
GCN) training and propose a new DP GCN mechanism LapGraph. We show that these
DP GCN mechanisms are not always resilient against LinkTeller empirically under
mild privacy guarantees ($\varepsilon>5$). Our studies will shed light on
future research towards designing more resilient privacy-preserving GCN models;
in the meantime, provide an in-depth understanding of the tradeoff between GCN
model utility and robustness against potential privacy attacks."
24,['cs.LG'],Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems,"Spatio-temporal forecasting is of great importance in a wide range of
dynamical systems applications from atmospheric science, to recent COVID-19
spread modeling. These applications rely on accurate predictions of
spatio-temporal structured data reflecting real-world phenomena. A stunning
characteristic is that the dynamical system is not only driven by some physics
laws but also impacted by the localized factor in spatial and temporal regions.
One of the major challenges is to infer the underlying causes, which generate
the perceived data stream and propagate the involved causal dynamics through
the distributed observing units. Another challenge is that the success of
machine learning based predictive models requires massive annotated data for
model training. However, the acquisition of high-quality annotated data is
objectively manual and tedious as it needs a considerable amount of human
intervention, making it infeasible in fields that require high levels of
expertise. To tackle these challenges, we advocate a spatio-temporal
physics-coupled neural networks (ST-PCNN) model to learn the underlying physics
of the dynamical system and further couple the learned physics to assist the
learning of the recurring dynamics. To deal with data-acquisition constraints,
an active learning mechanism with Kriging for actively acquiring the most
informative data is proposed for ST-PCNN training in a partially observable
environment. Our experiments on both synthetic and real-world datasets exhibit
that the proposed ST-PCNN with active learning converges to near optimal
accuracy with substantially fewer instances."
25,['cs.LG'],GIPA: General Information Propagation Algorithm for Graph Learning,"Graph neural networks (GNNs) have been popularly used in analyzing
graph-structured data, showing promising results in various applications such
as node classification, link prediction and network recommendation. In this
paper, we present a new graph attention neural network, namely GIPA, for
attributed graph data learning. GIPA consists of three key components:
attention, feature propagation and aggregation. Specifically, the attention
component introduces a new multi-layer perceptron based multi-head to generate
better non-linear feature mapping and representation than conventional
implementations such as dot-product. The propagation component considers not
only node features but also edge features, which differs from existing GNNs
that merely consider node features. The aggregation component uses a residual
connection to generate the final embedding. We evaluate the performance of GIPA
using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The
experimental results reveal that GIPA can beat the state-of-the-art models in
terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of
$0.8700\pm 0.0010$ and outperforms all the previous methods listed in the
ogbn-proteins leaderboard."
26,"['cs.CV', 'cs.CL']",StrucTexT: Structured Text Understanding with Multi-Modal Transformers,"Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets."
27,"['cs.LG', 'cs.CR', 'stat.ML']",Graph Backdoor,"One intriguing property of deep neural networks (DNNs) is their inherent
vulnerability to backdoor attacks -- a trojan model responds to
trigger-embedded inputs in a highly predictable manner while functioning
normally otherwise. Despite the plethora of prior work on DNNs for continuous
data (e.g., images), the vulnerability of graph neural networks (GNNs) for
discrete-structured data (e.g., graphs) is largely unexplored, which is highly
concerning given their increasing use in security-sensitive domains. To bridge
this gap, we present GTA, the first backdoor attack on GNNs. Compared with
prior work, GTA departs in significant ways: graph-oriented -- it defines
triggers as specific subgraphs, including both topological structures and
descriptive features, entailing a large design spectrum for the adversary;
input-tailored -- it dynamically adapts triggers to individual graphs, thereby
optimizing both attack effectiveness and evasiveness; downstream model-agnostic
-- it can be readily launched without knowledge regarding downstream models or
fine-tuning strategies; and attack-extensible -- it can be instantiated for
both transductive (e.g., node classification) and inductive (e.g., graph
classification) tasks, constituting severe threats for a range of
security-critical applications. Through extensive evaluation using benchmark
datasets and state-of-the-art models, we demonstrate the effectiveness of GTA.
We further provide analytical justification for its effectiveness and discuss
potential countermeasures, pointing to several promising research directions."
28,"['cs.CV', 'cs.LG']",LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices,"Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation."
29,['cs.LG'],On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization,"Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP."
30,"['cs.LG', 'cs.AI']",Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks,"Deep learning's performance has been extensively recognized recently. Graph
neural networks (GNNs) are designed to deal with graph-structural data that
classical deep learning does not easily manage. Since most GNNs were created
using distinct theories, direct comparisons are impossible. Prior research has
primarily concentrated on categorizing existing models, with little attention
paid to their intrinsic connections. The purpose of this study is to establish
a unified framework that integrates GNNs based on spectral graph and
approximation theory. The framework incorporates a strong integration between
spatial- and spectral-based GNNs while tightly associating approaches that
exist within each respective domain."
31,"['cs.LG', 'stat.ML', '68T05']",Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks,"Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification."
32,"['cs.LG', 'cs.DC']",DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs,"Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions of nodes and several billions
of edges. To tackle this challenge, we develop DistDGL, a system for training
GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the
Deep Graph Library (DGL), a popular GNN development framework. DistDGL
distributes the graph and its associated data (initial features and embeddings)
across the machines and uses this distribution to derive a computational
decomposition by following an owner-compute rule. DistDGL follows a synchronous
training approach and allows ego-networks forming the mini-batches to include
non-local nodes. To minimize the overheads associated with distributed
computations, DistDGL uses a high-quality and light-weight min-cut graph
partitioning algorithm along with multiple balancing constraints. This allows
it to reduce communication overheads and statically balance the computations.
It further reduces the communication by replicating halo nodes and by using
sparse embedding updates. The combination of these design choices allows
DistDGL to train high-quality models while achieving high parallel efficiency
and memory scalability. We demonstrate our optimizations on both inductive and
transductive GNN models. Our results show that DistDGL achieves linear speedup
without compromising model accuracy and requires only 13 seconds to complete a
training epoch for a graph with 100 million nodes and 3 billion edges on a
cluster with 16 machines. DistDGL is now publicly available as part of
DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed."
33,"['cs.LG', 'cs.AI']",Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization,"Data selection methods, such as active learning and core-set selection, are
useful tools for improving the data efficiency of deep learning models on
large-scale datasets. However, recent deep learning models have moved forward
from independent and identically distributed data to graph-structured data,
such as social networks, e-commerce user-item graphs, and knowledge graphs.
This evolution has led to the emergence of Graph Neural Networks (GNNs) that go
beyond the models existing data selection methods are designed for. Therefore,
we present Grain, an efficient framework that opens up a new perspective
through connecting data selection in GNNs with social influence maximization.
By exploiting the common patterns of GNNs, Grain introduces a novel feature
propagation concept, a diversified influence maximization objective with novel
influence and diversity functions, and a greedy algorithm with an approximation
guarantee into a unified framework. Empirical studies on public datasets
demonstrate that Grain significantly improves both the performance and
efficiency of data selection (including active learning and core-set selection)
for GNNs. To the best of our knowledge, this is the first attempt to bridge two
largely parallel threads of research, data selection, and social influence
maximization, in the setting of GNNs, paving new ways for improving data
efficiency."
34,"['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'stat.ML']",Data-driven effective model shows a liquid-like deep learning,"The geometric structure of an optimization landscape is argued to be
fundamentally important to support the success of deep neural network learning.
A direct computation of the landscape beyond two layers is hard. Therefore, to
capture the global view of the landscape, an interpretable model of the
network-parameter (or weight) space must be established. However, the model is
lacking so far. Furthermore, it remains unknown what the landscape looks like
for deep networks of binary synapses, which plays a key role in robust and
energy efficient neuromorphic computation. Here, we propose a statistical
mechanics framework by directly building a least structured model of the
high-dimensional weight space, considering realistic structured data,
stochastic gradient descent training, and the computational depth of neural
networks. We also consider whether the number of network parameters outnumbers
the number of supplied training data, namely, over- or under-parametrization.
Our least structured model reveals that the weight spaces of the
under-parametrization and over-parameterization cases belong to the same class,
in the sense that these weight spaces are well-connected without any
hierarchical clustering structure. In contrast, the shallow-network has a
broken weight space, characterized by a discontinuous phase transition, thereby
clarifying the benefit of depth in deep learning from the angle of high
dimensional geometry. Our effective model also reveals that inside a deep
network, there exists a liquid-like central part of the architecture in the
sense that the weights in this part behave as randomly as possible, providing
algorithmic implications. Our data-driven model thus provides a statistical
mechanics insight about why deep learning is unreasonably effective in terms of
the high-dimensional weight space, and how deep networks are different from
shallow ones."
35,['cs.CV'],CKConv: Learning Feature Voxelization for Point Cloud Analysis,"Despite the remarkable success of deep learning, optimal convolution
operation on point cloud remains indefinite due to its irregular data
structure. In this paper, we present Cubic Kernel Convolution (CKConv) that
learns to voxelize the features of local points by exploiting both continuous
and discrete convolutions. Our continuous convolution uniquely employs a 3D
cubic form of kernel weight representation that splits a feature into voxels in
embedding space. By consecutively applying discrete 3D convolutions on the
voxelized features in a spatial manner, preceding continuous convolution is
forced to learn spatial feature mapping, i.e., feature voxelization. In this
way, geometric information can be detailed by encoding with subdivided
features, and our 3D convolutions on these fixed structured data do not suffer
from discretization artifacts thanks to voxelization in embedding space.
Furthermore, we propose a spatial attention module, Local Set Attention (LSA),
to provide comprehensive structure awareness within the local point set and
hence produce representative features. By learning feature voxelization with
LSA, CKConv can extract enriched features for effective point cloud analysis.
We show that CKConv has great applicability to point cloud processing tasks
including object classification, object part segmentation, and scene semantic
segmentation with state-of-the-art results."
36,"['cs.LG', 'cs.DC', 'stat.ML']",Computing Graph Neural Networks: A Survey from Algorithms to Accelerators,"Graph Neural Networks (GNNs) have exploded onto the machine learning scene in
recent years owing to their capability to model and learn from graph-structured
data. Such an ability has strong implications in a wide variety of fields whose
data is inherently relational, for which conventional neural networks do not
perform well. Indeed, as recent reviews can attest, research in the area of
GNNs has grown rapidly and has lead to the development of a variety of GNN
algorithm variants as well as to the exploration of groundbreaking applications
in chemistry, neurology, electronics, or communication networks, among others.
At the current stage of research, however, the efficient processing of GNNs is
still an open challenge for several reasons. Besides of their novelty, GNNs are
hard to compute due to their dependence on the input graph, their combination
of dense and very sparse operations, or the need to scale to huge graphs in
some applications. In this context, this paper aims to make two main
contributions. On the one hand, a review of the field of GNNs is presented from
the perspective of computing. This includes a brief tutorial on the GNN
fundamentals, an overview of the evolution of the field in the last decade, and
a summary of operations carried out in the multiple phases of different GNN
algorithm variants. On the other hand, an in-depth analysis of current software
and hardware acceleration schemes is provided, from which a hardware-software,
graph-aware, and communication-centric vision for GNN accelerators is
distilled."
37,['cs.LG'],Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks,"Graph neural networks (GNNs) have achieved remarkable success as a framework
for deep learning on graph-structured data. However, GNNs are fundamentally
limited by their tree-structured inductive bias: the WL-subtree kernel
formulation bounds the representational capacity of GNNs, and polynomial-time
GNNs are provably incapable of recognizing triangles in a graph. In this work,
we propose to augment the GNN message-passing operations with information
defined on ego graphs (i.e., the induced subgraph surrounding each node). We
term these approaches Ego-GNNs and show that Ego-GNNs are provably more
powerful than standard message-passing GNNs. In particular, we show that
Ego-GNNs are capable of recognizing closed triangles, which is essential given
the prominence of transitivity in real-world graphs. We also motivate our
approach from the perspective of graph signal processing as a form of multiplex
graph convolution. Experimental results on node classification using synthetic
and real data highlight the achievable performance gains using this approach."
38,['cs.LG'],Adaptive Transfer Learning on Graph Neural Networks,"Graph neural networks (GNNs) is widely used to learn a powerful
representation of graph-structured data. Recent work demonstrates that
transferring knowledge from self-supervised tasks to downstream tasks could
further improve graph representation. However, there is an inherent gap between
self-supervised tasks and downstream tasks in terms of optimization objective
and training data. Conventional pre-training methods may be not effective
enough on knowledge transfer since they do not make any adaptation for
downstream tasks. To solve such problems, we propose a new transfer learning
paradigm on GNNs which could effectively leverage self-supervised tasks as
auxiliary tasks to help the target task. Our methods would adaptively select
and combine different auxiliary tasks with the target task in the fine-tuning
stage. We design an adaptive auxiliary loss weighting model to learn the
weights of auxiliary tasks by quantifying the consistency between auxiliary
tasks and the target task. In addition, we learn the weighting model through
meta-learning. Our methods can be applied to various transfer learning
approaches, it performs well not only in multi-task learning but also in
pre-training and fine-tuning. Comprehensive experiments on multiple downstream
tasks demonstrate that the proposed methods can effectively combine auxiliary
tasks with the target task and significantly improve the performance compared
to state-of-the-art methods."
39,"['cs.LG', 'cs.SI']",Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning,"Graph representation learning plays a vital role in processing
graph-structured data. However, prior arts on graph representation learning
heavily rely on labeling information. To overcome this problem, inspired by the
recent success of graph contrastive learning and Siamese networks in visual
representation learning, we propose a novel self-supervised approach in this
paper to learn node representations by enhancing Siamese self-distillation with
multi-scale contrastive learning. Specifically, we first generate two augmented
views from the input graph based on local and global perspectives. Then, we
employ two objectives called cross-view and cross-network contrastiveness to
maximize the agreement between node representations across different views and
networks. To demonstrate the effectiveness of our approach, we perform
empirical experiments on five real-world datasets. Our method not only achieves
new state-of-the-art results but also surpasses some semi-supervised
counterparts by large margins. Code is made available at
https://github.com/GRAND-Lab/MERIT"
40,"['cs.LG', 'cs.SD', 'eess.AS']",Deep Neural Networks and End-to-End Learning for Audio Compression,"Recent achievements in end-to-end deep learning have encouraged the
exploration of tasks dealing with highly structured data with unified deep
network models. Having such models for compressing audio signals has been
challenging since it requires discrete representations that are not easy to
train with end-to-end backpropagation. In this paper, we present an end-to-end
deep learning approach that combines recurrent neural networks (RNNs) within
the training strategy of variational autoencoders (VAEs) with a binary
representation of the latent space. We apply a reparametrization trick for the
Bernoulli distribution for the discrete representations, which allows smooth
backpropagation. In addition, our approach allows the separation of the encoder
and decoder, which is necessary for compression tasks. To our best knowledge,
this is the first end-to-end learning for a single audio compression model with
RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54."
41,"['cs.LG', 'cs.AI']",Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering,"Graph convolutional networks are becoming indispensable for deep learning
from graph-structured data. Most of the existing graph convolutional networks
share two big shortcomings. First, they are essentially low-pass filters, thus
the potentially useful middle and high frequency band of graph signals are
ignored. Second, the bandwidth of existing graph convolutional filters is
fixed. Parameters of a graph convolutional filter only transform the graph
inputs without changing the curvature of a graph convolutional filter function.
In reality, we are uncertain about whether we should retain or cut off the
frequency at a certain point unless we have expert domain knowledge. In this
paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture
the full spectrum of graph signals and automatically update the bandwidth of
graph convolutional filters. While it is based on graph spectral theory, our
AutoGCN is also localized in space and has a spatial form. Experimental results
show that AutoGCN achieves significant improvement over baseline methods which
only work as low-pass filters."
42,['cs.LG'],Inter-domain Multi-relational Link Prediction,"Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods."
43,"['cs.LG', 'cs.AI']",Multi-Level Graph Contrastive Learning,"Graph representation learning has attracted a surge of interest recently,
whose target at learning discriminant embedding for each node in the graph.
Most of these representation methods focus on supervised learning and heavily
depend on label information. However, annotating graphs are expensive to obtain
in the real world, especially in specialized domains (i.e. biology), as it
needs the annotator to have the domain knowledge to label the graph. To
approach this problem, self-supervised learning provides a feasible solution
for graph representation learning. In this paper, we propose a Multi-Level
Graph Contrastive Learning (MLGCL) framework for learning robust representation
of graph data by contrasting space views of graphs. Specifically, we introduce
a novel contrastive view - topological and feature space views. The original
graph is first-order approximation structure and contains uncertainty or error,
while the $k$NN graph generated by encoding features preserves high-order
proximity. Thus $k$NN graph generated by encoding features not only provide a
complementary view, but is more suitable to GNN encoder to extract discriminant
representation. Furthermore, we develop a multi-level contrastive mode to
preserve the local similarity and semantic similarity of graph-structured data
simultaneously. Extensive experiments indicate MLGCL achieves promising results
compared with the existing state-of-the-art graph representation learning
methods on seven datasets."
44,"['cs.LG', 'cs.IR', 'stat.ML', 'I.2.6']",Overlapping Spaces for Compact Graph Representations,"Various non-trivial spaces are becoming popular for embedding structured data
such as graphs, texts, or images. Following spherical and hyperbolic spaces,
more general product spaces have been proposed. However, searching for the best
configuration of product space is a resource-intensive procedure, which reduces
the practical applicability of the idea. We generalize the concept of product
space and introduce an overlapping space that does not have the configuration
search problem. The main idea is to allow subsets of coordinates to be shared
between spaces of different types (Euclidean, hyperbolic, spherical). As a
result, parameter optimization automatically learns the optimal configuration.
Additionally, overlapping spaces allow for more compact representations since
their geometry is more complex. Our experiments confirm that overlapping spaces
outperform the competitors in graph embedding tasks. Here, we consider both
distortion setup, where the aim is to preserve distances, and ranking setup,
where the relative order should be preserved. The proposed method effectively
solves the problem and outperforms the competitors in both settings. We also
perform an empirical analysis in a realistic information retrieval task, where
we compare all spaces by incorporating them into DSSM. In this case, the
proposed overlapping space consistently achieves nearly optimal results without
any configuration tuning. This allows for reducing training time, which can be
significant in large-scale applications."
45,"['cs.LG', 'cs.AI']",ARM-Net: Adaptive Relation Modeling Network for Structured Data,"Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
  In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making."
46,"['cs.LG', 'cs.SY', 'eess.SY']",Learning deep autoregressive models for hierarchical data,"We propose a model for hierarchical structured data as an extension to the
stochastic temporal convolutional network. The proposed model combines an
autoregressive model with a hierarchical variational autoencoder and
downsampling to achieve superior computational complexity. We evaluate the
proposed model on two different types of sequential data: speech and
handwritten text. The results are promising with the proposed model achieving
state-of-the-art performance."
47,['cs.LG'],Edge Representation Learning with Hypergraphs,"Graph neural networks have recently achieved remarkable success in
representing graph-structured data, with rapid progress in both the node
embedding and graph pooling methods. Yet, they mostly focus on capturing
information from the nodes considering their connectivity, and not much work
has been done in representing the edges, which are essential components of a
graph. However, for tasks such as graph reconstruction and generation, as well
as graph classification tasks for which the edges are important for
discrimination, accurately representing edges of a given graph is crucial to
the success of the graph representation learning. To this end, we propose a
novel edge representation learning framework based on Dual Hypergraph
Transformation (DHT), which transforms the edges of a graph into the nodes of a
hypergraph. This dual hypergraph construction allows us to apply message
passing techniques for node representations to edges. After obtaining edge
representations from the hypergraphs, we then cluster or drop edges to obtain
holistic graph-level edge representations. We validate our edge representation
learning method with hypergraphs on diverse graph datasets for graph
representation and generation performance, on which our method largely
outperforms existing graph representation learning methods. Moreover, our edge
representation learning and pooling method also largely outperforms
state-of-the-art graph pooling methods on graph classification, not only
because of its accurate edge representation learning, but also due to its
lossless compression of the nodes and removal of irrelevant edges for effective
message passing."
48,"['cs.CV', '68T45 (Primary) 68T10, 68T07 (Secondary)', 'I.4.9; I.5.4; I.2.10']",Domain adaptation for person re-identification on new unlabeled data using AlignedReID++,"In the world where big data reigns and there is plenty of hardware prepared
to gather a huge amount of non structured data, data acquisition is no longer a
problem. Surveillance cameras are ubiquitous and they capture huge numbers of
people walking across different scenes. However, extracting value from this
data is challenging, specially for tasks that involve human images, such as
face recognition and person re-identification. Annotation of this kind of data
is a challenging and expensive task. In this work we propose a domain
adaptation workflow to allow CNNs that were trained in one domain to be applied
to another domain without the need for new annotation of the target data. Our
method uses AlignedReID++ as the baseline, trained using a Triplet loss with
batch hard. Domain adaptation is done by using pseudo-labels generated using an
unsupervised learning strategy. Our results show that domain adaptation
techniques really improve the performance of the CNN when applied in the target
domain."
49,['cs.LG'],GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily,"In representation learning on the graph-structured data, under heterophily
(or low homophily), many popular GNNs may fail to capture long-range
dependencies, which leads to their performance degradation. To solve the
above-mentioned issue, we propose a graph convolutional networks with structure
learning (GCN-SL), and furthermore, the proposed approach can be applied to
node classification. The proposed GCN-SL contains two improvements:
corresponding to node features and edges, respectively. In the aspect of node
features, we propose an efficient-spectral-clustering (ESC) and an ESC with
anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations
from all similar nodes. In the aspect of edges, we build a re-connected
adjacency matrix by using a special data preprocessing technique and similarity
learning, and the re-connected adjacency matrix can be optimized directly along
with GCN-SL parameters. Considering that the original adjacency matrix may
provide misleading information for aggregation in GCN, especially the graphs
being with a low level of homophily. The proposed GCN-SL can aggregate feature
representations from nearby nodes via re-connected adjacency matrix and is
applied to graphs with various levels of homophily. Experimental results on a
wide range of benchmark datasets illustrate that the proposed GCN-SL
outperforms the stateof-the-art GNN counterparts."
50,['cs.LG'],Graph Contrastive Learning Automated,"Self-supervised learning on graph-structured data has drawn recent interest
for learning generalizable, transferable and robust representations from
unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged
with promising representation learning performance. Unfortunately, unlike its
counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data
augmentations, which have to be manually picked per dataset, by either rules of
thumb or trial-and-errors, owing to the diverse nature of graph data. That
significantly limits the more general applicability of GraphCL. Aiming to fill
in this crucial gap, this paper proposes a unified bi-level optimization
framework to automatically, adaptively and dynamically select data
augmentations when performing GraphCL on specific graph data. The general
framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
min-max optimization. The selections of augmentations made by JOAO are shown to
be in general aligned with previous ""best practices"" observed from handcrafted
tuning: yet now being automated, more flexible and versatile. Moreover, we
propose a new augmentation-aware projection head mechanism, which will route
output features through different projection heads corresponding to different
augmentations chosen at each training step. Extensive experiments demonstrate
that JOAO performs on par with or sometimes better than the state-of-the-art
competitors including GraphCL, on multiple graph datasets of various scales and
types, yet without resorting to any laborious dataset-specific tuning on
augmentation selection. We release the code at
https://github.com/Shen-Lab/GraphCL_Automated."
51,['cs.LG'],CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals,"Data augmentation is a key element of deep learning pipelines, as it informs
the network during training about transformations of the input data that keep
the label unchanged. Manually finding adequate augmentation methods and
parameters for a given pipeline is however rapidly cumbersome. In particular,
while intuition can guide this decision for images, the design and choice of
augmentation policies remains unclear for more complex types of data, such as
neuroscience signals. Moreover, label independent strategies might not be
suitable for such structured data and class-dependent augmentations might be
necessary. This idea has been surprisingly unexplored in the literature, while
it is quite intuitive: changing the color of a car image does not change the
object class to be predicted, but doing the same to the picture of an orange
does. This paper aims to increase the generalization power added through
class-wise data augmentation. Yet, as seeking transformations depending on the
class largely increases the complexity of the task, using gradient-free
optimization techniques as done by most existing automatic approaches becomes
intractable for real-world datasets. For this reason we propose to use
differentiable data augmentation amenable to gradient-based learning. EEG
signals are a perfect example of data for which good augmentation policies are
mostly unknown. In this work, we demonstrate the relevance of our approach on
the clinically relevant sleep staging classification task, for which we also
propose differentiable transformations."
52,['cs.LG'],Temporal Graph Signal Decomposition,"Temporal graph signals are multivariate time series with individual
components associated with nodes of a fixed graph structure. Data of this kind
arises in many domains including activity of social network users, sensor
network readings over time, and time course gene expression within the
interaction network of a model organism. Traditional matrix decomposition
methods applied to such data fall short of exploiting structural regularities
encoded in the underlying graph and also in the temporal patterns of the
signal. How can we take into account such structure to obtain a succinct and
interpretable representation of temporal graph signals?
  We propose a general, dictionary-based framework for temporal graph signal
decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of
the data via a combination of graph and time dictionaries. We propose a highly
scalable decomposition algorithm for both complete and incomplete data, and
demonstrate its advantage for matrix decomposition, imputation of missing
values, temporal interpolation, clustering, period estimation, and rank
estimation in synthetic and real-world data ranging from traffic patterns to
social media activity. Our framework achieves 28% reduction in RMSE compared to
baselines for temporal interpolation when as many as 75% of the observations
are missing. It scales best among baselines taking under 20 seconds on 3.5
million data points and produces the most parsimonious models. To the best of
our knowledge, TGSD is the first framework to jointly model graph signals by
temporal and graph dictionaries."
53,"['cs.LG', 'cs.AI', 'stat.ML']",Graph Mixture Density Networks,"We introduce the Graph Mixture Density Networks, a new family of machine
learning models that can fit multimodal output distributions conditioned on
graphs of arbitrary topology. By combining ideas from mixture models and graph
representation learning, we address a broader class of challenging conditional
density estimation problems that rely on structured data. In this respect, we
evaluate our method on a new benchmark application that leverages random graphs
for stochastic epidemic simulations. We show a significant improvement in the
likelihood of epidemic outcomes when taking into account both multimodality and
structure. The empirical analysis is complemented by two real-world regression
tasks showing the effectiveness of our approach in modeling the output
prediction uncertainty. Graph Mixture Density Networks open appealing research
opportunities in the study of structure-dependent phenomena that exhibit
non-trivial conditional output distributions."
54,"['cs.LG', 'cs.AI']",You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks,"Hypergraphs are used to model higher-order interactions amongst agents and
there exist many practically relevant instances of hypergraph datasets. To
enable efficient processing of hypergraph-structured data, several hypergraph
neural network platforms have been proposed for learning hypergraph properties
and structure, with a special focus on node classification. However, almost all
existing methods use heuristic propagation rules and offer suboptimal
performance on many datasets. We propose AllSet, a new hypergraph neural
network paradigm that represents a highly general framework for (hyper)graph
neural networks and for the first time implements hypergraph neural network
layers as compositions of two multiset functions that can be efficiently
learned for each task and each dataset. Furthermore, AllSet draws on new
connections between hypergraph neural networks and recent advances in deep
learning of multiset functions. In particular, the proposed architecture
utilizes Deep Sets and Set Transformer architectures that allow for significant
modeling flexibility and offer high expressive power. To evaluate the
performance of AllSet, we conduct the most extensive experiments to date
involving ten known benchmarking datasets and three newly curated datasets that
represent significant challenges for hypergraph node classification. The
results demonstrate that AllSet has the unique ability to consistently either
match or outperform all other hypergraph neural networks across the tested
datasets. Our implementation and dataset will be released upon acceptance."
55,"['cs.LG', 'cs.CR', 'cs.SI', 'stat.ML']",Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models,"Deep neural networks, while generalize well, are known to be sensitive to
small adversarial perturbations. This phenomenon poses severe security threat
and calls for in-depth investigation of the robustness of deep learning models.
With the emergence of neural networks for graph structured data, similar
investigations are urged to understand their robustness. It has been found that
adversarially perturbing the graph structure and/or node features may result in
a significant degradation of the model performance. In this work, we show from
a different angle that such fragility similarly occurs if the graph contains a
few bad-actor nodes, which compromise a trained graph neural network through
flipping the connections to any targeted victim. Worse, the bad actors found
for one graph model severely compromise other models as well. We call the bad
actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them.
Thorough empirical investigations suggest an interesting finding that the
anchor nodes often belong to the same class; and they also corroborate the
intuitive trade-off between the number of anchor nodes and the attack success
rate. For the dataset Cora which contains 2708 nodes, as few as six anchor
nodes will result in an attack success rate higher than 80\% for GCN and other
three models."
56,"['cs.LG', '68T07, 68T30, 68R99', 'I.2.0; I.2.4']",Graph Attention Networks with LSTM-based Path Reweighting,"Graph Neural Networks (GNNs) have been extensively used for mining
graph-structured data with impressive performance. However, traditional GNNs
suffer from over-smoothing, non-robustness and over-fitting problems. To solve
these weaknesses, we design a novel GNN solution, namely Graph Attention
Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically
aggregate multi-hop information, highlight important paths and filter out
noises. In addition, we utilize random path sampling in PR-GAT for data
augmentation. The augmented data is used for predicting the distribution of
corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the
issues of over-smoothing, non-robustness and overfitting. We achieve
state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for
other 2 datasets. The average accuracy of 7 datasets have been improved by
0.5\% than the best SOTA from literature."
57,"['cs.LG', 'cs.AI', 'cs.NA', 'math.NA', '68T07, 05C85, 42C40', 'I.2.4; I.2.6']",How Framelets Enhance Graph Neural Networks,"This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance."
58,"['cs.LG', 'cs.AI']",Do Transformers Really Perform Bad for Graph Representation?,"The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer."
59,"['stat.ML', 'cs.LG']",Distance Metric Learning for Graph Structured Data,"Graphs are versatile tools for representing structured data. As a result, a
variety of machine learning methods have been studied for graph data analysis.
Although many such learning methods depend on the measurement of differences
between input graphs, defining an appropriate distance metric for graphs
remains a controversial issue. Hence, we propose a supervised distance metric
learning method for the graph classification problem. Our method, named
interpretable graph metric learning (IGML), learns discriminative metrics in a
subgraph-based feature space, which has a strong graph representation
capability. By introducing a sparsity-inducing penalty on the weight of each
subgraph, IGML can identify a small number of important subgraphs that can
provide insight into the given classification task. Because our formulation has
a large number of optimization variables, an efficient algorithm that uses
pruning techniques based on safe screening and working set selection methods is
also proposed. An important property of IGML is that solution optimality is
guaranteed because the problem is formulated as a convex problem and our
pruning strategies only discard unnecessary subgraphs. Furthermore, we show
that IGML is also applicable to other structured data such as itemset and
sequence data, and that it can incorporate vertex-label similarity by using a
transportation-based subgraph feature. We empirically evaluate the
computational efficiency and classification performance of IGML on several
benchmark datasets and provide some illustrative examples of how IGML
identifies important subgraphs from a given graph dataset."
60,['cs.LG'],Comparison of Outlier Detection Techniques for Structured Data,"An outlier is an observation or a data point that is far from rest of the
data points in a given dataset or we can be said that an outlier is away from
the center of mass of observations. Presence of outliers can skew statistical
measures and data distributions which can lead to misleading representation of
the underlying data and relationships. It is seen that the removal of outliers
from the training dataset before modeling can give better predictions. With the
advancement of machine learning, the outlier detection models are also
advancing at a good pace. The goal of this work is to highlight and compare
some of the existing outlier detection techniques for the data scientists to
use that information for outlier algorithm selection while building a machine
learning model."
61,"['cs.LG', 'stat.ML']",ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling,"The spatio-temporal graph learning is becoming an increasingly important
object of graph study. Many application domains involve highly dynamic graphs
where temporal information is crucial, e.g. traffic networks and financial
transaction graphs. Despite the constant progress made on learning structured
data, there is still a lack of effective means to extract dynamic complex
features from spatio-temporal structures. Particularly, conventional models
such as convolutional networks or recurrent neural networks are incapable of
revealing the temporal patterns in short or long terms and exploring the
spatial properties in local or global scope from spatio-temporal graphs
simultaneously. To tackle this problem, we design a novel multi-scale
architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series
modeling. In this U-shaped network, a paired sampling operation is proposed in
spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in
spatial from its deterministic partition while abstracts multi-resolution
temporal dependencies through dilated recurrent skip connections; based on
previous settings in the downsampling, the unpooling (ST-Unpool) restores the
original structure of spatio-temporal graphs and resumes regular intervals
within graph sequences. Experiments on spatio-temporal prediction tasks
demonstrate that our model effectively captures comprehensive features in
multiple scales and achieves substantial improvements over mainstream methods
on several real-world datasets."
62,"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH', '62H30 (Primary) 54F45 (Secondary)']",Universal consistency of Wasserstein $k$-NN classifier,"The Wasserstein distance provides a notion of dissimilarities between
probability measures, which has recent applications in learning of structured
data with varying size such as images and text documents. In this work, we
analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein
distance and establish the universal consistency on families of distributions.
Using previous known results on the consistency of the $k$-NN classifier on
infinite dimensional metric spaces, it suffices to show that the families is a
countable union of finite dimension sets. As a result, we show that the $k$-NN
classifier is universally consistent on spaces of finitely supported measures,
the space of Gaussian measures, and the space of measures with finite wavelet
densities. In addition, we give a counterexample to show that the universal
consistency does not hold on $\mathcal{W}_p((0,1))$."
63,['cs.LG'],Graph Domain Adaptation: A Generative View,"Recent years have witnessed tremendous interest in deep learning on
graph-structured data. Due to the high cost of collecting labeled
graph-structured data, domain adaptation is important to supervised graph
learning tasks with limited samples. However, current graph domain adaptation
methods are generally adopted from traditional domain adaptation tasks, and the
properties of graph-structured data are not well utilized. For example, the
observed social networks on different platforms are controlled not only by the
different crowd or communities but also by the domain-specific policies and the
background noise. Based on these properties in graph-structured data, we first
assume that the graph-structured data generation process is controlled by three
independent types of latent variables, i.e., the semantic latent variables, the
domain latent variables, and the random latent variables. Based on this
assumption, we propose a disentanglement-based unsupervised domain adaptation
method for the graph-structured data, which applies variational graph
auto-encoders to recover these latent variables and disentangles them via three
supervised learning modules. Extensive experimental results on two real-world
datasets in the graph classification task reveal that our method not only
significantly outperforms the traditional domain adaptation methods and the
disentangled-based domain adaptation methods but also outperforms the
state-of-the-art graph domain adaptation algorithms."
64,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",Information Obfuscation of Graph Neural Networks,"While the advent of Graph Neural Networks (GNNs) has greatly improved node
and graph representation learning in many applications, the neighborhood
aggregation scheme exposes additional vulnerabilities to adversaries seeking to
extract node-level information about sensitive attributes. In this paper, we
study the problem of protecting sensitive attributes by information obfuscation
when learning with graph structured data. We propose a framework to locally
filter out pre-determined sensitive attributes via adversarial training with
the total variation and the Wasserstein distance. Our method creates a strong
defense against inference attacks, while only suffering small loss in task
performance. Theoretically, we analyze the effectiveness of our framework
against a worst-case adversary, and characterize an inherent trade-off between
maximizing predictive accuracy and minimizing information leakage. Experiments
across multiple datasets from recommender systems, knowledge graphs and quantum
chemistry demonstrate that the proposed approach provides a robust defense
across various graph structures and tasks, while producing competitive GNN
encoders for downstream tasks."
65,['cs.LG'],Link Prediction with Persistent Homology: An Interactive View,"Link prediction is an important learning task for graph-structured data. In
this paper, we propose a novel topological approach to characterize
interactions between two nodes. Our topological feature, based on the extended
persistent homology, encodes rich structural information regarding the
multi-hop paths connecting nodes. Based on this feature, we propose a graph
neural network method that outperforms state-of-the-arts on different
benchmarks. As another contribution, we propose a novel algorithm to more
efficiently compute the extended persistence diagrams for graphs. This
algorithm can be generally applied to accelerate many other topological methods
for graph learning tasks."
66,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']",A Review of Graph Neural Networks and Their Applications in Power Systems,"Deep neural networks have revolutionized many machine learning tasks in power
systems, ranging from pattern recognition to signal processing. The data in
these tasks is typically represented in Euclidean domains. Nevertheless, there
is an increasing number of applications in power systems, where data are
collected from non-Euclidean domains and represented as graph-structured data
with high dimensional features and interdependency among nodes. The complexity
of graph-structured data has brought significant challenges to the existing
deep neural networks defined in Euclidean domains. Recently, many publications
generalizing deep neural networks for graph-structured data in power systems
have emerged. In this paper, a comprehensive overview of graph neural networks
(GNNs) in power systems is proposed. Specifically, several classical paradigms
of GNNs structures (e.g., graph convolutional networks) are summarized, and key
applications in power systems, such as fault scenario application, time series
prediction, power flow calculation, and data generation are reviewed in detail.
Furthermore, main issues and some research trends about the applications of
GNNs in power systems are discussed."
67,['cs.LG'],Learnable Hypergraph Laplacian for Hypergraph Learning,"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability."
68,"['cs.LG', 'cs.AI']",What Can Knowledge Bring to Machine Learning? -- A Survey of Low-shot Learning for Structured Data,"Supervised machine learning has several drawbacks that make it difficult to
use in many situations. Drawbacks include: heavy reliance on massive training
data, limited generalizability and poor expressiveness of high-level semantics.
Low-shot Learning attempts to address these drawbacks. Low-shot learning allows
the model to obtain good predictive power with very little or no training data,
where structured knowledge plays a key role as a high-level semantic
representation of human. This article will review the fundamental factors of
low-shot learning technologies, with a focus on the operation of structured
knowledge under different low-shot conditions. We also introduce other
techniques relevant to low-shot learning. Finally, we point out the limitations
of low-shot learning, the prospects and gaps of industrial applications, and
future research directions."
69,"['cs.LG', 'cs.AI', 'cs.SI']",Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs,"Graph Neural Networks (GNNs) have been widely applied to various fields due
to their powerful representations of graph-structured data. Despite the success
of GNNs, most existing GNNs are designed to learn node representations on the
fixed and homogeneous graphs. The limitations especially become problematic
when learning representations on a misspecified graph or a heterogeneous graph
that consists of various types of nodes and edges. To address this limitations,
we propose Graph Transformer Networks (GTNs) that are capable of generating new
graph structures, which preclude noisy connections and include useful
connections (e.g., meta-paths) for tasks, while learning effective node
representations on the new graphs in an end-to-end fashion. We further propose
enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that
improve scalability of graph transformations. Compared to GTNs, FastGTNs are
230x faster and use 100x less memory while allowing the identical graph
transformations as GTNs. In addition, we extend graph transformations to the
semantic proximity of nodes allowing non-local operations beyond meta-paths.
Extensive experiments on both homogeneous graphs and heterogeneous graphs show
that GTNs and FastGTNs with non-local operations achieve the state-of-the-art
performance for node classification tasks. The code is available:
https://github.com/seongjunyun/Graph_Transformer_Networks"
70,['cs.LG'],Learning to Pool in Graph Neural Networks for Extrapolation,"Graph neural networks (GNNs) are one of the most popular approaches to using
deep learning on graph-structured data, and they have shown state-of-the-art
performances on a variety of tasks. However, according to a recent study, a
careful choice of pooling functions, which are used for the aggregation or
readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without
the ideal combination of pooling functions, which varies across tasks, GNNs
completely fail to generalize to out-of-distribution data, while the number of
possible combinations grows exponentially with the number of layers. In this
paper, we present GNP, a $L^p$ norm-like pooling function that is trainable
end-to-end for any given task. Notably, GNP generalizes most of the widely-used
pooling functions. We verify experimentally that simply replacing all pooling
functions with GNP enables GNNs to extrapolate well on many node-level,
graph-level, and set-related tasks; and GNP sometimes performs even better than
optimal combinations of existing pooling functions."
71,"['cs.LG', 'cs.AI']",Learnable Hypergraph Laplacian for Hypergraph Learning,"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability."
72,['cs.LG'],Learning subtree pattern importance for Weisfeiler-Lehmanbased graph kernels,"Graph is an usual representation of relational data, which are ubiquitous in
manydomains such as molecules, biological and social networks. A popular
approach to learningwith graph structured data is to make use of graph kernels,
which measure the similaritybetween graphs and are plugged into a kernel
machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph
kernels, which employ WL labeling scheme to extract subtree patterns and
perform node embedding, are demonstrated to achieve great performance while
being efficiently computable. However, one of the main drawbacks of ageneral
kernel is the decoupling of kernel construction and learning process. For
moleculargraphs, usual kernels such as WL subtree, based on substructures of
the molecules, consider all available substructures having the same importance,
which might not be suitable inpractice. In this paper, we propose a method to
learn the weights of subtree patterns in the framework of WWL kernels, the
state of the art method for graph classification task [14]. To overcome the
computational issue on large scale data sets, we present an efficient learning
algorithm and also derive a generalization gap bound to show its convergence.
Finally, through experiments on synthetic and real-world data sets, we
demonstrate the effectiveness of our proposed method for learning the weights
of subtree patterns."
73,"['cs.LG', 'stat.ML']",ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,"Event forecasting is a challenging, yet important task, as humans seek to
constantly plan for the future. Existing automated forecasting studies rely
mostly on structured data, such as time-series or event-based knowledge graphs,
to help predict future events. In this work, we aim to formulate a task,
construct a dataset, and provide benchmarks for developing methods for event
forecasting with large volumes of unstructured text data. To simulate the
forecasting scenario on temporal news documents, we formulate the problem as a
restricted-domain, multiple-choice, question-answering (QA) task. Unlike
existing QA tasks, our task limits accessible information, and thus a model has
to make a forecasting judgement. To showcase the usefulness of this task
formulation, we introduce ForecastQA, a question-answering dataset consisting
of 10,392 event forecasting questions, which have been collected and verified
via crowdsourcing efforts. We present our experiments on ForecastQA using
BERT-based models and find that our best model achieves 60.1% accuracy on the
dataset, which still lags behind human performance by about 19%. We hope
ForecastQA will support future research efforts in bridging this gap."
74,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.SI']",Graph-MLP: Node Classification without Message Passing in Graph,"Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task."
75,"['cs.LG', 'cs.DM', 'stat.ML', 'G.1.6; I.2.6']",Expressive Power of Invariant and Equivariant Graph Neural Networks,"Various classes of Graph Neural Networks (GNN) have been proposed and shown
to be successful in a wide range of applications with graph structured data. In
this paper, we propose a theoretical framework able to compare the expressive
power of these GNN architectures. The current universality theorems only apply
to intractable classes of GNNs. Here, we prove the first approximation
guarantees for practical GNNs, paving the way for a better understanding of
their generalization. Our theoretical results are proved for invariant GNNs
computing a graph embedding (permutation of the nodes of the input graph does
not affect the output) and equivariant GNNs computing an embedding of the nodes
(permutation of the input permutes the output). We show that Folklore Graph
Neural Networks (FGNN), which are tensor based GNNs augmented with matrix
multiplication are the most expressive architectures proposed so far for a
given tensor order. We illustrate our results on the Quadratic Assignment
Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to
learn how to solve the problem, leading to much better average performances
than existing algorithms (based on spectral, SDP or other GNNs architectures).
On a practical side, we also implement masked tensors to handle batches of
graphs of varying sizes."
76,['cs.LG'],Generative Causal Explanations for Graph Neural Networks,"This paper presents Gem, a model-agnostic approach for providing
interpretable explanations for any GNNs on various graph learning tasks.
Specifically, we formulate the problem of providing explanations for the
decisions of GNNs as a causal learning task. Then we train a causal explanation
model equipped with a loss function based on Granger causality. Different from
existing explainers for GNNs, Gem explains GNNs on graph-structured data from a
causal perspective. It has better generalization ability as it has no
requirements on the internal structure of the GNNs or prior knowledge on the
graph learning tasks. In addition, Gem, once trained, can be used to explain
the target GNN very quickly. Our theoretical analysis shows that several recent
explainers fall into a unified framework of additive feature attribution
methods. Experimental results on synthetic and real-world datasets show that
Gem achieves a relative increase of the explanation accuracy by up to $30\%$
and speeds up the explanation process by up to $110\times$ as compared to its
state-of-the-art alternatives."
77,['cs.LG'],Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space,"Learning the representation of data with hierarchical structures in the
hyperbolic space attracts increasing attention in recent years. Due to the
constant negative curvature, the hyperbolic space resembles tree metrics and
captures the tree-like properties naturally, which enables the hyperbolic
embeddings to improve over traditional Euclidean models. However, many
real-world hierarchically structured data such as taxonomies and multitree
networks have varying local structures and they are not trees, thus they do not
ubiquitously match the constant curvature property of the hyperbolic space. To
address this limitation of hyperbolic embeddings, we explore the complex
hyperbolic space, which has the variable negative curvature, for representation
learning. Specifically, we propose to learn the embeddings of hierarchically
structured data in the unit ball model of the complex hyperbolic space. The
unit ball model based embeddings have a more powerful representation capacity
to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the
hyperbolic embedding models significantly."
78,"['cs.LG', 'cs.AI']",ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks,"Imbalanced classification on graphs is ubiquitous yet challenging in many
real-world applications, such as fraudulent node detection. Recently, graph
neural networks (GNNs) have shown promising performance on many network
analysis tasks. However, most existing GNNs have almost exclusively focused on
the balanced networks, and would get unappealing performance on the imbalanced
networks. To bridge this gap, in this paper, we present a generative
adversarial graph network model, called ImGAGN to address the imbalanced
classification problem on graphs. It introduces a novel generator for graph
structure data, named GraphGenerator, which can simulate both the minority
class nodes' attribute distribution and network topological structure
distribution by generating a set of synthetic minority nodes such that the
number of nodes in different classes can be balanced. Then a graph
convolutional network (GCN) discriminator is trained to discriminate between
real nodes and fake (i.e., generated) nodes, and also between minority nodes
and majority nodes on the synthetic balanced network. To validate the
effectiveness of the proposed method, extensive experiments are conducted on
four real-world imbalanced network datasets. Experimental results demonstrate
that the proposed method ImGAGN outperforms state-of-the-art algorithms for
semi-supervised imbalanced node classification task."
79,['cs.LG'],SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks,"Graph Neural Networks (GNNs) are the first choice methods for graph machine
learning problems thanks to their ability to learn state-of-the-art level
representations from graph-structured data. However, centralizing a massive
amount of real-world graph data for GNN training is prohibitive due to
user-side privacy concerns, regulation restrictions, and commercial
competition. Federated Learning is the de-facto standard for collaborative
training of machine learning models over many distributed edge devices without
the need for centralization. Nevertheless, training graph neural networks in a
federated setting is vaguely defined and brings statistical and systems
challenges. This work proposes SpreadGNN, a novel multi-task federated training
framework capable of operating in the presence of partial labels and absence of
a central server for the first time in the literature. SpreadGNN extends
federated multi-task learning to realistic serverless settings for GNNs, and
utilizes a novel optimization algorithm with a convergence guarantee,
Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized
multi-task learning problems. We empirically demonstrate the efficacy of our
framework on a variety of non-I.I.D. distributed graph-level molecular property
prediction datasets with partial labels. Our results show that SpreadGNN
outperforms GNN models trained over a central server-dependent federated
learning system, even in constrained topologies. The source code is publicly
available at https://github.com/FedML-AI/SpreadGNN"
80,"['cs.LG', 'stat.ML']",Implicit Graph Neural Networks,"Graph Neural Networks (GNNs) are widely used deep learning models that learn
meaningful representations from graph-structured data. Due to the finite nature
of the underlying recurrent structure, current GNN methods may struggle to
capture long-range dependencies in underlying graphs. To overcome this
difficulty, we propose a graph learning framework, called Implicit Graph Neural
Networks (IGNN), where predictions are based on the solution of a fixed-point
equilibrium equation involving implicitly defined ""state"" vectors. We use the
Perron-Frobenius theory to derive sufficient conditions that ensure
well-posedness of the framework. Leveraging implicit differentiation, we derive
a tractable projected gradient descent method to train the framework.
Experiments on a comprehensive range of tasks show that IGNNs consistently
capture long-range dependencies and outperform the state-of-the-art GNN models."
81,"['cs.LG', 'eess.SP']",Node-Variant Graph Filters in Graph Neural Networks,"Graph neural networks (GNNs) have been successfully employed in a myriad of
applications involving graph-structured data. Theoretical findings establish
that GNNs use nonlinear activation functions to create low-eigenvalue frequency
content that can be processed in a stable manner by subsequent graph
convolutional filters. However, the exact shape of the frequency content
created by nonlinear functions is not known, and thus, it cannot be learned nor
controlled. In this work, node-variant graph filters (NVGFs) are shown to be
capable of creating frequency content and are thus used in lieu of nonlinear
activation functions. This results in a novel GNN architecture that, although
linear, is capable of creating frequency content as well. Furthermore, this new
frequency content can be either designed or learned from data. In this way, the
role of frequency creation is separated from the nonlinear nature of
traditional GNNs. Extensive simulations are carried out to differentiate the
contributions of frequency creation from those of the nonlinearity."
82,"['cs.LG', 'cs.SI']",Hashing-Accelerated Graph Neural Networks for Link Prediction,"Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources."
83,"['cs.CV', 'cs.LG']",Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data,"Estimating the amount of electricity that can be produced by rooftop
photovoltaic systems is a time-consuming process that requires on-site
measurements, a difficult task to achieve on a large scale. In this paper, we
present an approach to estimate the solar potential of rooftops based on their
location and architectural characteristics, as well as the amount of solar
radiation they receive annually. Our technique uses computer vision to achieve
semantic segmentation of roof sections and roof objects on the one hand, and a
machine learning model based on structured building features to predict roof
pitch on the other hand. We then compute the azimuth and maximum number of
solar panels that can be installed on a rooftop with geometric approaches.
Finally, we compute precise shading masks and combine them with solar
irradiation data that enables us to estimate the yearly solar potential of a
rooftop."
84,"['cs.CV', 'cs.LG', 'q-bio.NC']",Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention,"Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at https://github.com/egyptdj/stagin"
85,"['stat.ML', 'cs.AI', 'cs.LG']",Scalable Gaussian Processes on Discrete Domains,"Kernel methods on discrete domains have shown great promise for many
challenging data types, for instance, biological sequence data and molecular
structure data. Scalable kernel methods like Support Vector Machines may offer
good predictive performances but do not intrinsically provide uncertainty
estimates. In contrast, probabilistic kernel methods like Gaussian Processes
offer uncertainty estimates in addition to good predictive performance but fall
short in terms of scalability. While the scalability of Gaussian processes can
be improved using sparse inducing point approximations, the selection of these
inducing points remains challenging. We explore different techniques for
selecting inducing points on discrete domains, including greedy selection,
determinantal point processes, and simulated annealing. We find that simulated
annealing, which can select inducing points that are not in the training set,
can perform competitively with support vector machines and full Gaussian
processes on synthetic data, as well as on challenging real-world DNA sequence
data."
86,"['cs.LG', 'cs.AI', 'quant-ph', '68Txx, 81Pxx', 'I.2']",An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics,"This paper presents Sparse Tensor Classifier (STC), a supervised
classification algorithm for categorical data inspired by the notion of
superposition of states in quantum physics. By regarding an observation as a
superposition of features, we introduce the concept of wave-particle duality in
machine learning and propose a generalized framework that unifies the classical
and the quantum probability. We show that STC possesses a wide range of
desirable properties not available in most other machine learning methods but
it is at the same time exceptionally easy to comprehend and use. Empirical
evaluation of STC on structured data and text classification demonstrates that
our methodology achieves state-of-the-art performances compared to both
standard classifiers and deep learning, at the additional benefit of requiring
minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides
a native explanation of its predictions both for single instances and for each
target label globally."
87,"['cs.LG', 'cs.AI']",Dynamic Filters in Graph Convolutional Neural Networks,"Over the last few years, we have seen increasing data generated from
non-Euclidean domains, which are usually represented as graphs with complex
relationships, and Graph Neural Networks (GNN) have gained a high interest
because of their potential in processing graph-structured data. In particular,
there is a strong interest in exploring the possibilities in performing
convolution on graphs using an extension of the GNN architecture, generally
referred to as Graph Convolutional Neural Networks (GCNN). Convolution on
graphs has been achieved mainly in two forms: spectral and spatial
convolutions. Due to the higher flexibility in exploring and exploiting the
graph structure of data, recently, there is an increasing interest in
investigating the possibilities that the spatial approach can offer. The idea
of finding a way to adapt the network behaviour to the inputs they process to
maximize the total performances has aroused much interest in the neural
networks literature over the years. This paper presents a novel method to adapt
the behaviour of a GCNN to the input proposing two ways to perform spatial
convolution on graphs using input-based filters which are dynamically
generated. Our model also investigates the problem of discovering and refining
relations among nodes. The experimental assessment confirms the capabilities of
the proposed approach, which achieves satisfying results using simple
architectures with a low number of filters."
88,['cs.CV'],Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution,"Graph convolutional networks (GCNs) have achieved great success in dealing
with data of non-Euclidean structures. Their success directly attributes to
fitting graph structures effectively to data such as in social media and
knowledge databases. For image processing applications, the use of graph
structures and GCNs have not been fully explored. In this paper, we propose a
novel encoder-decoder network with added graph convolutions by converting
feature maps to vertexes of a pre-generated graph to synthetically construct
graph-structured data. By doing this, we inexplicitly apply graph Laplacian
regularization to the feature maps, making them more structured. The
experiments show that it significantly boosts performance for image restoration
tasks, including deblurring and super-resolution. We believe it opens up
opportunities for GCN-based approaches in more applications."
89,"['cs.LG', 'cs.CR', 'cs.DB']",rx-anon -- A Novel Approach on the De-Identification of Heterogeneous Data based on a Modified Mondrian Algorithm,"Traditional approaches for data anonymization consider relational data and
textual data independently. We propose rx-anon, an anonymization approach for
heterogeneous semi-structured documents composed of relational and textual
attributes. We map sensitive terms extracted from the text to the structured
data. This allows us to use concepts like k-anonymity to generate a joined,
privacy-preserved version of the heterogeneous data input. We introduce the
concept of redundant sensitive information to consistently anonymize the
heterogeneous data. To control the influence of anonymization over unstructured
textual data versus structured data attributes, we introduce a modified,
parameterized Mondrian algorithm. The parameter $\lambda$ allows to give
different weight on the relational and textual attributes during the
anonymization process. We evaluate our approach with two real-world datasets
using a Normalized Certainty Penalty score, adapted to the problem of jointly
anonymizing relational and textual data. The results show that our approach is
capable of reducing information loss by using the tuning parameter to control
the Mondrian partitioning while guaranteeing k-anonymity for relational
attributes as well as for sensitive terms. As rx-anon is a framework approach,
it can be reused and extended by other anonymization algorithms, privacy
models, and textual similarity metrics."
90,"['cs.LG', 'math.AT', 'stat.ML']",Graph Filtration Learning,"We propose an approach to learning with graph-structured data in the problem
domain of graph classification. In particular, we present a novel type of
readout operation to aggregate node features into a graph-level representation.
To this end, we leverage persistent homology computed via a real-valued,
learnable, filter function. We establish the theoretical foundation for
differentiating through the persistent homology computation. Empirically, we
show that this type of readout operation compares favorably to previous
techniques, especially when the graph connectivity structure is informative for
the learning problem."
91,"['stat.ML', 'cs.LG', 'math.OC']",Deep Networks and the Multiple Manifold Problem,"We study the multiple manifold problem, a binary classification task modeled
on applications in machine vision, in which a deep fully-connected neural
network is trained to separate two low-dimensional submanifolds of the unit
sphere. We provide an analysis of the one-dimensional case, proving for a
simple manifold configuration that when the network depth $L$ is large relative
to certain geometric and statistical properties of the data, the network width
$n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d.
samples from the manifolds is polynomial in $L$, randomly-initialized gradient
descent rapidly learns to classify the two manifolds perfectly with high
probability. Our analysis demonstrates concrete benefits of depth and width in
the context of a practically-motivated model problem: the depth acts as a
fitting resource, with larger depths corresponding to smoother networks that
can more readily separate the class manifolds, and the width acts as a
statistical resource, enabling concentration of the randomly-initialized
network and its gradients. The argument centers around the neural tangent
kernel and its role in the nonasymptotic analysis of training overparameterized
neural networks; to this literature, we contribute essentially optimal rates of
concentration for the neural tangent kernel of deep fully-connected networks,
requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform
concentration of the initial kernel over a $d_0$-dimensional submanifold of the
unit sphere $\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for
establishing generalization of networks trained in the NTK regime with
structured data. The proof makes heavy use of martingale concentration to
optimally treat statistical dependencies across layers of the initial random
network. This approach should be of use in establishing similar results for
other network architectures."
92,"['cs.CV', 'eess.IV']",VoxelContext-Net: An Octree based Framework for Point Cloud Compression,"In this paper, we propose a two-stage deep learning framework called
VoxelContext-Net for both static and dynamic point cloud compression. Taking
advantages of both octree based methods and voxel based schemes, our approach
employs the voxel context to compress the octree structured data. Specifically,
we first extract the local voxel representation that encodes the spatial
neighbouring context information for each node in the constructed octree. Then,
in the entropy coding stage, we propose a voxel context based deep entropy
model to compress the symbols of non-leaf nodes in a lossless way. Furthermore,
for dynamic point cloud compression, we additionally introduce the local voxel
representations from the temporal neighbouring point clouds to exploit temporal
dependency. More importantly, to alleviate the distortion from the octree
construction procedure, we propose a voxel context based 3D coordinate
refinement method to produce more accurate reconstructed point cloud at the
decoder side, which is applicable to both static and dynamic point cloud
compression. The comprehensive experiments on both static and dynamic point
cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate
the effectiveness of our newly proposed method VoxelContext-Net for 3D point
cloud geometry compression."
93,"['cs.LG', 'stat.ML']",Ripple Walk Training: A Subgraph-based training framework for Large and Deep Graph Neural Network,"Graph neural networks (GNNs) have achieved outstanding performance in
learning graph-structured data and various tasks. However, many current GNNs
suffer from three common problems when facing large-size graphs or using a
deeper structure: neighbors explosion, node dependence, and oversmoothing. Such
problems attribute to the data structures of the graph itself or the designing
of the multi-layers GNNs framework, and can lead to low training efficiency and
high space complexity. To deal with these problems, in this paper, we propose a
general subgraph-based training framework, namely Ripple Walk Training (RWT),
for deep and large graph neural networks. RWT samples subgraphs from the full
graph to constitute a mini-batch, and the full GNN is updated based on the
mini-batch gradient. We analyze the high-quality subgraphs to train GNNs in a
theoretical way. A novel sampling method Ripple Walk Sampler works for sampling
these high-quality subgraphs to constitute the mini-batch, which considers both
the randomness and connectivity of the graph-structured data. Extensive
experiments on different sizes of graphs demonstrate the effectiveness and
efficiency of RWT in training various GNNs (GCN & GAT)."
94,"['cs.LG', 'cs.SI', 'stat.ML']",Lifelong Learning of Graph Neural Networks for Open-World Node Classification,"Graph neural networks (GNNs) have emerged as the standard method for numerous
tasks on graph-structured data such as node classification. However, real-world
graphs are often evolving over time and even new classes may arise. We model
these challenges as an instance of lifelong learning, in which a learner faces
a sequence of tasks and may take over knowledge acquired in past tasks. Such
knowledge may be stored explicitly as historic data or implicitly within model
parameters. In this work, we systematically analyze the influence of implicit
and explicit knowledge. Therefore, we present an incremental training method
for lifelong learning on graphs and introduce a new measure based on
$k$-neighborhood time differences to address variances in the historic data. We
apply our training method to five representative GNN architectures and evaluate
them on three new lifelong node classification datasets. Our results show that
no more than 50% of the GNN's receptive field is necessary to retain at least
95% accuracy compared to training over the complete history of the graph data.
Furthermore, our experiments confirm that implicit knowledge becomes more
important when fewer explicit knowledge is available."
95,"['cs.LG', 'cs.AI', 'cs.AR']",VersaGNN: a Versatile accelerator for Graph neural networks,"\textit{Graph Neural Network} (GNN) is a promising approach for analyzing
graph-structured data that tactfully captures their dependency information via
node-level message passing. It has achieved state-of-the-art performances in
many tasks, such as node classification, graph matching, clustering, and graph
generation. As GNNs operate on non-Euclidean data, their irregular data access
patterns cause considerable computational costs and overhead on conventional
architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid
computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase
performs vector additions where vectors are fetched with irregular strides. The
\textit{Transformation} (or \textit{Node Embedding}) phase can be either dense
or sparse-dense matrix multiplication. In this work, We propose
\textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware
accelerator that unifies dense and sparse matrix multiplication. By applying
this single optimized systolic array to both aggregation and transformation
phases, we have significantly reduced chip sizes and energy consumption. We
then divide the computing engine into blocked systolic arrays to support the
\textit{Strassen}'s algorithm for dense matrix multiplication, dramatically
scaling down the number of multiplications and enabling high-throughput
computation of GNNs. To balance the workload of sparse-dense matrix
multiplication, we also introduced a greedy algorithm to combine sparse
sub-matrices of compressed format into condensed ones to reduce computational
cycles. Compared with current state-of-the-art GNN software frameworks,
\textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$
energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy
reduction on GPU."
96,['cs.LG'],An Energy-Based View of Graph Neural Networks,"Graph neural networks are a popular variant of neural networks that work with
graph-structured data. In this work, we consider combining graph neural
networks with the energy-based view of Grathwohl et al. (2019) with the aim of
obtaining a more robust classifier. We successfully implement this framework by
proposing a novel method to ensure generation over features as well as the
adjacency matrix and evaluate our method against the standard graph
convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach
obtains comparable discriminative performance while improving robustness,
opening promising new directions for future research for energy-based graph
neural networks."
97,['cs.LG'],Network Embedding via Deep Prediction Model,"Network-structured data becomes ubiquitous in daily life and is growing at a
rapid pace. It presents great challenges to feature engineering due to the high
non-linearity and sparsity of the data. The local and global structure of the
real-world networks can be reflected by dynamical transfer behaviors among
nodes. This paper proposes a network embedding framework to capture the
transfer behaviors on structured networks via deep prediction models. We first
design a degree-weight biased random walk model to capture the transfer
behaviors on the network. Then a deep network embedding method is introduced to
preserve the transfer possibilities among the nodes. A network structure
embedding layer is added into conventional deep prediction models, including
Long Short-Term Memory Network and Recurrent Neural Network, to utilize the
sequence prediction ability. To keep the local network neighborhood, we further
perform a Laplacian supervised space optimization on the embedding feature
representations. Experimental studies are conducted on various datasets
including social networks, citation networks, biomedical network, collaboration
network and language network. The results show that the learned representations
can be effectively used as features in a variety of tasks, such as clustering,
visualization, classification, reconstruction and link prediction, and achieve
promising performance compared with state-of-the-arts."
98,"['cs.CV', 'cs.DC', 'cs.MM']",VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the Edge for the Internet of Multimedia Things,"Efficient video processing is a critical component in many IoMT applications
to detect events of interest. Presently, many window optimization techniques
have been proposed in event processing with an underlying assumption that the
incoming stream has a structured data model. Videos are highly complex due to
the lack of any underlying structured data model. Video stream sources such as
CCTV cameras and smartphones are resource-constrained edge nodes. At the same
time, video content extraction is expensive and requires computationally
intensive Deep Neural Network (DNN) models that are primarily deployed at
high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage
allied windowing approach to accelerate video event analytics in an edge-cloud
paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the
query and resource-aware optimization for state-based complex event matching.
VID-WIN exploits the video content and DNN input knobs to accelerate the video
inference process across nodes. The paper proposes a novel content-driven
micro-batch resizing, queryaware caching and micro-batch based utility
filtering strategy of video frames under resource-constrained edge nodes to
improve the overall system throughput, latency, and network usage. Extensive
evaluations are performed over five real-world datasets. The experimental
results show that VID-WIN video event matching achieves ~2.3X higher throughput
with minimal latency and ~99% bandwidth reduction compared to other baselines
while maintaining query-level accuracy and resource bounds."
99,"['cs.LG', 'cs.DC']",Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural Networks,"Graph neural networks (GNNs), an emerging deep learning model class, can
extract meaningful representations from highly expressive graph-structured data
and are therefore gaining popularity for wider ranges of applications. However,
current GNNs suffer from the poor performance of their sparse-dense matrix
multiplication (SpMM) operator, even when using powerful GPUs. Our analysis
shows that 95% of the inference time could be spent on SpMM when running
popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance
bottleneck hinders GNNs' applicability to large-scale problems or the
development of more sophisticated GNN models. To address this inference time
bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and
codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit
into GPU's shared memory. It thus reduces the computation cost and improves
SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with
a popular GNN framework, DGL, and tested it using representative GNN models and
datasets. Our results show that ES-SpMM outperforms the highly optimized
cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with
less than a 1% accuracy loss."
100,"['cs.LG', 'quant-ph', 'stat.ML']",Tensor Networks for Probabilistic Sequence Modeling,"Tensor networks are a powerful modeling framework developed for computational
many-body physics, which have only recently been applied within machine
learning. In this work we utilize a uniform matrix product state (u-MPS) model
for probabilistic modeling of sequence data. We first show that u-MPS enable
sequence-level parallelism, with length-n sequences able to be evaluated in
depth O(log n). We then introduce a novel generative algorithm giving trained
u-MPS the ability to efficiently sample from a wide variety of conditional
distributions, each one defined by a regular expression. Special cases of this
algorithm correspond to autoregressive and fill-in-the-blank sampling, but more
complex regular expressions permit the generation of richly structured data in
a manner that has no direct analogue in neural generative models. Experiments
on sequence modeling with synthetic and real text data show u-MPS outperforming
a variety of baselines and effectively generalizing their predictions in the
presence of limited data."
101,"['cs.LG', 'physics.app-ph', 'physics.chem-ph', 'physics.optics']",Scalable and Flexible Deep Bayesian Optimization with Auxiliary Information for Scientific Problems,"Bayesian optimization (BO) is a popular paradigm for global optimization of
expensive black-box functions, but there are many domains where the function is
not completely black-box. The data may have some known structure, e.g.
symmetries, and the data generation process can yield useful intermediate or
auxiliary information in addition to the value of the optimization objective.
However, surrogate models traditionally employed in BO, such as Gaussian
Processes (GPs), scale poorly with dataset size and struggle to incorporate
known structure or auxiliary information. Instead, we propose performing BO on
complex, structured problems by using Bayesian Neural Networks (BNNs), a class
of scalable surrogate models that have the representation power and flexibility
to handle structured data and exploit auxiliary information. We demonstrate BO
on a number of realistic problems in physics and chemistry, including topology
optimization of photonic crystal materials using convolutional neural networks,
and chemical property optimization of molecules using graph neural networks. On
these complex tasks, we show that BNNs often outperform GPs as surrogate models
for BO in terms of both sampling efficiency and computational cost."
102,"['stat.ML', 'cs.LG', 'stat.CO', '62G08']",Space Partitioning and Regression Mode Seeking via a Mean-Shift-Inspired Algorithm,"The mean shift (MS) algorithm is a nonparametric method used to cluster
sample points and find the local modes of kernel density estimates, using an
idea based on iterative gradient ascent. In this paper we develop a
mean-shift-inspired algorithm to estimate the modes of regression functions and
partition the sample points in the input space. We prove convergence of the
sequences generated by the algorithm and derive the non-asymptotic rates of
convergence of the estimated local modes for the underlying regression model.
We also demonstrate the utility of the algorithm for data-enabled discovery
through an application on biomolecular structure data. An extension to subspace
constrained mean shift (SCMS) algorithm used to extract ridges of regression
functions is briefly discussed."
103,['cs.LG'],Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning,"Recently, there has been great success in applying deep neural networks on
graph structured data. Most work, however, focuses on either node- or
graph-level supervised learning, such as node, link or graph classification or
node-level unsupervised learning (e.g. node clustering). Despite its wide range
of possible applications, graph-level unsupervised learning has not received
much attention yet. This might be mainly attributed to the high representation
complexity of graphs, which can be represented by n! equivalent adjacency
matrices, where n is the number of nodes. In this work we address this issue by
proposing a permutation-invariant variational autoencoder for graph structured
data. Our proposed model indirectly learns to match the node ordering of input
and output graph, without imposing a particular node ordering or performing
expensive graph matching. We demonstrate the effectiveness of our proposed
model on various graph reconstruction and generation tasks and evaluate the
expressive power of extracted representations for downstream graph-level
classification and regression."
104,['cs.LG'],Ranking Structured Objects with Graph Neural Networks,"Graph neural networks (GNNs) have been successfully applied in many
structured data domains, with applications ranging from molecular property
prediction to the analysis of social networks. Motivated by the broad
applicability of GNNs, we propose the family of so-called RankGNNs, a
combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are
trained with a set of pair-wise preferences between graphs, suggesting that one
of them is preferred over the other. One practical application of this problem
is drug screening, where an expert wants to find the most promising molecules
in a large collection of drug candidates. We empirically demonstrate that our
proposed pair-wise RankGNN approach either significantly outperforms or at
least matches the ranking performance of the naive point-wise baseline
approach, in which the LtR problem is solved via GNN-based graph regression."
105,"['cs.LG', 'cs.NI', 'cs.SI']",Hierarchical Adaptive Pooling by Capturing High-order Dependency for Graph Representation Learning,"Graph neural networks (GNN) have been proven to be mature enough for handling
graph-structured data on node-level graph representation learning tasks.
However, the graph pooling technique for learning expressive graph-level
representation is critical yet still challenging. Existing pooling methods
either struggle to capture the local substructure or fail to effectively
utilize high-order dependency, thus diminishing the expression capability. In
this paper we propose HAP, a hierarchical graph-level representation learning
framework, which is adaptively sensitive to graph structures, i.e., HAP
clusters local substructures incorporating with high-order dependencies. HAP
utilizes a novel cross-level attention mechanism MOA to naturally focus more on
close neighborhood while effectively capture higher-order dependency that may
contain crucial information. It also learns a global graph content GCont that
extracts the graph pattern properties to make the pre- and post-coarsening
graph content maintain stable, thus providing global guidance in graph
coarsening. This novel innovation also facilitates generalization across graphs
with the same form of features. Extensive experiments on fourteen datasets show
that HAP significantly outperforms twelve popular graph pooling methods on
graph classification task with an maximum accuracy improvement of 22.79%, and
exceeds the performance of state-of-the-art graph matching and graph similarity
learning algorithms by over 3.5% and 16.7%."
106,['cs.LG'],Self-supervised Auxiliary Learning for Graph Neural Networks via Meta-Learning,"In recent years, graph neural networks (GNNs) have been widely adopted in the
representation learning of graph-structured data and provided state-of-the-art
performance in various applications such as link prediction, node
classification, and recommendation. Motivated by recent advances of
self-supervision for representation learning in natural language processing and
computer vision, self-supervised learning has been recently studied to leverage
unlabeled graph-structured data. However, employing self-supervision tasks as
auxiliary tasks to assist a primary task has been less explored in the
literature on graphs. In this paper, we propose a novel self-supervised
auxiliary learning framework to effectively learn graph neural networks.
Moreover, this work is the first study showing that a meta-path prediction is
beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our
method is learning to learn a primary task with various auxiliary tasks to
improve generalization performance. The proposed method identifies an effective
combination of auxiliary tasks and automatically balances them to improve the
primary task. Our methods can be applied to any graph neural network in a
plug-in manner without manual labeling or additional data. Also, it can be
extended to any other auxiliary tasks. Our experiments demonstrate that the
proposed method consistently improves the performance of node classification
and link prediction."
107,"['cs.LG', 'stat.ML']",Sparse Graph Attention Networks,"Graph Neural Networks (GNNs) have proved to be an effective representation
learning framework for graph-structured data, and have achieved
state-of-the-art performance on many practical predictive tasks, such as node
classification, link prediction and graph classification. Among the variants of
GNNs, Graph Attention Networks (GATs) learn to assign dense attention
coefficients over all neighbors of a node for feature aggregation, and improve
the performance of many graph learning tasks. However, real-world graphs are
often very large and noisy, and GATs are prone to overfitting if not
regularized properly. Even worse, the local aggregation mechanism of GATs may
fail on disassortative graphs, where nodes within local neighborhood provide
more noise than useful information for feature aggregation. In this paper, we
propose Sparse Graph Attention Networks (SGATs) that learn sparse attention
coefficients under an $L_0$-norm regularization, and the learned sparse
attentions are then used for all GNN layers, resulting in an edge-sparsified
graph. By doing so, we can identify noisy/task-irrelevant edges, and thus
perform feature aggregation on most informative neighbors. Extensive
experiments on synthetic and real-world graph learning benchmarks demonstrate
the superior performance of SGATs. In particular, SGATs can remove about
50\%-80\% edges from large assortative graphs, while retaining similar
classification accuracies. On disassortative graphs, SGATs prune majority of
noisy edges and outperform GATs in classification accuracies by significant
margins. Furthermore, the removed edges can be interpreted intuitively and
quantitatively. To the best of our knowledge, this is the first graph learning
algorithm that shows significant redundancies in graphs and edge-sparsified
graphs can achieve similar or sometimes higher predictive performances than
original graphs."
108,"['cs.LG', 'cs.AI', 'stat.ML']",Graph Neural Networks: A Review of Methods and Applications,"Lots of learning tasks require dealing with graph data which contains rich
relation information among elements. Modeling physics systems, learning
molecular fingerprints, predicting protein interface, and classifying diseases
demand a model to learn from graph inputs. In other domains such as learning
from non-structural data like texts and images, reasoning on extracted
structures (like the dependency trees of sentences and the scene graphs of
images) is an important research topic which also needs graph reasoning models.
Graph neural networks (GNNs) are neural models that capture the dependence of
graphs via message passing between the nodes of graphs. In recent years,
variants of GNNs such as graph convolutional network (GCN), graph attention
network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking
performances on many deep learning tasks. In this survey, we propose a general
design pipeline for GNN models and discuss the variants of each component,
systematically categorize the applications, and propose four open problems for
future research."
109,['cs.CV'],Attentional Graph Neural Network for Parking-slot Detection,"Deep learning has recently demonstrated its promising performance for
vision-based parking-slot detection. However, very few existing methods
explicitly take into account learning the link information of the
marking-points, resulting in complex post-processing and erroneous detection.
In this paper, we propose an attentional graph neural network based
parking-slot detection method, which refers the marking-points in an
around-view image as graph-structured data and utilize graph neural network to
aggregate the neighboring information between marking-points. Without any
manually designed post-processing, the proposed method is end-to-end trainable.
Extensive experiments have been conducted on public benchmark dataset, where
the proposed method achieves state-of-the-art accuracy. Code is publicly
available at \url{https://github.com/Jiaolong/gcn-parking-slot}."
110,"['cs.LG', 'cs.AI']",Graph Contrastive Learning with Augmentations,"Generalizable, transferrable, and robust representation learning on
graph-structured data remains a challenge for current graph neural networks
(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)
for image data, self-supervised learning and pre-training are less explored for
GNNs. In this paper, we propose a graph contrastive learning (GraphCL)
framework for learning unsupervised representations of graph data. We first
design four types of graph augmentations to incorporate various priors. We then
systematically study the impact of various combinations of graph augmentations
on multiple datasets, in four different settings: semi-supervised,
unsupervised, and transfer learning as well as adversarial attacks. The results
show that, even without tuning augmentation extents nor using sophisticated GNN
architectures, our GraphCL framework can produce graph representations of
similar or better generalizability, transferrability, and robustness compared
to state-of-the-art methods. We also investigate the impact of parameterized
graph augmentation extents and patterns, and observe further performance gains
in preliminary experiments. Our codes are available at
https://github.com/Shen-Lab/GraphCL."
111,"['cs.LG', 'cs.CV']",SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data,"Generative modeling of set-structured data, such as point clouds, requires
reasoning over local and global structures at various scales. However, adopting
multi-scale frameworks for ordinary sequential data to a set-structured data is
nontrivial as it should be invariant to the permutation of its elements. In
this paper, we propose SetVAE, a hierarchical variational autoencoder for sets.
Motivated by recent progress in set encoding, we build SetVAE upon attentive
modules that first partition the set and project the partition back to the
original cardinality. Exploiting this module, our hierarchical VAE learns
latent variables at multiple scales, capturing coarse-to-fine dependency of the
set elements while achieving permutation invariance. We evaluate our model on
point cloud generation task and achieve competitive performance to the prior
arts with substantially smaller model capacity. We qualitatively demonstrate
that our model generalizes to unseen set sizes and learns interesting subset
relations without supervision. Our implementation is available at
https://github.com/jw9730/setvae."
112,"['cs.LG', 'cs.SI']",Hierarchical Graph Capsule Network,"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the
topological information of structured data. However, existing GNNs suffer from
limited capability in capturing the hierarchical graph representation which
plays an important role in graph classification. In this paper, we innovatively
propose hierarchical graph capsule network (HGCN) that can jointly learn node
embeddings and extract graph hierarchies. Specifically, disentangled graph
capsules are established by identifying heterogeneous factors underlying each
node, such that their instantiation parameters represent different properties
of the same entity. To learn the hierarchical representation, HGCN
characterizes the part-whole relationship between lower-level capsules (part)
and higher-level capsules (whole) by explicitly considering the structure
information among the parts. Experimental studies demonstrate the effectiveness
of HGCN and the contribution of each component."
113,"['cs.LG', 'cs.AI']",Spatio-Temporal Sparsification for General Robust Graph Convolution Networks,"Graph Neural Networks (GNNs) have attracted increasing attention due to its
successful applications on various graph-structure data. However, recent
studies have shown that adversarial attacks are threatening the functionality
of GNNs. Although numerous works have been proposed to defend adversarial
attacks from various perspectives, most of them can be robust against the
attacks only on specific scenarios. To address this shortage of robust
generalization, we propose to defend the adversarial attacks on GNN through
applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN
hidden node representation. ST-Sparse is similar to the Dropout regularization
in spirit. Through intensive experiment evaluation with GCN as the target GNN
model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows
the defense performance improvement in most cases, as it can effectively
increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse
illustrates its robust generalization capability by integrating with the
existing defense methods, similar to the integration of Dropout into various
deep learning models as a standard regularization technique; (3) ST-Sparse also
shows its ordinary generalization capability on clean datasets, in that
ST-SparseGCN (the integration of ST-Sparse and the original GCN) even
outperform the original GCN, while the other three representative defense
methods are inferior to the original GCN."
114,"['cs.LG', 'stat.AP']",Understanding Heart-Failure Patients EHR Clinical Features via SHAP Interpretation of Tree-Based Machine Learning Model Predictions,"Heart failure (HF) is a major cause of mortality. Accurately monitoring HF
progress and adjust therapies are critical for improving patient outcomes. An
experienced cardiologist can make accurate HF stage diagnoses based on
combination of symptoms, signs, and lab results from the electronic health
records (EHR) of a patient, without directly measuring heart function. We
examined whether machine learning models, more specifically the XGBoost model,
can accurately predict patient stage based on EHR, and we further applied the
SHapley Additive exPlanations (SHAP) framework to identify informative features
and their interpretations. Our results indicate that based on structured data
from EHR, our models could predict patients' ejection fraction (EF) scores with
moderate accuracy. SHAP analyses identified informative features and revealed
potential clinical subtypes of HF. Our findings provide insights on how to
design computing systems to accurately monitor disease progression of HF
patients through continuously mining patients' EHR data."
115,"['cs.LG', 'cs.AI']",Recognizing Predictive Substructures with Subgraph Information Bottleneck,"The emergence of Graph Convolutional Network (GCN) has greatly boosted the
progress of graph learning. However, two disturbing factors, noise and
redundancy in graph data, and lack of interpretation for prediction results,
impede further development of GCN. One solution is to recognize a predictive
yet compressed subgraph to get rid of the noise and redundancy and obtain the
interpretable part of the graph. This setting of subgraph is similar to the
information bottleneck (IB) principle, which is less studied on
graph-structured data and GCN. Inspired by the IB principle, we propose a novel
subgraph information bottleneck (SIB) framework to recognize such subgraphs,
named IB-subgraph. However, the intractability of mutual information and the
discrete nature of graph data makes the objective of SIB notoriously hard to
optimize. To this end, we introduce a bilevel optimization scheme coupled with
a mutual information estimator for irregular graphs. Moreover, we propose a
continuous relaxation for subgraph selection with a connectivity loss for
stabilization. We further theoretically prove the error bound of our estimation
scheme for mutual information and the noise-invariant nature of IB-subgraph.
Extensive experiments on graph learning and large-scale point cloud tasks
demonstrate the superior property of IB-subgraph."
116,"['cs.LG', 'math.ST', 'stat.TH']",Analyzing the tree-layer structure of Deep Forests,"Random forests on the one hand, and neural networks on the other hand, have
met great success in the machine learning community for their predictive
performance. Combinations of both have been proposed in the literature, notably
leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper,
our aim is not to benchmark DF performances but to investigate instead their
underlying mechanisms. Additionally, DF architecture can be generally
simplified into more simple and computationally efficient shallow forest
networks. Despite some instability, the latter may outperform standard
predictive tree-based methods. We exhibit a theoretical framework in which a
shallow tree network is shown to enhance the performance of classical decision
trees. In such a setting, we provide tight theoretical lower and upper bounds
on its excess risk. These theoretical results show the interest of tree-network
architectures for well-structured data provided that the first layer, acting as
a data encoder, is rich enough."
117,"['cs.LG', 'stat.ML']",CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks,"Graph-structured data are ubiquitous. However, graphs encode diverse types of
information and thus play different roles in data representation. In this
paper, we distinguish the \textit{representational} and the
\textit{correlational} roles played by the graphs in node-level prediction
tasks, and we investigate how Graph Neural Network (GNN) models can effectively
leverage both types of information. Conceptually, the representational
information provides guidance for the model to construct better node features;
while the correlational information indicates the correlation between node
outcomes conditional on node features. Through a simulation study, we find that
many popular GNN models are incapable of effectively utilizing the
correlational information. By leveraging the idea of the copula, a principled
way to describe the dependence among multivariate random variables, we offer a
general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take
a wide range of GNN models as base models and utilize both representational and
correlational information stored in the graphs. Experimental results on two
types of regression tasks verify the effectiveness of the proposed method."
118,"['cs.LG', 'stat.ML']",Graph Convolutional Neural Networks with Node Transition Probability-based Message Passing and DropNode Regularization,"Graph convolutional neural networks (GCNNs) have received much attention
recently, owing to their capability in handling graph-structured data. Among
the existing GCNNs, many methods can be viewed as instances of a neural message
passing motif; features of nodes are passed around their neighbors, aggregated
and transformed to produce better nodes' representations. Nevertheless, these
methods seldom use node transition probabilities, a measure that has been found
useful in exploring graphs. Furthermore, when the transition probabilities are
used, their transition direction is often improperly considered in the feature
aggregation step, resulting in an inefficient weighting scheme. In addition,
although a great number of GCNN models with increasing level of complexity have
been introduced, the GCNNs often suffer from over-fitting when being trained on
small graphs. Another issue of the GCNNs is over-smoothing, which tends to make
nodes' representations indistinguishable. This work presents a new method to
improve the message passing process based on node transition probabilities by
properly considering the transition direction, leading to a better weighting
scheme in nodes' features aggregation compared to the existing counterpart.
Moreover, we propose a novel regularization method termed DropNode to address
the over-fitting and over-smoothing issues simultaneously. DropNode randomly
discards part of a graph, thus it creates multiple deformed versions of the
graph, leading to data augmentation regularization effect. Additionally,
DropNode lessens the connectivity of the graph, mitigating the effect of
over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets
for node and graph classification tasks demonstrate the effectiveness of the
proposed methods in comparison with the state of the art."
119,"['cs.LG', 'stat.ML']",Degree-Quant: Quantization-Aware Training for Graph Neural Networks,"Graph neural networks (GNNs) have demonstrated strong performance on a wide
variety of tasks due to their ability to model non-uniform structured data.
Despite their promise, there exists little research exploring methods to make
them more efficient at inference time. In this work, we explore the viability
of training quantized GNNs, enabling the usage of low precision integer
arithmetic during inference. We identify the sources of error that uniquely
arise when attempting to quantize GNNs, and propose an architecturally-agnostic
method, Degree-Quant, to improve performance over existing quantization-aware
training baselines commonly used on other architectures, such as CNNs. We
validate our method on six datasets and show, unlike previous attempts, that
models generalize to unseen graphs. Models trained with Degree-Quant for INT8
quantization perform as well as FP32 models in most cases; for INT4 models, we
obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups
on CPU when using INT8 arithmetic."
120,"['cs.LG', 'cs.AI', 'stat.ML']",Learning to Represent and Predict Sets with Deep Neural Networks,"In this thesis, we develop various techniques for working with sets in
machine learning. Each input or output is not an image or a sequence, but a
set: an unordered collection of multiple objects, each object described by a
feature vector. Their unordered nature makes them suitable for modeling a wide
variety of data, ranging from objects in images to point clouds to graphs. Deep
learning has recently shown great success on other types of structured data, so
we aim to build the necessary structures for sets into deep neural networks.
  The first focus of this thesis is the learning of better set representations
(sets as input). Existing approaches have bottlenecks that prevent them from
properly modeling relations between objects within the set. To address this
issue, we develop a variety of techniques for different scenarios and show that
alleviating the bottleneck leads to consistent improvements across many
experiments.
  The second focus of this thesis is the prediction of sets (sets as output).
Current approaches do not take the unordered nature of sets into account
properly. We determine that this results in a problem that causes discontinuity
issues with many set prediction tasks and prevents them from learning some
extremely simple datasets. To avoid this problem, we develop two models that
properly take the structure of sets into account. Various experiments show that
our set prediction techniques can significantly benefit over existing
approaches."
121,['cs.LG'],Set Representation Learning with Generalized Sliced-Wasserstein Embeddings,"An increasing number of machine learning tasks deal with learning
representations from set-structured data. Solutions to these problems involve
the composition of permutation-equivariant modules (e.g., self-attention, or
individual processing via feed-forward neural networks) and
permutation-invariant modules (e.g., global average pooling, or pooling by
multi-head attention). In this paper, we propose a geometrically-interpretable
framework for learning representations from set-structured data, which is
rooted in the optimal mass transportation problem. In particular, we treat
elements of a set as samples from a probability measure and propose an exact
Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn
from set-structured data effectively. We evaluate our proposed framework on
multiple supervised and unsupervised set learning tasks and demonstrate its
superiority over state-of-the-art set representation learning approaches."
122,['cs.LG'],Unified Robust Training for Graph NeuralNetworks against Label Noise,"Graph neural networks (GNNs) have achieved state-of-the-art performance for
node classification on graphs. The vast majority of existing works assume that
genuine node labels are always provided for training. However, there has been
very little research effort on how to improve the robustness of GNNs in the
presence of label noise. Learning with label noise has been primarily studied
in the context of image classification, but these techniques cannot be directly
applied to graph-structured data, due to two major challenges -- label sparsity
and label dependency -- faced by learning on graphs. In this paper, we propose
a new framework, UnionNET, for learning with noisy labels on graphs under a
semi-supervised setting. Our approach provides a unified solution for robustly
training GNNs and performing label correction simultaneously. The key idea is
to perform label aggregation to estimate node-level class probability
distributions, which are used to guide sample reweighting and label correction.
Compared with existing works, UnionNET has two appealing advantages. First, it
requires no extra clean supervision, or explicit estimation of the noise
transition matrix. Second, a unified learning framework is proposed to robustly
train GNNs in an end-to-end manner. Experimental results show that our proposed
approach: (1) is effective in improving model robustness against different
types and levels of label noise; (2) yields significant improvements over
state-of-the-art baselines."
123,"['cs.LG', 'cs.SE']",Learning Structural Edits via Incremental Tree Transformations,"While most neural generative models generate outputs in a single pass, the
human creative process is usually one of iterative building and refinement.
Recent work has proposed models of editing processes, but these mostly focus on
editing sequential data and/or only model a single editing pass. In this paper,
we present a generic model for incremental editing of structured data (i.e.,
""structural edits""). Particularly, we focus on tree-structured data, taking
abstract syntax trees of computer programs as our canonical example. Our editor
learns to iteratively generate tree edits (e.g., deleting or adding a subtree)
and applies them to the partially edited data, thereby the entire editing
process can be formulated as consecutive, incremental tree transformations. To
show the unique benefits of modeling tree edits directly, we further propose a
novel edit encoder for learning to represent edits, as well as an imitation
learning method that allows the editor to be more robust. We evaluate our
proposed editor on two source code edit datasets, where results show that, with
the proposed edit encoder, our editor significantly improves accuracy over
previous approaches that generate the edited program directly in one pass.
Finally, we demonstrate that training our editor to imitate experts and correct
its mistakes dynamically can further improve its performance."
124,"['cs.LG', 'cs.SI']",Deep Graph Structure Learning for Robust Representations: A Survey,"Graph Neural Networks (GNNs) are widely used for analyzing graph-structured
data. Most GNN methods are highly sensitive to the quality of graph structures
and usually require a perfect graph structure for learning informative
embeddings. However, the pervasiveness of noise in graphs necessitates learning
robust representations for real-world problems. To improve the robustness of
GNN models, many studies have been proposed around the central concept of Graph
Structure Learning (GSL), which aims to jointly learn an optimized graph
structure and corresponding representations. Towards this end, in the presented
survey, we broadly review recent progress of GSL methods for learning robust
representations. Specifically, we first formulate a general paradigm of GSL,
and then review state-of-the-art methods classified by how they model graph
structures, followed by applications that incorporate the idea of GSL in other
graph tasks. Finally, we point out some issues in current studies and discuss
future directions."
125,"['cs.LG', 'cs.AI']",Wide Graph Neural Networks: Aggregation Provably Leads to Exponentially Trainability Loss,"Graph convolutional networks (GCNs) and their variants have achieved great
success in dealing with graph-structured data. However, it is well known that
deep GCNs will suffer from over-smoothing problem, where node representations
tend to be indistinguishable as we stack up more layers. Although extensive
research has confirmed this prevailing understanding, few theoretical analyses
have been conducted to study the expressivity and trainability of deep GCNs. In
this work, we demonstrate these characterizations by studying the Gaussian
Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an
infinitely-wide GCN, corresponding to the analysis on expressivity and
trainability, respectively. We first prove the expressivity of infinitely-wide
GCNs decaying at an exponential rate by applying the mean-field theory on GPK.
Besides, we formulate the asymptotic behaviors of GNTK in the large depth,
which enables us to reveal the dropping trainability of wide and deep GCNs at
an exponential rate. Additionally, we extend our theoretical framework to
analyze residual connection-resemble techniques. We found that these techniques
can mildly mitigate exponential decay, but they failed to overcome it
fundamentally. Finally, all theoretical results in this work are corroborated
experimentally on a variety of graph-structured datasets."
126,['cs.LG'],Mini-Batch Consistent Slot Set Encoder for Scalable Set Encoding,"Most existing set encoding algorithms operate under the assumption that all
the elements of the set are accessible during training and inference.
Additionally, it is assumed that there are enough computational resources
available for concurrently processing sets of large cardinality. However, both
assumptions fail when the cardinality of the set is prohibitively large such
that we cannot even load the set into memory. In more extreme cases, the set
size could be potentially unlimited, and the elements of the set could be given
in a streaming manner, where the model receives subsets of the full set data at
irregular intervals. To tackle such practical challenges in large-scale set
encoding, we go beyond the usual constraints of invariance and equivariance and
introduce a new property termed Mini-Batch Consistency that is required for
large scale mini-batch set encoding. We present a scalable and efficient set
encoding mechanism that is amenable to mini-batch processing with respect to
set elements and capable of updating set representations as more data arrives.
The proposed method respects the required symmetries of invariance and
equivariance as well as being Mini-Batch Consistent for random partitions of
the input set. We perform extensive experiments and show that our method is
computationally efficient and results in rich set encoding representations for
set-structured data."
127,"['cs.LG', 'stat.ML']",Implicit Kernel Attention,"\textit{Attention} computes the dependency between representations, and it
encourages the model to focus on the important selective features.
Attention-based models, such as Transformer and graph attention network (GAT),
are widely utilized for sequential data and graph-structured data. This paper
suggests a new interpretation and generalized structure of the attention in
Transformer and GAT. For the attention in Transformer and GAT, we derive that
the attention is a product of two parts: 1) the RBF kernel to measure the
similarity of two instances and 2) the exponential of $L^{2}$ norm to compute
the importance of individual instances. From this decomposition, we generalize
the attention in three ways. First, we propose implicit kernel attention with
an implicit kernel function instead of manual kernel selection. Second, we
generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to
structured multi-head attention. Our generalized attention shows better
performance on classification, translation, and regression tasks."
128,"['cs.LG', 'stat.ML']",Labeled Graph Generative Adversarial Networks,"As a new approach to train generative models, \emph{generative adversarial
networks} (GANs) have achieved considerable success in image generation. This
framework has also recently been applied to data with graph structures. We
propose labeled-graph generative adversarial networks (LGGAN) to train deep
generative models for graph-structured data with node labels. We test the
approach on various types of graph datasets, such as collections of citation
networks and protein graphs. Experiment results show that our model can
generate diverse labeled graphs that match the structural characteristics of
the training data and outperforms all alternative approaches in quality and
generality. To further evaluate the quality of the generated graphs, we use
them on a downstream task of graph classification, and the results show that
LGGAN can faithfully capture the important aspects of the graph structure."
129,"['cs.LG', 'stat.ML']",CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting,"This paper introduces CloudLSTM, a new branch of recurrent neural models
tailored to forecasting over data streams generated by geospatial point-cloud
sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the
core component of CloudLSTMs, which performs convolution directly over
point-clouds and extracts local spatial features from sets of neighboring
points that surround different elements of the input. This operator maintains
the permutation invariance of sequence-to-sequence learning frameworks, while
representing neighboring correlations at each time step -- an important aspect
in spatiotemporal predictive learning. The DConv operator resolves the
grid-structural data requirements of existing spatiotemporal forecasting models
and can be easily plugged into traditional LSTM architectures with
sequence-to-sequence learning and attention mechanisms. We apply our proposed
architecture to two representative, practical use cases that involve
point-cloud streams, i.e., mobile service traffic forecasting and air quality
indicator forecasting. Our results, obtained with real-world datasets collected
in diverse scenarios for each use case, show that CloudLSTM delivers accurate
long-term predictions, outperforming a variety of competitor neural network
models."
130,"['cs.LG', 'cs.AI', 'cs.NE']",Incorporating Symbolic Domain Knowledge into Graph Neural Networks,"Our interest is in scientific problems with the following characteristics:
(1) Data are naturally represented as graphs; (2) The amount of data available
is typically small; and (3) There is significant domain-knowledge, usually
expressed in some symbolic form. These kinds of problems have been addressed
effectively in the past by Inductive Logic Programming (ILP), by virtue of 2
important characteristics: (a) The use of a representation language that easily
captures the relation encoded in graph-structured data, and (b) The inclusion
of prior information encoded as domain-specific relations, that can alleviate
problems of data scarcity, and construct new relations. Recent advances have
seen the emergence of deep neural networks specifically developed for
graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have
been shown to be able to handle graph-structured data, less has been done to
investigate the inclusion of domain-knowledge. Here we investigate this aspect
of GNNs empirically by employing an operation we term ""vertex-enrichment"" and
denote the corresponding GNNs as ""VEGNNs"". Using over 70 real-world datasets
and substantial amounts of symbolic domain-knowledge, we examine the result of
vertex-enrichment across 5 different variants of GNNs. Our results provide
support for the following: (a) Inclusion of domain-knowledge by
vertex-enrichment can significantly improve the performance of a GNN. That is,
the performance VEGNNs is significantly better than GNNs across all GNN
variants; (b) The inclusion of domain-specific relations constructed using ILP
improves the performance of VEGNNs, across all GNN variants. Taken together,
the results provide evidence that it is possible to incorporate symbolic domain
knowledge into a GNN, and that ILP can play an important role in providing
high-level relationships that are not easily discovered by a GNN."
131,['cs.LG'],Interpretable Stability Bounds for Spectral Graph Filters,"Graph-structured data arise in a variety of real-world context ranging from
sensor and transportation to biological and social networks. As a ubiquitous
tool to process graph-structured data, spectral graph filters have been used to
solve common tasks such as denoising and anomaly detection, as well as design
deep learning architectures such as graph neural networks. Despite being an
important tool, there is a lack of theoretical understanding of the stability
properties of spectral graph filters, which are important for designing robust
machine learning models. In this paper, we study filter stability and provide a
novel and interpretable upper bound on the change of filter output, where the
bound is expressed in terms of the endpoint degrees of the deleted and newly
added edges, as well as the spatial proximity of those edges. This upper bound
allows us to reason, in terms of structural properties of the graph, when a
spectral graph filter will be stable. We further perform extensive experiments
to verify intuition that can be gained from the bound."
132,"['cs.LG', 'cs.SI']",Ego-based Entropy Measures for Structural Representations on Graphs,"Machine learning on graph-structured data has attracted high research
interest due to the emergence of Graph Neural Networks (GNNs). Most of the
proposed GNNs are based on the node homophily, i.e neighboring nodes share
similar characteristics. However, in many complex networks, nodes that lie to
distant parts of the graph share structurally equivalent characteristics and
exhibit similar roles (e.g chemical properties of distant atoms in a molecule,
type of social network users). A growing literature proposed representations
that identify structurally equivalent nodes. However, most of the existing
methods require high time and space complexity. In this paper, we propose
VNEstruct, a simple approach, based on entropy measures of the neighborhood's
topology, for generating low-dimensional structural representations, that is
time-efficient and robust to graph perturbations. Empirically, we observe that
VNEstruct exhibits robustness on structural role identification tasks.
Moreover, VNEstruct can achieve state-of-the-art performance on graph
classification, without incorporating the graph structure information in the
optimization, in contrast to GNN competitors."
133,"['cs.LG', 'econ.GN', 'q-fin.EC', 'stat.AP']",Integrating Floor Plans into Hedonic Models for Rent Price Appraisal,"Online real estate platforms have become significant marketplaces
facilitating users' search for an apartment or a house. Yet it remains
challenging to accurately appraise a property's value. Prior works have
primarily studied real estate valuation based on hedonic price models that take
structured data into account while accompanying unstructured data is typically
ignored. In this study, we investigate to what extent an automated visual
analysis of apartment floor plans on online real estate platforms can enhance
hedonic rent price appraisal. We propose a tailored two-staged deep learning
approach to learn price-relevant designs of floor plans from historical price
data. Subsequently, we integrate the floor plan predictions into hedonic rent
price models that account for both structural and locational characteristics of
an apartment. Our empirical analysis based on a unique dataset of 9174 real
estate listings suggests that current hedonic models underutilize the available
data. We find that (1) the visual design of floor plans has significant
explanatory power regarding rent prices - even after controlling for structural
and locational apartment characteristics, and (2) harnessing floor plans
results in an up to 10.56% lower out-of-sample prediction error. We further
find that floor plans yield a particularly high gain in prediction performance
for older and smaller apartments. Altogether, our empirical findings contribute
to the existing research body by establishing the link between the visual
design of floor plans and real estate prices. Moreover, our approach has
important implications for online real estate platforms, which can use our
findings to enhance user experience in their real estate listings."
134,"['cs.LG', 'eess.SP', 'stat.ML']",Graph Neural Network for Large-Scale Network Localization,"Graph neural networks (GNNs) are popular to use for classifying structured
data in the context of machine learning. But surprisingly, they are rarely
applied to regression problems. In this work, we adopt GNN for a classic but
challenging nonlinear regression problem, namely the network localization. Our
main findings are in order. First, GNN is potentially the best solution to
large-scale network localization in terms of accuracy, robustness and
computational time. Second, proper thresholding of the communication range is
essential to its superior performance. Simulation results corroborate that the
proposed GNN based method outperforms all state-of-the-art benchmarks by far.
Such inspiring results are theoretically justified in terms of data
aggregation, non-line-of-sight (NLOS) noise removal and low-pass filtering
effect, all affected by the threshold for neighbor selection. Code is available
at https://github.com/Yanzongzi/GNN-For-localization."
135,"['cs.LG', 'cs.MS']",On PyTorch Implementation of Density Estimators for von Mises-Fisher and Its Mixture,"The von Mises-Fisher (vMF) is a well-known density model for directional
random variables. The recent surge of the deep embedding methodologies for
high-dimensional structured data such as images or texts, aimed at extracting
salient directional information, can make the vMF model even more popular. In
this article, we will review the vMF model and its mixture, provide detailed
recipes of how to train the models, focusing on the maximum likelihood
estimators, in Python/PyTorch. In particular, implementation of vMF typically
suffers from the notorious numerical issue of the Bessel function evaluation in
the density normalizer, especially when the dimensionality is high, and we
address the issue using the MPMath library that supports arbitrary precision.
For the mixture learning, we provide both minibatch-based large-scale SGD
learning, as well as the EM algorithm which is a full batch estimator. For each
estimator/methodology, we test our implementation on some synthetic data, while
we also demonstrate the use case in a more realistic scenario of image
clustering. Our code is publicly available in
https://github.com/minyoungkim21/vmf-lib."
136,"['cs.LG', 'stat.ML']",Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous Graphs,"Graph neural networks have shown superior performance in a wide range of
applications providing a powerful representation of graph-structured data.
Recent works show that the representation can be further improved by auxiliary
tasks. However, the auxiliary tasks for heterogeneous graphs, which contain
rich semantic information with various types of nodes and edges, have less
explored in the literature. In this paper, to learn graph neural networks on
heterogeneous graphs we propose a novel self-supervised auxiliary learning
method using meta-paths, which are composite relations of multiple edge types.
Our proposed method is learning to learn a primary task by predicting
meta-paths as auxiliary tasks. This can be viewed as a type of meta-learning.
The proposed method can identify an effective combination of auxiliary tasks
and automatically balance them to improve the primary task. Our methods can be
applied to any graph neural networks in a plug-in manner without manual
labeling or additional data. The experiments demonstrate that the proposed
method consistently improves the performance of link prediction and node
classification on heterogeneous graphs."
137,['cs.LG'],Graph Joint Attention Networks,"Graph attention networks (GATs) have been recognized as powerful tools for
learning in graph structured data. However, how to enable the attention
mechanisms in GATs to smoothly consider both structural and feature information
is still very challenging. In this paper, we propose Graph Joint Attention
Networks (JATs) to address the aforementioned challenge. Different from
previous attention-based graph neural networks (GNNs), JATs adopt novel joint
attention mechanisms which can automatically determine the relative
significance between node features and structural coefficients learned from
graph topology, when computing the attention scores. Therefore, representations
concerning more structural properties can be inferred by JATs. Besides, we
theoretically analyze the expressive power of JATs and further propose an
improved strategy for the joint attention mechanisms that enables JATs to reach
the upper bound of expressive power which every message-passing GNN can
ultimately achieve, i.e., 1-WL test. JATs can thereby be seen as most powerful
message-passing GNNs. The proposed neural architecture has been extensively
tested on widely used benchmarking datasets, and has been compared with
state-of-the-art GNNs for various downstream predictive tasks. Experimental
results show that JATs achieve state-of-the-art performance on all the testing
datasets."
138,['cs.LG'],Lookup subnet based Spatial Graph Convolutional neural Network,"Convolutional Neural Networks(CNNs) has achieved remarkable performance
breakthrough in Euclidean structure data. Recently, aggregation-transformation
based Graph Neural networks(GNNs) gradually produce a powerful performance on
non-Euclidean data. In this paper, we propose a cross-correlation based graph
convolution method allowing to naturally generalize CNNs to non-Euclidean
domains and inherit the excellent natures of CNNs, such as local filters,
parameter sharing, flexible receptive field, etc. Meanwhile, it leverages
dynamically generated convolution kernel and cross-correlation operators to
address the shortcomings of prior methods based on aggregation-transformation
or their approximations. Our method has achieved or matched popular
state-of-the-art results across three established graph benchmarks: the Cora,
Citeseer, and Pubmed citation network datasets."
139,"['cs.CV', 'cs.AI', 'cs.LG']",Deep Learning in Cardiology,"The medical field is creating large amount of data that physicians are unable
to decipher and use efficiently. Moreover, rule-based expert systems are
inefficient in solving complicated medical tasks or for creating insights using
big data. Deep learning has emerged as a more accurate and effective technology
in a wide range of medical problems such as diagnosis, prediction and
intervention. Deep learning is a representation learning method that consists
of layers that transform the data non-linearly, thus, revealing hierarchical
relationships and structures. In this review we survey deep learning
application papers that use structured data, signal and imaging modalities from
cardiology. We discuss the advantages and limitations of applying deep learning
in cardiology that also apply in medicine in general, while proposing certain
directions as the most viable for clinical use."
140,"['cs.LG', 'cs.CL', 'cs.SE', 'stat.ML']",Structured Neural Summarization,"Summarization of long sequences into a concise statement is a core problem in
natural language processing, requiring non-trivial understanding of the input.
Based on the promising results of graph neural networks on highly structured
data, we develop a framework to extend existing sequence encoders with a graph
component that can reason about long-distance relationships in weakly
structured data such as text. In an extensive evaluation, we show that the
resulting hybrid sequence-graph models outperform both pure sequence models as
well as pure graph models on a range of summarization tasks."
141,"['cs.LG', 'cs.AI']",Directed Acyclic Graph Neural Networks,"Graph-structured data ubiquitously appears in science and engineering. Graph
neural networks (GNNs) are designed to exploit the relational inductive bias
exhibited in graphs; they have been shown to outperform other forms of neural
networks in scenarios where structure information supplements node features.
The most common GNN architecture aggregates information from neighborhoods
based on message passing. Its generality has made it broadly applicable. In
this paper, we focus on a special, yet widely used, type of graphs -- DAGs --
and inject a stronger inductive bias -- partial ordering -- into the neural
network design. We propose the \emph{directed acyclic graph neural network},
DAGNN, an architecture that processes information according to the flow defined
by the partial order. DAGNN can be considered a framework that entails earlier
works as special cases (e.g., models for trees and models updating node
representations recurrently), but we identify several crucial components that
prior architectures lack. We perform comprehensive experiments, including
ablation studies, on representative DAG datasets (i.e., source code, neural
architectures, and probabilistic graphical models) and demonstrate the
superiority of DAGNN over simpler DAG architectures as well as general graph
architectures."
142,"['cs.LG', 'math.GR', 'stat.ML']",Deep Autoencoders: From Understanding to Generalization Guarantees,"A big mystery in deep learning continues to be the ability of methods to
generalize when the number of model parameters is larger than the number of
training examples. In this work, we take a step towards a better understanding
of the underlying phenomena of Deep Autoencoders (AEs), a mainstream deep
learning solution for learning compressed, interpretable, and structured data
representations. In particular, we interpret how AEs approximate the data
manifold by exploiting their continuous piecewise affine structure. Our
reformulation of AEs provides new insights into their mapping, reconstruction
guarantees, as well as an interpretation of commonly used regularization
techniques. We leverage these findings to derive two new regularizations that
enable AEs to capture the inherent symmetry in the data. Our regularizations
leverage recent advances in the group of transformation learning to enable AEs
to better approximate the data manifold without explicitly defining the group
underlying the manifold. Under the assumption that the symmetry of the data can
be explained by a Lie group, we prove that the regularizations ensure the
generalization of the corresponding AEs. A range of experimental evaluations
demonstrate that our methods outperform other state-of-the-art regularization
techniques."
143,"['cs.LG', 'cs.SI']",Interpreting and Unifying Graph Neural Networks with An Optimization Framework,"Graph Neural Networks (GNNs) have received considerable attention on
graph-structured data learning for a wide variety of tasks. The well-designed
propagation mechanism which has been demonstrated effective is the most
fundamental part of GNNs. Although most of GNNs basically follow a message
passing manner, litter effort has been made to discover and analyze their
essential relations. In this paper, we establish a surprising connection
between different propagation mechanisms with a unified optimization problem,
showing that despite the proliferation of various GNNs, in fact, their proposed
propagation mechanisms are the optimal solution optimizing a feature fitting
function over a wide class of graph kernels with a graph regularization term.
Our proposed unified optimization framework, summarizing the commonalities
between several of the most representative GNNs, not only provides a
macroscopic view on surveying the relations between different GNNs, but also
further opens up new opportunities for flexibly designing new GNNs. With the
proposed framework, we discover that existing works usually utilize naive graph
convolutional kernels for feature fitting function, and we further develop two
novel objective functions considering adjustable graph kernels showing low-pass
or high-pass filtering capabilities respectively. Moreover, we provide the
convergence proofs and expressive power comparisons for the proposed models.
Extensive experiments on benchmark datasets clearly show that the proposed GNNs
not only outperform the state-of-the-art methods but also have good ability to
alleviate over-smoothing, and further verify the feasibility for designing GNNs
with our unified optimization framework."
144,"['stat.ML', 'cs.LG', 'math.OA']",Reproducing kernel Hilbert C*-module and kernel mean embeddings,"Kernel methods have been among the most popular techniques in machine
learning, where learning tasks are solved using the property of reproducing
kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis
framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean
embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or
vector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and
extract structural properties in multivariate data, functional data and other
structured data. We show a branch of theories for RKHM to apply to data
analysis, including the representer theorem, and the injectivity and
universality of the proposed KME. We also show RKHM generalizes RKHS and vv
RKHS. Then, we provide concrete procedures for employing RKHM and the proposed
KME to data analysis."
145,"['cs.LG', 'cs.NE']",A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for Hyperparameter Optimisation of Graph Neural Networks,"Graph representation of structured data can facilitate the extraction of
stereoscopic features, and it has demonstrated excellent ability when working
with deep learning systems, the so-called Graph Neural Networks (GNNs).
Choosing a promising architecture for constructing GNNs can be transferred to a
hyperparameter optimisation problem, a very challenging task due to the size of
the underlying search space and high computational cost for evaluating
candidate GNNs. To address this issue, this research presents a novel genetic
algorithm with a hierarchical evaluation strategy (HESGA), which combines the
full evaluation of GNNs with a fast evaluation approach. By using full
evaluation, a GNN is represented by a set of hyperparameter values and trained
on a specified dataset, and root mean square error (RMSE) will be used to
measure the quality of the GNN represented by the set of hyperparameter values
(for regression problems). While in the proposed fast evaluation process, the
training will be interrupted at an early stage, the difference of RMSE values
between the starting and interrupted epochs will be used as a fast score, which
implies the potential of the GNN being considered. To coordinate both types of
evaluations, the proposed hierarchical strategy uses the fast evaluation in a
lower level for recommending candidates to a higher level, where the full
evaluation will act as a final assessor to maintain a group of elite
individuals. To validate the effectiveness of HESGA, we apply it to optimise
two types of deep graph neural networks. The experimental results on three
benchmark datasets demonstrate its advantages compared to Bayesian
hyperparameter optimization."
146,"['cs.LG', 'cs.SI']",Generating a Doppelganger Graph: Resembling but Distinct,"Deep generative models, since their inception, have become increasingly more
capable of generating novel and perceptually realistic signals (e.g., images
and sound waves). With the emergence of deep models for graph structured data,
natural interests seek extensions of these generative models for graphs.
Successful extensions were seen recently in the case of learning from a
collection of graphs (e.g., protein data banks), but the learning from a single
graph has been largely under explored. The latter case, however, is important
in practice. For example, graphs in financial and healthcare systems contain so
much confidential information that their public accessibility is nearly
impossible, but open science in these fields can only advance when similar data
are available for benchmarking.
  In this work, we propose an approach to generating a doppelganger graph that
resembles a given one in many graph properties but nonetheless can hardly be
used to reverse engineer the original one, in the sense of a near zero edge
overlap. The approach is an orchestration of graph representation learning,
generative adversarial networks, and graph realization algorithms. Through
comparison with several graph generative models (either parameterized by neural
networks or not), we demonstrate that our result barely reproduces the given
graph but closely matches its properties. We further show that downstream
tasks, such as node classification, on the generated graphs reach similar
performance to the use of the original ones."
147,"['cs.LG', 'cs.NA', 'cs.SI', 'math.NA']",Density of States Graph Kernels,"A fundamental problem on graph-structured data is that of quantifying
similarity between graphs. Graph kernels are an established technique for such
tasks; in particular, those based on random walks and return probabilities have
proven to be effective in wide-ranging applications, from bioinformatics to
social networks to computer vision. However, random walk kernels generally
suffer from slowness and tottering, an effect which causes walks to
overemphasize local graph topology, undercutting the importance of global
structure. To correct for these issues, we recast return probability graph
kernels under the more general framework of density of states -- a framework
which uses the lens of spectral analysis to uncover graph motifs and properties
hidden within the interior of the spectrum -- and use our interpretation to
construct scalable, composite density of states based graph kernels which
balance local and global information, leading to higher classification
accuracies on a host of benchmark datasets."
148,"['cs.LG', 'cs.AI']",Edge-Featured Graph Attention Network,"Lots of neural network architectures have been proposed to deal with learning
tasks on graph-structured data. However, most of these models concentrate on
only node features during the learning process. The edge features, which
usually play a similarly important role as the nodes, are often ignored or
simplified by these models. In this paper, we present edge-featured graph
attention networks, namely EGATs, to extend the use of graph neural networks to
those tasks learning on graphs with both node and edge features. These models
can be regarded as extensions of graph attention networks (GATs). By reforming
the model structure and the learning process, the new models can accept node
and edge features as inputs, incorporate the edge information into feature
representations, and iterate both node and edge features in a parallel but
mutual way. The results demonstrate that our work is highly competitive against
other node classification approaches, and can be well applied in edge-featured
graph learning tasks."
149,"['cs.LG', 'cs.NE', 'cs.SI', 'stat.ML']",Machine Learning on Graphs: A Model and Comprehensive Taxonomy,"There has been a surge of recent interest in learning representations for
graph-structured data. Graph representation learning methods have generally
fallen into three main categories, based on the availability of labeled data.
The first, network embedding (such as shallow graph embedding or graph
auto-encoders), focuses on learning unsupervised representations of relational
structure. The second, graph regularized neural networks, leverages graphs to
augment neural network losses with a regularization objective for
semi-supervised learning. The third, graph neural networks, aims to learn
differentiable functions over discrete topologies with arbitrary structure.
However, despite the popularity of these areas there has been surprisingly
little work on unifying the three paradigms. Here, we aim to bridge the gap
between graph neural networks, network embedding and graph regularization
models. We propose a comprehensive taxonomy of representation learning methods
for graph-structured data, aiming to unify several disparate bodies of work.
Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which
generalizes popular algorithms for semi-supervised learning on graphs (e.g.
GraphSage, Graph Convolutional Networks, Graph Attention Networks), and
unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc)
into a single consistent approach. To illustrate the generality of this
approach, we fit over thirty existing methods into this framework. We believe
that this unifying view both provides a solid foundation for understanding the
intuition behind these methods, and enables future research in the area."
150,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'stat.ML']",Deep Parametric Continuous Convolutional Neural Networks,"Standard convolutional neural networks assume a grid structured input is
available and exploit discrete convolutions as their fundamental building
blocks. This limits their applicability to many real-world applications. In
this paper we propose Parametric Continuous Convolution, a new learnable
operator that operates over non-grid structured data. The key idea is to
exploit parameterized kernel functions that span the full continuous vector
space. This generalization allows us to learn over arbitrary data structures as
long as their support relationship is computable. Our experiments show
significant improvement over the state-of-the-art in point cloud segmentation
of indoor and outdoor scenes, and lidar motion estimation of driving scenes."
151,['cs.LG'],BiGCN: A Bi-directional Low-Pass Filtering Graph Neural Network,"Graph convolutional networks have achieved great success on graph-structured
data. Many graph convolutional networks can be regarded as low-pass filters for
graph signals. In this paper, we propose a new model, BiGCN, which represents a
graph neural network as a bi-directional low-pass filter. Specifically, we not
only consider the original graph structure information but also the latent
correlation between features, thus BiGCN can filter the signals along with both
the original graph and a latent feature-connection graph. Our model outperforms
previous graph neural networks in the tasks of node classification and link
prediction on most of the benchmark datasets, especially when we add noise to
the node features."
152,"['cs.LG', 'cs.AI']",SPAGAN: Shortest Path Graph Attention Network,"Graph convolutional networks (GCN) have recently demonstrated their potential
in analyzing non-grid structure data that can be represented as graphs. The
core idea is to encode the local topology of a graph, via convolutions, into
the feature of a center node. In this paper, we propose a novel GCN model,
which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike
conventional GCN models that carry out node-based attentions within each layer,
the proposed SPAGAN conducts path-based attention that explicitly accounts for
the influence of a sequence of nodes yielding the minimum cost, or shortest
path, between the center node and its higher-order neighbors. SPAGAN therefore
allows for a more informative and intact exploration of the graph structure and
further {a} more effective aggregation of information from distant neighbors
into the center node, as compared to node-based GCN methods. We test SPAGAN on
the downstream classification task on several standard datasets, and achieve
performances superior to the state of the art. Code is publicly available at
https://github.com/ihollywhy/SPAGAN."
153,['cs.LG'],GraphHop: An Enhanced Label Propagation Method for Node Classification,"A scalable semi-supervised node classification method on graph-structured
data, called GraphHop, is proposed in this work. The graph contains attributes
of all nodes but labels of a few nodes. The classical label propagation (LP)
method and the emerging graph convolutional network (GCN) are two popular
semi-supervised solutions to this problem. The LP method is not effective in
modeling node attributes and labels jointly or facing a slow convergence rate
on large-scale graphs. GraphHop is proposed to its shortcoming. With proper
initial label vector embeddings, each iteration of GraphHop contains two steps:
1) label aggregation and 2) label update. In Step 1, each node aggregates its
neighbors' label vectors obtained in the previous iteration. In Step 2, a new
label vector is predicted for each node based on the label of the node itself
and the aggregated label information obtained in Step 1. This iterative
procedure exploits the neighborhood information and enables GraphHop to perform
well in an extremely small label rate setting and scale well for very large
graphs. Experimental results show that GraphHop outperforms state-of-the-art
graph learning methods on a wide range of tasks (e.g., multi-label and
multi-class classification on citation networks, social graphs, and commodity
consumption graphs) in graphs of various sizes. Our codes are publicly
available on GitHub (https://github.com/TianXieUSC/GraphHop)."
154,"['cs.LG', 'stat.ML', '05C99, 62M45', 'G.2.2']",Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for
analyzing graph-structured data. However, it is known that they do not improve
(or sometimes worsen) their predictive performance as we pile up many layers
and add non-lineality. To tackle this problem, we investigate the expressive
power of graph NNs via their asymptotic behaviors as the layer size tends to
infinity. Our strategy is to generalize the forward propagation of a Graph
Convolutional Network (GCN), which is a popular graph NN variant, as a specific
dynamical system. In the case of a GCN, we show that when its weights satisfy
the conditions determined by the spectra of the (augmented) normalized
Laplacian, its output exponentially approaches the set of signals that carry
information of the connected components and node degrees only for
distinguishing nodes. Our theory enables us to relate the expressive power of
GCNs with the topological information of the underlying graphs inherent in the
graph spectra. To demonstrate this, we characterize the asymptotic behavior of
GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s --
R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it
suffers from the ""information loss"" in the limit of infinite layers with high
probability. Based on the theory, we provide a principled guideline for weight
normalization of graph NNs. We experimentally confirm that the proposed weight
scaling enhances the predictive performance of GCNs in real data. Code is
available at https://github.com/delta2323/gnn-asymptotics."
155,"['cs.LG', 'cs.AI']",Passenger Mobility Prediction via Representation Learning for Dynamic Directed and Weighted Graph,"In recent years, ride-hailing services have been increasingly prevalent as
they provide huge convenience for passengers. As a fundamental problem, the
timely prediction of passenger demands in different regions is vital for
effective traffic flow control and route planning. As both spatial and temporal
patterns are indispensable passenger demand prediction, relevant research has
evolved from pure time series to graph-structured data for modeling historical
passenger demand data, where a snapshot graph is constructed for each time slot
by connecting region nodes via different relational edges (e.g.,
origin-destination relationship, geographical distance, etc.). Consequently,
the spatiotemporal passenger demand records naturally carry dynamic patterns in
the constructed graphs, where the edges also encode important information about
the directions and volume (i.e., weights) of passenger demands between two
connected regions. However, existing graph-based solutions fail to
simultaneously consider those three crucial aspects of dynamic, directed, and
weighted (DDW) graphs, leading to limited expressiveness when learning graph
representations for passenger demand prediction. Therefore, we propose a novel
spatiotemporal graph attention network, namely Gallat (Graph prediction with
all attention) as a solution. In Gallat, by comprehensively incorporating those
three intrinsic properties of DDW graphs, we build three attention layers to
fully capture the spatiotemporal dependencies among different regions across
all historical time slots. Moreover, the model employs a subtask to conduct
pretraining so that it can obtain accurate results more quickly. We evaluate
the proposed model on real-world datasets, and our experimental results
demonstrate that Gallat outperforms the state-of-the-art approaches."
156,['cs.LG'],Bosonic Random Walk Networks for Graph Learning,"The development of Graph Neural Networks (GNNs) has led to great progress in
machine learning on graph-structured data. These networks operate via diffusing
information across the graph nodes while capturing the structure of the graph.
Recently there has also seen tremendous progress in quantum computing
techniques. In this work, we explore applications of multi-particle quantum
walks on diffusing information across graphs. Our model is based on learning
the operators that govern the dynamics of quantum random walkers on graphs. We
demonstrate the effectiveness of our method on classification and regression
tasks."
157,"['stat.ML', 'cs.LG']",Graph Networks with Spectral Message Passing,"Graph Neural Networks (GNNs) are the subject of intense focus by the machine
learning community for problems involving relational reasoning. GNNs can be
broadly divided into spatial and spectral approaches. Spatial approaches use a
form of learned message-passing, in which interactions among vertices are
computed locally, and information propagates over longer distances on the graph
with greater numbers of message-passing steps. Spectral approaches use
eigendecompositions of the graph Laplacian to produce a generalization of
spatial convolutions to graph structured data which access information over
short and long time scales simultaneously. Here we introduce the Spectral Graph
Network, which applies message passing to both the spatial and spectral
domains. Our model projects vertices of the spatial graph onto the Laplacian
eigenvectors, which are each represented as vertices in a fully connected
""spectral graph"", and then applies learned message passing to them. We apply
this model to various benchmark tasks including a graph-based variant of MNIST
classification, molecular property prediction on MoleculeNet and QM9, and
shortest path problems on random graphs. Our results show that the Spectral GN
promotes efficient training, reaching high performance with fewer training
iterations despite having more parameters. The model also provides robustness
to edge dropout and outperforms baselines for the classification tasks. We also
explore how these performance benefits depend on properties of the dataset."
158,"['cs.LG', 'cs.CV', 'stat.ML']",Principal Neighbourhood Aggregation for Graph Nets,"Graph Neural Networks (GNNs) have been shown to be effective models for
different predictive tasks on graph-structured data. Recent work on their
expressive power has focused on isomorphism tasks and countable feature spaces.
We extend this theoretical framework to include continuous features - which
occur regularly in real-world input domains and within the hidden layers of
GNNs - and we demonstrate the requirement for multiple aggregation functions in
this context. Accordingly, we propose Principal Neighbourhood Aggregation
(PNA), a novel architecture combining multiple aggregators with degree-scalers
(which generalize the sum aggregator). Finally, we compare the capacity of
different models to capture and exploit the graph structure via a novel
benchmark containing multiple tasks taken from classical graph theory,
alongside existing benchmarks from real-world domains, all of which demonstrate
the strength of our model. With this work, we hope to steer some of the GNN
research towards new aggregation methods which we believe are essential in the
search for powerful and robust models."
159,"['cs.CV', 'cs.AI', 'cs.IT', 'math.IT']",Deep Hashing for Secure Multimodal Biometrics,"When compared to unimodal systems, multimodal biometric systems have several
advantages, including lower error rate, higher accuracy, and larger population
coverage. However, multimodal systems have an increased demand for integrity
and privacy because they must store multiple biometric traits associated with
each user. In this paper, we present a deep learning framework for
feature-level fusion that generates a secure multimodal template from each
user's face and iris biometrics. We integrate a deep hashing (binarization)
technique into the fusion architecture to generate a robust binary multimodal
shared latent representation. Further, we employ a hybrid secure architecture
by combining cancelable biometrics with secure sketch techniques and integrate
it with a deep hashing framework, which makes it computationally prohibitive to
forge a combination of multiple biometrics that pass the authentication. The
efficacy of the proposed approach is shown using a multimodal database of face
and iris and it is observed that the matching performance is improved due to
the fusion of multiple biometrics. Furthermore, the proposed approach also
provides cancelability and unlinkability of the templates along with improved
privacy of the biometric data. Additionally, we also test the proposed hashing
function for an image retrieval application using a benchmark dataset. The main
goal of this paper is to develop a method for integrating multimodal fusion,
deep hashing, and biometric security, with an emphasis on structural data from
modalities like face and iris. The proposed approach is in no way a general
biometric security framework that can be applied to all biometric modalities,
as further research is needed to extend the proposed framework to other
unconstrained biometric modalities."
160,['cs.CV'],Hierarchical Representation via Message Propagation for Robust Model Fitting,"In this paper, we propose a novel hierarchical representation via message
propagation (HRMP) method for robust model fitting, which simultaneously takes
advantages of both the consensus analysis and the preference analysis to
estimate the parameters of multiple model instances from data corrupted by
outliers, for robust model fitting. Instead of analyzing the information of
each data point or each model hypothesis independently, we formulate the
consensus information and the preference information as a hierarchical
representation to alleviate the sensitivity to gross outliers. Specifically, we
firstly construct a hierarchical representation, which consists of a model
hypothesis layer and a data point layer. The model hypothesis layer is used to
remove insignificant model hypotheses and the data point layer is used to
remove gross outliers. Then, based on the hierarchical representation, we
propose an effective hierarchical message propagation (HMP) algorithm and an
improved affinity propagation (IAP) algorithm to prune insignificant vertices
and cluster the remaining data points, respectively. The proposed HRMP can not
only accurately estimate the number and parameters of multiple model instances,
but also handle multi-structural data contaminated with a large number of
outliers. Experimental results on both synthetic data and real images show that
the proposed HRMP significantly outperforms several state-of-the-art model
fitting methods in terms of fitting accuracy and speed."
161,['cs.LG'],Semi-Supervised Node Classification on Graphs: Markov Random Fields vs. Graph Neural Networks,"Semi-supervised node classification on graph-structured data has many
applications such as fraud detection, fake account and review detection, user's
private attribute inference in social networks, and community detection.
Various methods such as pairwise Markov Random Fields (pMRF) and graph neural
networks were developed for semi-supervised node classification. pMRF is more
efficient than graph neural networks. However, existing pMRF-based methods are
less accurate than graph neural networks, due to a key limitation that they
assume a heuristics-based constant edge potential for all edges. In this work,
we aim to address the key limitation of existing pMRF-based methods. In
particular, we propose to learn edge potentials for pMRF. Our evaluation
results on various types of graph datasets show that our optimized pMRF-based
method consistently outperforms existing graph neural networks in terms of both
accuracy and efficiency. Our results highlight that previous work may have
underestimated the power of pMRF for semi-supervised node classification."
162,"['cs.CV', 'cs.LG', 'eess.IV']",Learning Local Neighboring Structure for Robust 3D Shape Representation,"Mesh is a powerful data structure for 3D shapes. Representation learning for
3D meshes is important in many computer vision and graphics applications. The
recent success of convolutional neural networks (CNNs) for structured data
(e.g., images) suggests the value of adapting insight from CNN for 3D shapes.
However, 3D shape data are irregular since each node's neighbors are unordered.
Various graph neural networks for 3D shapes have been developed with isotropic
filters or predefined local coordinate systems to overcome the node
inconsistency on graphs. However, isotropic filters or predefined local
coordinate systems limit the representation power. In this paper, we propose a
local structure-aware anisotropic convolutional operation (LSA-Conv) that
learns adaptive weighting matrices for each node according to the local
neighboring structure and performs shared anisotropic filters. In fact, the
learnable weighting matrix is similar to the attention matrix in the random
synthesizer -- a new Transformer model for natural language processing (NLP).
Comprehensive experiments demonstrate that our model produces significant
improvement in 3D shape reconstruction compared to state-of-the-art methods."
163,['cs.LG'],An Experimental Study of the Transferability of Spectral Graph Networks,"Spectral graph convolutional networks are generalizations of standard
convolutional networks for graph-structured data using the Laplacian operator.
A common misconception is the instability of spectral filters, i.e. the
impossibility to transfer spectral filters between graphs of variable size and
topology. This misbelief has limited the development of spectral networks for
multi-graph tasks in favor of spatial graph networks. However, recent works
have proved the stability of spectral filters under graph perturbation. Our
work complements and emphasizes further the high quality of spectral
transferability by benchmarking spectral graph networks on tasks involving
graphs of different size and connectivity. Numerical experiments exhibit
favorable performance on graph regression, graph classification, and node
classification problems on two graph benchmarks. The implementation of our
experiments is available on GitHub for reproducibility."
164,['cs.LG'],Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions,"Named Entity Recognition has been extensively investigated in many fields.
However, the application of sensitive entity detection for production systems
in financial institutions has not been well explored due to the lack of
publicly available, labeled datasets. In this paper, we use internal and
synthetic datasets to evaluate various methods of detecting NPI (Nonpublic
Personally Identifiable) information commonly found within financial
institutions, in both unstructured and structured data formats. Character-level
neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are
investigated on two prediction tasks: (i) entity detection on multiple data
formats, and (ii) column-wise entity prediction on tabular datasets. We compare
these models with other standard approaches on both real and synthetic data,
with respect to F1-score, precision, recall, and throughput. The real datasets
include internal structured data and public email data with manually tagged
labels. Our experimental results show that the CNN model is simple yet
effective with respect to accuracy and throughput and thus, is the most
suitable candidate model to be deployed in the production environment(s).
Finally, we provide several lessons learned on data limitations, data labelling
and the intrinsic overlap of data entities."
165,"['cs.LG', 'cs.NA', 'math.NA']",Decimated Framelet System on Graphs and Fast G-Framelet Transforms,"Graph representation learning has many real-world applications, from
super-resolution imaging, 3D computer vision to drug repurposing, protein
classification, social networks analysis. An adequate representation of graph
data is vital to the learning performance of a statistical or machine learning
model for graph-structured data. In this paper, we propose a novel multiscale
representation system for graph data, called decimated framelets, which form a
localized tight frame on the graph. The decimated framelet system allows
storage of the graph data representation on a coarse-grained chain and
processes the graph data at multi scales where at each scale, the data is
stored at a subgraph. Based on this, we then establish decimated G-framelet
transforms for the decomposition and reconstruction of the graph data at multi
resolutions via a constructive data-driven filter bank. The graph framelets are
built on a chain-based orthonormal basis that supports fast graph Fourier
transforms. From this, we give a fast algorithm for the decimated G-framelet
transforms, or FGT, that has linear computational complexity O(N) for a graph
of size N. The theory of decimated framelets and FGT is verified with numerical
examples for random graphs. The effectiveness is demonstrated by real-world
applications, including multiresolution analysis for traffic network, and graph
neural networks for graph classification tasks."
166,"['stat.ML', 'cs.LG', 'stat.AP']",Multi-Output Gaussian Processes with Functional Data: A Study on Coastal Flood Hazard Assessment,"Most of the existing coastal flood Forecast and Early-Warning Systems do not
model the flood, but instead, rely on the prediction of hydrodynamic conditions
at the coast and on expert judgment. Recent scientific contributions are now
capable to precisely model flood events, even in situations where wave
overtopping plays a significant role. Such models are nevertheless
costly-to-evaluate and surrogate ones need to be exploited for substantial
computational savings. For the latter models, the hydro-meteorological forcing
conditions (inputs) or flood events (outputs) are conveniently parametrised
into scalar representations. However, they neglect the fact that inputs are
actually functions (more precisely, time series), and that floods spatially
propagate inland. Here, we introduce a multi-output Gaussian process model
accounting for both criteria. On various examples, we test its versatility for
both learning spatial maps and inferring unobserved ones. We demonstrate that
efficient implementations are obtained by considering tensor-structured data
and/or sparse-variational approximations. Finally, the proposed framework is
applied on a coastal application aiming at predicting flood events. We conclude
that accurate predictions are obtained in the order of minutes rather than the
couples of days required by dedicated hydrodynamic simulators."
167,"['cs.CV', 'cs.LG']",Removing Class Imbalance using Polarity-GAN: An Uncertainty Sampling Approach,"Class imbalance is a challenging issue in practical classification problems
for deep learning models as well as for traditional models. Traditionally
successful countermeasures such as synthetic over-sampling have had limited
success with complex, structured data handled by deep learning models. In this
work, we propose to use a Generative Adversarial Network (GAN) equipped with a
generator network G, a discriminator network D and a classifier network C to
remove the class-imbalance in visual data sets. The generator network is
initialized with auto-encoder to make it stable. The discriminator D ensures
that G adheres to class distribution of imbalanced class. In conventional
methods, where Generator G competes with discriminator D in a min-max game, we
propose to further add an additional classifier network to the original
network. Now, the generator network tries to compete in a min-max game with
Discriminator as well as the new classifier that we have introduced. An
additional condition is enforced on generator network G to produce points in
the convex hull of desired imbalanced class. Further the contention of
adversarial game with classifier C, pushes conditional distribution learned by
G towards the periphery of the respective class, compensating the problem of
class imbalance. Experimental evidence shows that this initialization results
in stable training of the network. We achieve state of the art performance on
extreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark,
MVTec Anomaly Detection dataset, Chest X-Ray dataset and others."
168,"['cs.CV', 'cs.AI']",Mitigating the Impact of Adversarial Attacks in Very Deep Networks,"Deep Neural Network (DNN) models have vulnerabilities related to security
concerns, with attackers usually employing complex hacking techniques to expose
their structures. Data poisoning-enabled perturbation attacks are complex
adversarial ones that inject false data into models. They negatively impact the
learning process, with no benefit to deeper networks, as they degrade a model's
accuracy and convergence rates. In this paper, we propose an
attack-agnostic-based defense method for mitigating their influence. In it, a
Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture
which assists in neutralizing the effects of illegitimate perturbation samples
in the feature space. To boost the robustness and trustworthiness of this
method for correctly classifying attacked input samples, we regularize the
hidden space of a trained model with a discriminative loss function called
Polarized Contrastive Loss (PCL). It improves discrimination among samples in
different classes and maintains the resemblance of those in the same class.
Also, we integrate a DFL and PCL in a compact model for defending against data
poisoning attacks. This method is trained and tested using the CIFAR-10 and
MNIST datasets with data poisoning-enabled perturbation attacks, with the
experimental results revealing its excellent performance compared with those of
recent peer techniques."
169,['cs.LG'],Graph-Based Neural Network Models with Multiple Self-Supervised Auxiliary Tasks,"Self-supervised learning is currently gaining a lot of attention, as it
allows neural networks to learn robust representations from large quantities of
unlabeled data. Additionally, multi-task learning can further improve
representation learning by training networks simultaneously on related tasks,
leading to significant performance improvements. In this paper, we propose
three novel self-supervised auxiliary tasks to train graph-based neural network
models in a multi-task fashion. Since Graph Convolutional Networks are among
the most promising approaches for capturing relationships among structured data
points, we use them as a building block to achieve competitive results on
standard semi-supervised graph classification tasks."
170,"['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.LG']",Modelling the influence of data structure on learning in neural networks: the hidden manifold model,"Understanding the reasons for the success of deep neural networks trained
using stochastic gradient-based methods is a key open problem for the nascent
theory of deep learning. The types of data where these networks are most
successful, such as images or sequences of speech, are characterised by
intricate correlations. Yet, most theoretical work on neural networks does not
explicitly model training data, or assumes that elements of each data sample
are drawn independently from some factorised probability distribution. These
approaches are thus by construction blind to the correlation structure of
real-world data sets and their impact on learning in neural networks. Here, we
introduce a generative model for structured data sets that we call the hidden
manifold model (HMM). The idea is to construct high-dimensional inputs that lie
on a lower-dimensional manifold, with labels that depend only on their position
within this manifold, akin to a single layer decoder or generator in a
generative adversarial network. We demonstrate that learning of the hidden
manifold model is amenable to an analytical treatment by proving a ""Gaussian
Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of
two-layer neural networks trained using one-pass stochastic gradient descent is
captured by a set of integro-differential equations that track the performance
of the network at all times. This permits us to analyse in detail how a neural
network learns functions of increasing complexity during training, how its
performance depends on its size and how it is impacted by parameters such as
the learning rate or the dimension of the hidden manifold."
171,"['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",Approximate Cross-Validation for Structured Models,"Many modern data analyses benefit from explicitly modeling dependence
structure in data -- such as measurements across time or space, ordered words
in a sentence, or genes in a genome. A gold standard evaluation technique is
structured cross-validation (CV), which leaves out some data subset (such as
data within a time interval or data in a geographic region) in each fold. But
CV here can be prohibitively slow due to the need to re-run already-expensive
learning algorithms many times. Previous work has shown approximate
cross-validation (ACV) methods provide a fast and provably accurate alternative
in the setting of empirical risk minimization. But this existing ACV work is
restricted to simpler models by the assumptions that (i) data across CV folds
are independent and (ii) an exact initial model fit is available. In structured
data analyses, both these assumptions are often untrue. In the present work, we
address (i) by extending ACV to CV schemes with dependence structure between
the folds. To address (ii), we verify -- both theoretically and empirically --
that ACV quality deteriorates smoothly with noise in the initial fit. We
demonstrate the accuracy and computational benefits of our proposed methods on
a diverse set of real-world applications."
172,"['cs.LG', 'stat.ML']",Learning a manifold from a teacher's demonstrations,"We consider the problem of learning a manifold from a teacher's
demonstration. Extending existing approaches of learning from randomly sampled
data points, we consider contexts where data may be chosen by a teacher. We
analyze learning from teachers who can provide structured data such as
individual examples (isolated data points) and demonstrations (sequences of
points). Our analysis shows that for the purpose of teaching the topology of a
manifold, demonstrations can yield remarkable decreases in the amount of data
points required in comparison to teaching with randomly sampled points. We also
discuss the implications of our analysis for learning in humans and machines."
173,"['cs.LG', 'cs.CR']",A Targeted Universal Attack on Graph Convolutional Network,"Graph-structured data exist in numerous applications in real life. As a
state-of-the-art graph neural network, the graph convolutional network (GCN)
plays an important role in processing graph-structured data. However, a recent
study reported that GCNs are also vulnerable to adversarial attacks, which
means that GCN models may suffer malicious attacks with unnoticeable
modifications of the data. Among all the adversarial attacks on GCNs, there is
a special kind of attack method called the universal adversarial attack, which
generates a perturbation that can be applied to any sample and causes GCN
models to output incorrect results. Although universal adversarial attacks in
computer vision have been extensively researched, there are few research works
on universal adversarial attacks on graph structured data. In this paper, we
propose a targeted universal adversarial attack against GCNs. Our method
employs a few nodes as the attack nodes. The attack capability of the attack
nodes is enhanced through a small number of fake nodes connected to them.
During an attack, any victim node will be misclassified by the GCN as the
attack node class as long as it is linked to them. The experiments on three
popular datasets show that the average attack success rate of the proposed
attack on any victim node in the graph reaches 83% when using only 3 attack
nodes and 6 fake nodes. We hope that our work will make the community aware of
the threat of this type of attack and raise the attention given to its future
defense."
174,"['cs.LG', 'cs.AI', 'stat.ML']",Higher-Order Explanations of Graph Neural Networks via Relevant Walks,"Graph Neural Networks (GNNs) are a popular approach for predicting graph
structured data. As GNNs tightly entangle the input graph into the neural
network structure, common explainable AI approaches are not applicable. To a
large extent, GNNs have remained black-boxes for the user so far. In this
paper, we show that GNNs can in fact be naturally explained using higher-order
expansions, i.e. by identifying groups of edges that jointly contribute to the
prediction. Practically, we find that such explanations can be extracted using
a nested attribution scheme, where existing techniques such as layer-wise
relevance propagation (LRP) can be applied at each step. The output is a
collection of walks into the input graph that are relevant for the prediction.
Our novel explanation method, which we denote by GNN-LRP, is applicable to a
broad range of graph neural networks and lets us extract practically relevant
insights on sentiment analysis of text data, structure-property relationships
in quantum chemistry, and image classification."
175,"['cs.LG', 'cs.CV']",Explaining Deep Learning Models for Structured Data using Layer-Wise Relevance Propagation,"Trust and credibility in machine learning models is bolstered by the ability
of a model to explain itsdecisions. While explainability of deep learning
models is a well-known challenge, a further chal-lenge is clarity of the
explanation itself, which must be interpreted by downstream users.
Layer-wiseRelevance Propagation (LRP), an established explainability technique
developed for deep models incomputer vision, provides intuitive human-readable
heat maps of input images. We present the novelapplication of LRP for the first
time with structured datasets using a deep neural network (1D-CNN),for Credit
Card Fraud detection and Telecom Customer Churn prediction datasets. We show
how LRPis more effective than traditional explainability concepts of Local
Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive
Explanations (SHAP) for explainability. This effectivenessis both local to a
sample level and holistic over the whole testing set. We also discuss the
significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP
(108s), and thus its poten-tial for real time application scenarios. In
addition, our validation of LRP has highlighted features forenhancing model
performance, thus opening up a new area of research of using XAI as an
approachfor feature subset selection"
176,['cs.CV'],KShapeNet: Riemannian network on Kendall shape space for Skeleton based Action Recognition,"Deep Learning architectures, albeit successful in most computer vision tasks,
were designed for data with an underlying Euclidean structure, which is not
usually fulfilled since pre-processed data may lie on a non-linear space. In
this paper, we propose a geometry aware deep learning approach for
skeleton-based action recognition. Skeleton sequences are first modeled as
trajectories on Kendall's shape space and then mapped to the linear tangent
space. The resulting structured data are then fed to a deep learning
architecture, which includes a layer that optimizes over rigid and non rigid
transformations of the 3D skeletons, followed by a CNN-LSTM network. The
assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D
120, has proven that proposed approach outperforms existing geometric deep
learning methods and is competitive with respect to recently published
approaches."
177,"['cs.LG', 'cs.AI']",Time Series Data Imputation: A Survey on Deep Learning Approaches,"Time series are all around in real-world applications. However, unexpected
accidents for example broken sensors or missing of the signals will cause
missing values in time series, making the data hard to be utilized. It then
does harm to the downstream applications such as traditional classification or
regression, sequential data integration and forecasting tasks, thus raising the
demand for data imputation. Currently, time series data imputation is a
well-studied problem with different categories of methods. However, these works
rarely take the temporal relations among the observations and treat the time
series as normal structured data, losing the information from the time data. In
recent, deep learning models have raised great attention. Time series methods
based on deep learning have made progress with the usage of models like RNN,
since it captures time information from data. In this paper, we mainly focus on
time series imputation technique with deep learning methods, which recently
made progress in this field. We will review and discuss their model
architectures, their pros and cons as well as their effects to show the
development of the time series imputation methods."
178,['cs.LG'],Scalable Graph Neural Networks for Heterogeneous Graphs,"Graph neural networks (GNNs) are a popular class of parametric model for
learning over graph-structured data. Recent work has argued that GNNs primarily
use the graph for feature smoothing, and have shown competitive results on
benchmark tasks by simply operating on graph-smoothed node features, rather
than using end-to-end learned feature hierarchies that are challenging to scale
to large graphs. In this work, we ask whether these results can be extended to
heterogeneous graphs, which encode multiple types of relationship between
different entities. We propose Neighbor Averaging over Relation Subgraphs
(NARS), which trains a classifier on neighbor-averaged features for
randomly-sampled subgraphs of the ""metagraph"" of relations. We describe
optimizations to allow these sets of node features to be computed in a
memory-efficient way, both at training and inference time. NARS achieves a new
state of the art accuracy on several benchmark datasets, outperforming more
expensive GNN-based methods"
179,"['cs.LG', 'stat.CO', 'stat.ME', 'stat.ML']",Cluster-Specific Predictions with Multi-Task Gaussian Processes,"A model involving Gaussian processes (GPs) is introduced to simultaneously
handle multi-task learning, clustering, and prediction for multiple functional
data. This procedure acts as a model-based clustering method for functional
data as well as a learning step for subsequent predictions for new tasks. The
model is instantiated as a mixture of multi-task GPs with common mean
processes. A variational EM algorithm is derived for dealing with the
optimisation of the hyper-parameters along with the hyper-posteriors'
estimation of latent variables and processes. We establish explicit formulas
for integrating the mean processes and the latent clustering variables within a
predictive distribution, accounting for uncertainty on both aspects. This
distribution is defined as a mixture of cluster-specific GP predictions, which
enhances the performances when dealing with group-structured data. The model
handles irregular grid of observations and offers different hypotheses on the
covariance structure for sharing additional information across tasks. The
performances on both clustering and prediction tasks are assessed through
various simulated scenarios and real datasets. The overall algorithm, called
MagmaClust, is publicly available as an R package."
180,"['cs.LG', 'cs.AI']",Self-supervised Graph Representation Learning via Bootstrapping,"Graph neural networks~(GNNs) apply deep learning techniques to
graph-structured data and have achieved promising performance in graph
representation learning. However, existing GNNs rely heavily on enough labels
or well-designed negative samples. To address these issues, we propose a new
self-supervised graph representation method: deep graph bootstrapping~(DGB).
DGB consists of two neural networks: online and target networks, and the input
of them are different augmented views of the initial graph. The online network
is trained to predict the target network while the target network is updated
with a slow-moving average of the online network, which means the online and
target networks can learn from each other. As a result, the proposed DGB can
learn graph representation without negative examples in an unsupervised manner.
In addition, we summarize three kinds of augmentation methods for
graph-structured data and apply them to the DGB. Experiments on the benchmark
datasets show the DGB performs better than the current state-of-the-art methods
and how the augmentation methods affect the performances."
181,['cs.LG'],Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration,"Discrete structures play an important role in applications like program
language modeling and software engineering. Current approaches to predicting
complex structures typically consider autoregressive models for their
tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on
the other hand offer a more flexible and thus more powerful approach to
modeling such distributions, but require partition function estimation. In this
paper we propose ALOE, a new algorithm for learning conditional and
unconditional EBMs for discrete structured data, where parameter gradients are
estimated using a learned sampler that mimics local search. We show that the
energy function and sampler can be trained efficiently via a new variational
form of power iteration, achieving a better trade-off between flexibility and
tractability. Experimentally, we show that learning local search leads to
significant improvements in challenging application domains. Most notably, we
present an energy model guided fuzzer for software testing that achieves
comparable performance to well engineered fuzzing engines like libfuzzer."
182,"['cs.LG', 'stat.ML']",Graph Kernels: State-of-the-Art and Future Challenges,"Graph-structured data are an integral part of many application domains,
including chemoinformatics, computational biology, neuroimaging, and social
network analysis. Over the last two decades, numerous graph kernels, i.e.
kernel functions between graphs, have been proposed to solve the problem of
assessing the similarity between graphs, thereby making it possible to perform
predictions in both classification and regression settings. This manuscript
provides a review of existing graph kernels, their applications, software plus
data resources, and an empirical comparison of state-of-the-art graph kernels."
183,['cs.CV'],A low latency ASR-free end to end spoken language understanding system,"In recent years, developing a speech understanding system that classifies a
waveform to structured data, such as intents and slots, without first
transcribing the speech to text has emerged as an interesting research problem.
This work proposes such as system with an additional constraint of designing a
system that has a small enough footprint to run on small micro-controllers and
embedded systems with minimal latency. Given a streaming input speech signal,
the proposed system can process it segment-by-segment without the need to have
the entire stream at the moment of processing. The proposed system is evaluated
on the publicly available Fluent Speech Commands dataset. Experiments show that
the proposed system yields state-of-the-art performance with the advantage of
low latency and a much smaller model when compared to other published works on
the same task."
184,"['stat.ML', 'cs.LG']",A contribution to Optimal Transport on incomparable spaces,"Optimal Transport is a theory that allows to define geometrical notions of
distance between probability distributions and to find correspondences,
relationships, between sets of points. Many machine learning applications are
derived from this theory, at the frontier between mathematics and optimization.
This thesis proposes to study the complex scenario in which the different data
belong to incomparable spaces. In particular we address the following
questions: how to define and apply Optimal Transport between graphs, between
structured data? How can it be adapted when the data are varied and not
embedded in the same metric space? This thesis proposes a set of Optimal
Transport tools for these different cases. An important part is notably devoted
to the study of the Gromov-Wasserstein distance whose properties allow to
define interesting transport problems on incomparable spaces. More broadly, we
analyze the mathematical properties of the various proposed tools, we establish
algorithmic solutions to compute them and we study their applicability in
numerous machine learning scenarii which cover, in particular, classification,
simplification, partitioning of structured data, as well as heterogeneous
domain adaptation."
185,"['cs.LG', 'cs.AI', 'stat.AP', 'stat.ML']",$$-Cores: Robust Large-Scale Bayesian Data Summarization in the Presence of Outliers,"Modern machine learning applications should be able to address the intrinsic
challenges arising over inference on massive real-world datasets, including
scalability and robustness to outliers. Despite the multiple benefits of
Bayesian methods (such as uncertainty-aware predictions, incorporation of
experts knowledge, and hierarchical modeling), the quality of classic Bayesian
inference depends critically on whether observations conform with the assumed
data generating model, which is impossible to guarantee in practice. In this
work, we propose a variational inference method that, in a principled way, can
simultaneously scale to large datasets, and robustify the inferred posterior
with respect to the existence of outliers in the observed data. Reformulating
Bayes theorem via the $\beta$-divergence, we posit a robustified
pseudo-Bayesian posterior as the target of inference. Moreover, relying on the
recent formulations of Riemannian coresets for scalable Bayesian inference, we
propose a sparse variational approximation of the robustified posterior and an
efficient stochastic black-box algorithm to construct it. Overall our method
allows releasing cleansed data summaries that can be applied broadly in
scenarios including structured data corruption. We illustrate the applicability
of our approach in diverse simulated and real datasets, and various statistical
models, including Gaussian mean inference, logistic and neural linear
regression, demonstrating its superiority to existing Bayesian summarization
methods in the presence of outliers."
186,"['cs.LG', 'cs.DM', 'stat.ML']",Can Graph Neural Networks Count Substructures?,"The ability to detect and count certain substructures in graphs is important
for solving many tasks on graph-structured data, especially in the contexts of
computational chemistry and biology as well as social network analysis.
Inspired by this, we propose to study the expressive power of graph neural
networks (GNNs) via their ability to count attributed graph substructures,
extending recent works that examine their power in graph isomorphism testing
and function approximation. We distinguish between two types of substructure
counting: induced-subgraph-count and subgraph-count, and establish both
positive and negative answers for popular GNN architectures. Specifically, we
prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL)
and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count
of substructures consisting of 3 or more nodes, while they can perform
subgraph-count of star-shaped substructures. As an intermediary step, we prove
that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs,
partly answering an open problem raised in Maron et al. (2019). We also prove
positive results for k-WL and k-IGNs as well as negative results for k-WL with
a finite number of iterations. We then conduct experiments that support the
theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure
counting and inspired by Murphy et al. (2019), we propose the Local Relational
Pooling model and demonstrate that it is not only effective for substructure
counting but also able to achieve competitive performance on molecular
prediction tasks."
187,"['cs.LG', 'stat.ML']","Hypergraph Random Walks, Laplacians, and Clustering","We propose a flexible framework for clustering hypergraph-structured data
based on recently proposed random walks utilizing edge-dependent vertex
weights. When incorporating edge-dependent vertex weights (EDVW), a weight is
associated with each vertex-hyperedge pair, yielding a weighted incidence
matrix of the hypergraph. Such weightings have been utilized in term-document
representations of text data sets. We explain how random walks with EDVW serve
to construct different hypergraph Laplacian matrices, and then develop a suite
of clustering methods that use these incidence matrices and Laplacians for
hypergraph clustering. Using several data sets from real-life applications, we
compare the performance of these clustering algorithms experimentally against a
variety of existing hypergraph clustering methods. We show that the proposed
methods produce higher-quality clusters and conclude by highlighting avenues
for future work."
188,"['cs.LG', 'cs.AI']",GraphMDN: Leveraging graph structure and deep learning to solve inverse problems,"The recent introduction of Graph Neural Networks (GNNs) and their growing
popularity in the past few years has enabled the application of deep learning
algorithms to non-Euclidean, graph-structured data. GNNs have achieved
state-of-the-art results across an impressive array of graph-based machine
learning problems. Nevertheless, despite their rapid pace of development, much
of the work on GNNs has focused on graph classification and embedding
techniques, largely ignoring regression tasks over graph data. In this paper,
we develop a Graph Mixture Density Network (GraphMDN), which combines graph
neural networks with mixture density network (MDN) outputs. By combining these
techniques, GraphMDNs have the advantage of naturally being able to incorporate
graph structured information into a neural architecture, as well as the ability
to model multi-modal regression targets. As such, GraphMDNs are designed to
excel on regression tasks wherein the data are graph structured, and target
statistics are better represented by mixtures of densities rather than singular
values (so-called ``inverse problems""). To demonstrate this, we extend an
existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN
structure, and show results from the Human3.6M pose estimation task. The
extended model consistently outperforms both GCN and MDN architectures on their
own, with a comparable number of parameters."
189,['cs.LG'],Contrastive Graph Neural Network Explanation,"Graph Neural Networks achieve remarkable results on problems with structured
data but come as black-box predictors. Transferring existing explanation
techniques, such as occlusion, fails as even removing a single node or edge can
lead to drastic changes in the graph. The resulting graphs can differ from all
training examples, causing model confusion and wrong explanations. Thus, we
argue that explicability must use graphs compliant with the distribution
underlying the training data. We coin this property Distribution Compliant
Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE)
technique following this paradigm. An experimental study supports the efficacy
of CoGE."
190,"['cs.LG', 'cs.CV']",Co-embedding of Nodes and Edges with Graph Neural Networks,"Graph, as an important data representation, is ubiquitous in many real world
applications ranging from social network analysis to biology. How to correctly
and effectively learn and extract information from graph is essential for a
large number of machine learning tasks. Graph embedding is a way to transform
and encode the data structure in high dimensional and non-Euclidean feature
space to a low dimensional and structural space, which is easily exploited by
other machine learning algorithms. We have witnessed a huge surge of such
embedding methods, from statistical approaches to recent deep learning methods
such as the graph convolutional networks (GCN). Deep learning approaches
usually outperform the traditional methods in most graph learning benchmarks by
building an end-to-end learning framework to optimize the loss function
directly. However, most of the existing GCN methods can only perform
convolution operations with node features, while ignoring the handy information
in edge features, such as relations in knowledge graphs. To address this
problem, we present CensNet, Convolution with Edge-Node Switching graph neural
network, for learning tasks in graph-structured data with both node and edge
features. CensNet is a general graph embedding framework, which embeds both
nodes and edges to a latent feature space. By using line graph of the original
undirected graph, the role of nodes and edges are switched, and two novel graph
convolution operations are proposed for feature propagation. Experimental
results on real-world academic citation networks and quantum chemistry graphs
show that our approach achieves or matches the state-of-the-art performance in
four graph learning tasks, including semi-supervised node classification,
multi-task graph classification, graph regression, and link prediction."
191,"['cs.LG', 'stat.ML']",Graph Information Bottleneck,"Representation learning of graph-structured data is challenging because both
graph structure and node features carry important information. Graph Neural
Networks (GNNs) provide an expressive way to fuse information from network
structure and node features. However, GNNs are prone to adversarial attacks.
Here we introduce Graph Information Bottleneck (GIB), an information-theoretic
principle that optimally balances expressiveness and robustness of the learned
representation of graph-structured data. Inheriting from the general
Information Bottleneck (IB), GIB aims to learn the minimal sufficient
representation for a given task by maximizing the mutual information between
the representation and the target, and simultaneously constraining the mutual
information between the representation and the input data. Different from the
general IB, GIB regularizes the structural as well as the feature information.
We design two sampling algorithms for structural regularization and instantiate
the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate
the benefits by evaluating the resilience to adversarial attacks. We show that
our proposed models are more robust than state-of-the-art graph defense models.
GIB-based models empirically achieve up to 31% improvement with adversarial
perturbation of the graph structure as well as node features."
192,"['cs.LG', 'cs.CR']",Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization,"Graph neural networks (GNNs) have been widely used to analyze the
graph-structured data in various application domains, e.g., social networks,
molecular biology, and anomaly detection. With great power, the GNN models,
usually as valuable Intellectual Properties of their owners, also become
attractive targets of the attacker. Recent studies show that machine learning
models are facing a severe threat called Model Extraction Attacks, where a
well-trained private model owned by a service provider can be stolen by the
attacker pretending as a client. Unfortunately, existing works focus on the
models trained on the Euclidean space, e.g., images and texts, while how to
extract a GNN model that contains a graph structure and node features is yet to
be explored. In this paper, we explore and develop model extraction attacks
against GNN models. Given only black-box access to a target GNN model, the
attacker aims to reconstruct a duplicated one via several nodes he obtained
(called attacker nodes). We first systematically formalise the threat modeling
in the context of GNN model extraction and classify the adversarial threats
into seven categories by considering different background knowledge of the
attacker, e.g., attributes and/or neighbor connectives of the attacker nodes.
Then we present the detailed methods which utilize the accessible knowledge in
each threat to implement the attacks. By evaluating over three real-world
datasets, our attacks are shown to extract duplicated models effectively, i.e.,
more than 89% inputs in the target domain have the same output predictions as
the victim model."
193,"['cs.LG', 'cs.DS', 'stat.ML']",Faster Graph Embeddings via Coarsening,"Graph embeddings are a ubiquitous tool for machine learning tasks, such as
node classification and link prediction, on graph-structured data. However,
computing the embeddings for large-scale graphs is prohibitively inefficient
even if we are interested only in a small subset of relevant vertices. To
address this, we present an efficient graph coarsening approach, based on Schur
complements, for computing the embedding of the relevant vertices. We prove
that these embeddings are preserved exactly by the Schur complement graph that
is obtained via Gaussian elimination on the non-relevant vertices. As computing
Schur complements is expensive, we give a nearly-linear time algorithm that
generates a coarsened graph on the relevant vertices that provably matches the
Schur complement in expectation in each iteration. Our experiments involving
prediction tasks on graphs demonstrate that computing embeddings on the
coarsened graph, rather than the entire graph, leads to significant time
savings without sacrificing accuracy."
194,"['cs.LG', 'cs.AI']",Graph Fairing Convolutional Networks for Anomaly Detection,"Graph convolution is a fundamental building block for many deep neural
networks on graph-structured data. In this paper, we introduce a simple, yet
very effective graph convolutional network with skip connections for
semi-supervised anomaly detection. The proposed multi-layer network
architecture is theoretically motivated by the concept of implicit fairing in
geometry processing, and comprises a graph convolution module for aggregating
information from immediate node neighbors and a skip connection module for
combining layer-wise neighborhood representations. In addition to capturing
information from distant graph nodes through skip connections between the
network's layers, our approach exploits both the graph structure and node
features for learning discriminative node representations. The effectiveness of
our model is demonstrated through extensive experiments on five benchmark
datasets, achieving better or comparable anomaly detection results against
strong baseline methods."
195,"['cs.LG', 'stat.ML']",The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems,"This paper presents and characterizes an Open Application Repository for
Federated Learning (OARF), a benchmark suite for federated machine learning
systems. Previously available benchmarks for federated learning have focused
mainly on synthetic datasets and use a very limited number of applications.
OARF includes different data partitioning methods (horizontal, vertical and
hybrid) as well as emerging applications in image, text and structured data,
which represent different scenarios in federated learning. Our characterization
shows that the benchmark suite is diverse in data size, distribution, feature
distribution and learning task complexity. We have developed reference
implementations, and evaluated the important aspects of federated learning,
including model accuracy, communication cost, differential privacy, secure
multiparty computation and vertical federated learning."
196,"['cs.LG', 'stat.ML']",Neuralizing Efficient Higher-order Belief Propagation,"Graph neural network models have been extensively used to learn node
representations for graph structured data in an end-to-end setting. These
models often rely on localized first order approximations of spectral graph
convolutions and hence are unable to capture higher-order relational
information between nodes. Probabilistic Graphical Models form another class of
models that provide rich flexibility in incorporating such relational
information but are limited by inefficient approximate inference algorithms at
higher order. In this paper, we propose to combine these approaches to learn
better node and graph representations. First, we derive an efficient
approximate sum-product loopy belief propagation inference algorithm for
higher-order PGMs. We then embed the message passing updates into a neural
network to provide the inductive bias of the inference algorithm in end-to-end
learning. This gives us a model that is flexible enough to accommodate domain
knowledge while maintaining the computational advantage. We further propose
methods for constructing higher-order factors that are conditioned on node and
edge features and share parameters wherever necessary. Our experimental
evaluation shows that our model indeed captures higher-order information,
substantially outperforming state-of-the-art $k$-order graph neural networks in
molecular datasets."
197,"['cs.LG', 'cs.AI']",FaiR-N: Fair and Robust Neural Networks for Structured Data,"Fairness in machine learning is crucial when individuals are subject to
automated decisions made by models in high-stake domains. Organizations that
employ these models may also need to satisfy regulations that promote
responsible and ethical A.I. While fairness metrics relying on comparing model
error rates across subpopulations have been widely investigated for the
detection and mitigation of bias, fairness in terms of the equalized ability to
achieve recourse for different protected attribute groups has been relatively
unexplored. We present a novel formulation for training neural networks that
considers the distance of data points to the decision boundary such that the
new objective: (1) reduces the average distance to the decision boundary
between two groups for individuals subject to a negative outcome in each group,
i.e. the network is more fair with respect to the ability to obtain recourse,
and (2) increases the average distance of data points to the boundary to
promote adversarial robustness. We demonstrate that training with this loss
yields more fair and robust neural networks with similar accuracies to models
trained without it. Moreover, we qualitatively motivate and empirically show
that reducing recourse disparity across groups also improves fairness measures
that rely on error rates. To the best of our knowledge, this is the first time
that recourse capabilities across groups are considered to train fairer neural
networks, and a relation between error rates based fairness and recourse based
fairness is investigated."
198,"['cs.LG', 'hep-ex']",Anomaly Detection With Conditional Variational Autoencoders,"Exploiting the rapid advances in probabilistic inference, in particular
variational Bayes and variational autoencoders (VAEs), for anomaly detection
(AD) tasks remains an open research question. Previous works argued that
training VAE models only with inliers is insufficient and the framework should
be significantly modified in order to discriminate the anomalous instances. In
this work, we exploit the deep conditional variational autoencoder (CVAE) and
we define an original loss function together with a metric that targets
hierarchically structured data AD. Our motivating application is a real world
problem: monitoring the trigger system which is a basic component of many
particle physics experiments at the CERN Large Hadron Collider (LHC). In the
experiments we show the superior performance of this method for classical
machine learning (ML) benchmarks and for our application."
199,"['cs.LG', 'cs.CV', 'stat.ML']",Hypergraph Convolution and Hypergraph Attention,"Recently, graph neural networks have attracted great attention and achieved
prominent performance in various research fields. Most of those algorithms have
assumed pairwise relationships of objects of interest. However, in many real
applications, the relationships between objects are in higher-order, beyond a
pairwise formulation. To efficiently learn deep embeddings on the high-order
graph-structured data, we introduce two end-to-end trainable operators to the
family of graph neural networks, i.e., hypergraph convolution and hypergraph
attention. Whilst hypergraph convolution defines the basic formulation of
performing convolution on a hypergraph, hypergraph attention further enhances
the capacity of representation learning by leveraging an attention module. With
the two operators, a graph neural network is readily extended to a more
flexible model and applied to diverse applications where non-pairwise
relationships are observed. Extensive experimental results with semi-supervised
node classification demonstrate the effectiveness of hypergraph convolution and
hypergraph attention."
200,"['cs.LG', 'stat.ML']",HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs,"Graphs are the most ubiquitous form of structured data representation used in
machine learning. They model, however, only pairwise relations between nodes
and are not designed for encoding the higher-order relations found in many
real-world datasets. To model such complex relations, hypergraphs have proven
to be a natural representation. Learning the node representations in a
hypergraph is more complex than in a graph as it involves information
propagation at two levels: within every hyperedge and across the hyperedges.
Most current approaches first transform a hypergraph structure to a graph for
use in existing geometric deep learning algorithms. This transformation leads
to information loss, and sub-optimal exploitation of the hypergraph's
expressive power. We present HyperSAGE, a novel hypergraph learning framework
that uses a two-level neural message passing strategy to accurately and
efficiently propagate information through hypergraphs. The flexible design of
HyperSAGE facilitates different ways of aggregating neighborhood information.
Unlike the majority of related work which is transductive, our approach,
inspired by the popular GraphSAGE method, is inductive. Thus, it can also be
used on previously unseen nodes, facilitating deployment in problems such as
evolving or partially observed hypergraphs. Through extensive experimentation,
we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods
on representative benchmark datasets. We also demonstrate that the higher
expressive power of HyperSAGE makes it more stable in learning node
representations as compared to the alternatives."
201,"['cs.LG', 'cs.SI', 'stat.ML']",Meta Graph Attention on Heterogeneous Graph with Node-Edge Co-evolution,"Graph neural networks have become an important tool for modeling structured
data. In many real-world systems, intricate hidden information may exist, e.g.,
heterogeneity in nodes/edges, static node/edge attributes, and spatiotemporal
node/edge features. However, most existing methods only take part of the
information into consideration. In this paper, we present the Co-evolved Meta
Graph Neural Network (CoMGNN), which applies meta graph attention to
heterogeneous graphs with co-evolution of node and edge states. We further
propose a spatiotemporal adaption of CoMGNN (ST-CoMGNN) for modeling
spatiotemporal patterns on nodes and edges. We conduct experiments on two
large-scale real-world datasets. Experimental results show that our models
significantly outperform the state-of-the-art methods, demonstrating the
effectiveness of encoding diverse information from different aspects."
202,"['cs.LG', 'cs.AI']",Friendship is All we Need: A Multi-graph Embedding Approach for Modeling Customer Behavior,"Understanding customer behavior is fundamental for many use-cases in
industry, especially in accelerated growth areas such as fin-tech and
e-commerce. Structured data are often expensive, time-consuming and inadequate
to analyze and study complex customer behaviors. In this paper, we propose a
multi-graph embedding approach for creating a non-linear representation of
customers in order to have a better knowledge of their characteristics without
having any prior information about their financial status or their interests.
By applying the current method we are able to predict users' future behavior
with a reasonably high accuracy only by having the information of their
friendship network. Potential applications include recommendation systems and
credit risk forecasting."
203,"['cs.LG', 'stat.ML']",A Unified View on Graph Neural Networks as Graph Signal Denoising,"Graph Neural Networks (GNNs) have risen to prominence in learning
representations for graph structured data. A single GNN layer typically
consists of a feature transformation and a feature aggregation operation. The
former normally uses feed-forward networks to transform features, while the
latter aggregates the transformed features over the graph. Numerous recent
works have proposed GNN models with different designs in the aggregation
operation. In this work, we establish mathematically that the aggregation
processes in a group of representative GNN models including GCN, GAT, PPNP, and
APPNP can be regarded as (approximately) solving a graph denoising problem with
a smoothness assumption. Such a unified view across GNNs not only provides a
new perspective to understand a variety of aggregation operations but also
enables us to develop a unified graph neural network framework UGNN. To
demonstrate its promising potential, we instantiate a novel GNN model,
ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across
nodes. Comprehensive experiments show the effectiveness of ADA-UGNN."
204,"['cs.LG', 'stat.ML']",Evaluating Progress on Machine Learning for Longitudinal Electronic Healthcare Data,"The Large Scale Visual Recognition Challenge based on the well-known Imagenet
dataset catalyzed an intense flurry of progress in computer vision. Benchmark
tasks have propelled other sub-fields of machine learning forward at an equally
impressive pace, but in healthcare it has primarily been image processing
tasks, such as in dermatology and radiology, that have experienced similar
benchmark-driven progress. In the present study, we performed a comprehensive
review of benchmarks in medical machine learning for structured data,
identifying one based on the Medical Information Mart for Intensive Care
(MIMIC-III) that allows the first direct comparison of predictive performance
and thus the evaluation of progress on four clinical prediction tasks:
mortality, length of stay, phenotyping, and patient decompensation. We find
that little meaningful progress has been made over a 3 year period on these
tasks, despite significant community engagement. Through our meta-analysis, we
find that the performance of deep recurrent models is only superior to logistic
regression on certain tasks. We conclude with a synthesis of these results,
possible explanations, and a list of desirable qualities for future benchmarks
in medical machine learning."
205,"['stat.ML', 'cs.CR', 'cs.LG']",Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks,"Graph Neural Networks (GNNs), a generalization of neural networks to
graph-structured data, are often implemented using message passes between
entities of a graph. While GNNs are effective for node classification, link
prediction and graph classification, they are vulnerable to adversarial
attacks, i.e., a small perturbation to the structure can lead to a non-trivial
performance degradation. In this work, we propose Uncertainty Matching GNN
(UM-GNN), that is aimed at improving the robustness of GNN models, particularly
against poisoning attacks to the graph structure, by leveraging epistemic
uncertainties from the message passing framework. More specifically, we propose
to build a surrogate predictor that does not directly access the graph
structure, but systematically extracts reliable knowledge from a standard GNN
through a novel uncertainty-matching strategy. Interestingly, this uncoupling
makes UM-GNN immune to evasion attacks by design, and achieves significantly
improved robustness against poisoning attacks. Using empirical studies with
standard benchmarks and a suite of global and target attacks, we demonstrate
the effectiveness of UM-GNN, when compared to existing baselines including the
state-of-the-art robust GCN."
206,"['cs.LG', 'stat.ML']",GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks,"Graph structured data has wide applicability in various domains such as
physics, chemistry, biology, computer vision, and social networks, to name a
few. Recently, graph neural networks (GNN) were shown to be successful in
effectively representing graph structured data because of their good
performance and generalization ability. GNN is a deep learning based method
that learns a node representation by combining specific nodes and the
structural/topological information of a graph. However, like other deep models,
explaining the effectiveness of GNN models is a challenging task because of the
complex nonlinear transformations made over the iterations. In this paper, we
propose GraphLIME, a local interpretable model explanation for graphs using the
Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear
feature selection method. GraphLIME is a generic GNN-model explanation
framework that learns a nonlinear interpretable model locally in the subgraph
of the node being explained. More specifically, to explain a node, we generate
a nonlinear interpretable model from its $N$-hop neighborhood and then compute
the K most representative features as the explanations of its prediction using
HSIC Lasso. Through experiments on two real-world datasets, the explanations of
GraphLIME are found to be of extraordinary degree and more descriptive in
comparison to the existing explanation methods."
207,"['cs.LG', 'cs.CV']",Learning Graph Normalization for Graph Neural Networks,"Graph Neural Networks (GNNs) have attracted considerable attention and have
emerged as a new promising paradigm to process graph-structured data. GNNs are
usually stacked to multiple layers and the node representations in each layer
are computed through propagating and aggregating the neighboring node features
with respect to the graph. By stacking to multiple layers, GNNs are able to
capture the long-range dependencies among the data on the graph and thus bring
performance improvements. To train a GNN with multiple layers effectively, some
normalization techniques (e.g., node-wise normalization, batch-wise
normalization) are necessary. However, the normalization techniques for GNNs
are highly task-relevant and different application tasks prefer to different
normalization techniques, which is hard to know in advance. To tackle this
deficiency, in this paper, we propose to learn graph normalization by
optimizing a weighted combination of normalization techniques at four different
levels, including node-wise normalization, adjacency-wise normalization,
graph-wise normalization, and batch-wise normalization, in which the
adjacency-wise normalization and the graph-wise normalization are newly
proposed in this paper to take into account the local structure and the global
structure on the graph, respectively. By learning the optimal weights, we are
able to automatically select a single best or a best combination of multiple
normalizations for a specific task. We conduct extensive experiments on
benchmark datasets for different tasks, including node classification, link
prediction, graph classification and graph regression, and confirm that the
learned graph normalization leads to competitive results and that the learned
weights suggest the appropriate normalization techniques for the specific task.
Source code is released here https://github.com/cyh1112/GraphNormalization."
208,"['cs.CV', 'cs.LG']",Residual Embedding Similarity-Based Network Selection for Predicting Brain Network Evolution Trajectory from a Single Observation,"While existing predictive frameworks are able to handle Euclidean structured
data (i.e, brain images), they might fail to generalize to geometric
non-Euclidean data such as brain networks. Besides, these are rooted the sample
selection step in using Euclidean or learned similarity measure between
vectorized training and testing brain networks. Such sample connectomic
representation might include irrelevant and redundant features that could
mislead the training sample selection step. Undoubtedly, this fails to exploit
and preserve the topology of the brain connectome. To overcome this major
drawback, we propose Residual Embedding Similarity-Based Network selection
(RESNets) for predicting brain network evolution trajectory from a single
timepoint. RESNets first learns a compact geometric embedding of each training
and testing sample using adversarial connectome embedding network. This nicely
reduces the high-dimensionality of brain networks while preserving their
topological properties via graph convolutional networks. Next, to compute the
similarity between subjects, we introduce the concept of a connectional brain
template (CBT), a fixed network reference, where we further represent each
training and testing network as a deviation from the reference CBT in the
embedding space. As such, we select the most similar training subjects to the
testing subject at baseline by comparing their learned residual embeddings with
respect to the pre-defined CBT. Once the best training samples are selected at
baseline, we simply average their corresponding brain networks at follow-up
timepoints to predict the evolution trajectory of the testing network. Our
experiments on both healthy and disordered brain networks demonstrate the
success of our proposed method in comparison to RESNets ablated versions and
traditional approaches."
209,"['cs.LG', 'cs.AI', 'stat.ML']",Scalable Adversarial Attack on Graph Neural Networks with Alternating Direction Method of Multipliers,"Graph neural networks (GNNs) have achieved high performance in analyzing
graph-structured data and have been widely deployed in safety-critical areas,
such as finance and autonomous driving. However, only a few works have explored
GNNs' robustness to adversarial attacks, and their designs are usually limited
by the scale of input datasets (i.e., focusing on small graphs with only
thousands of nodes). In this work, we propose, SAG, the first scalable
adversarial attack method with Alternating Direction Method of Multipliers
(ADMM). We first decouple the large-scale graph into several smaller graph
partitions and cast the original problem into several subproblems. Then, we
propose to solve these subproblems using projected gradient descent on both the
graph topology and the node features that lead to considerably lower memory
consumption compared to the conventional attack methods. Rigorous experiments
further demonstrate that SAG can significantly reduce the computation and
memory overhead compared with the state-of-the-art approach, making SAG
applicable towards graphs with large size of nodes and edges."
210,"['cs.LG', 'stat.ML']",Source Separation with Deep Generative Priors,"Despite substantial progress in signal source separation, results for richly
structured data continue to contain perceptible artifacts. In contrast, recent
deep generative models can produce authentic samples in a variety of domains
that are indistinguishable from samples of the data distribution. This paper
introduces a Bayesian approach to source separation that uses generative models
as priors over the components of a mixture of sources, and noise-annealed
Langevin dynamics to sample from the posterior distribution of sources given a
mixture. This decouples the source separation problem from generative modeling,
enabling us to directly use cutting-edge generative models as priors. The
method achieves state-of-the-art performance for MNIST digit separation. We
introduce new methodology for evaluating separation quality on richer datasets,
providing quantitative evaluation of separation results on CIFAR-10. We also
provide qualitative results on LSUN."
211,"['cs.LG', 'stat.ML']",Learning Representations for Axis-Aligned Decision Forests through Input Perturbation,"Axis-aligned decision forests have long been the leading class of machine
learning algorithms for modeling tabular data. In many applications of machine
learning such as learning-to-rank, decision forests deliver remarkable
performance. They also possess other coveted characteristics such as
interpretability. Despite their widespread use and rich history, decision
forests to date fail to consume raw structured data such as text, or learn
effective representations for them, a factor behind the success of deep neural
networks in recent years. While there exist methods that construct smoothed
decision forests to achieve representation learning, the resulting models are
decision forests in name only: They are no longer axis-aligned, use stochastic
decisions, or are not interpretable. Furthermore, none of the existing methods
are appropriate for problems that require a Transfer Learning treatment. In
this work, we present a novel but intuitive proposal to achieve representation
learning for decision forests without imposing new restrictions or
necessitating structural changes. Our model is simply a decision forest,
possibly trained using any forest learning algorithm, atop a deep neural
network. By approximating the gradients of the decision forest through input
perturbation, a purely analytical procedure, the decision forest directs the
neural network to learn or fine-tune representations. Our framework has the
advantage that it is applicable to any arbitrary decision forest and that it
allows the use of arbitrary deep neural networks for representation learning.
We demonstrate the feasibility and effectiveness of our proposal through
experiments on synthetic and benchmark classification datasets."
212,"['cs.LG', 'cs.AI']",Deep Multi-View Spatiotemporal Virtual Graph Neural Network for Significant Citywide Ride-hailing Demand Prediction,"Urban ride-hailing demand prediction is a crucial but challenging task for
intelligent transportation system construction. Predictable ride-hailing demand
can facilitate more reasonable vehicle scheduling and online car-hailing
platform dispatch. Conventional deep learning methods with no external
structured data can be accomplished via hybrid models of CNNs and RNNs by
meshing plentiful pixel-level labeled data, but spatial data sparsity and
limited learning capabilities on temporal long-term dependencies are still two
striking bottlenecks. To address these limitations, we propose a new virtual
graph modeling method to focus on significant demand regions and a novel Deep
Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to
strengthen learning capabilities of spatial dynamics and temporal long-term
dependencies. Specifically, DMVST-VGNN integrates the structures of 1D
Convolutional Neural Network, Multi Graph Attention Neural Network and
Transformer layer, which correspond to short-term temporal dynamics view,
spatial dynamics view and long-term temporal dynamics view respectively. In
this paper, experiments are conducted on two large-scale New York City datasets
in fine-grained prediction scenes. And the experimental results demonstrate
effectiveness and superiority of DMVST-VGNN framework in significant citywide
ride-hailing demand prediction."
213,"['stat.ML', 'cs.LG']",Stochastic Graph Recurrent Neural Network,"Representation learning over graph structure data has been widely studied due
to its wide application prospects. However, previous methods mainly focus on
static graphs while many real-world graphs evolve over time. Modeling such
evolution is important for predicting properties of unseen networks. To resolve
this challenge, we propose SGRNN, a novel neural architecture that applies
stochastic latent variables to simultaneously capture the evolution in node
attributes and topology. Specifically, deterministic states are separated from
stochastic states in the iterative process to suppress mutual interference.
With semi-implicit variational inference integrated to SGRNN, a non-Gaussian
variational distribution is proposed to help further improve the performance.
In addition, to alleviate KL-vanishing problem in SGRNN, a simple and
interpretable structure is proposed based on the lower bound of KL-divergence.
Extensive experiments on real-world datasets demonstrate the effectiveness of
the proposed model. Code is available at
https://github.com/StochasticGRNN/SGRNN."
214,"['cs.LG', 'cs.CR', 'stat.ML']",Adversarial Privacy Preserving Graph Embedding against Inference Attack,"Recently, the surge in popularity of Internet of Things (IoT), mobile
devices, social media, etc. has opened up a large source for graph data. Graph
embedding has been proved extremely useful to learn low-dimensional feature
representations from graph structured data. These feature representations can
be used for a variety of prediction tasks from node classification to link
prediction. However, existing graph embedding methods do not consider users'
privacy to prevent inference attacks. That is, adversaries can infer users'
sensitive information by analyzing node representations learned from graph
embedding algorithms. In this paper, we propose Adversarial Privacy Graph
Embedding (APGE), a graph adversarial training framework that integrates the
disentangling and purging mechanisms to remove users' private information from
learned node representations. The proposed method preserves the structural
information and utility attributes of a graph while concealing users' private
attributes from inference attacks. Extensive experiments on real-world graph
datasets demonstrate the superior performance of APGE compared to the
state-of-the-arts. Our source code can be found at
https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding."
215,"['stat.ML', 'cs.LG']",Duality in RKHSs with Infinite Dimensional Outputs: Application to Robust Losses,"Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing
Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods
when the output space is a Hilbert space. Although primarily used in finite
dimension for problems like multi-task regression, the ability of this
framework to deal with infinite dimensional output spaces unlocks many more
applications, such as functional regression, structured output prediction, and
structured data representation. However, these sophisticated schemes crucially
rely on the kernel trick in the output space, so that most of previous works
have focused on the square norm loss function, completely neglecting robustness
issues that may arise in such surrogate problems. To overcome this limitation,
this paper develops a duality approach that allows to solve OVK machines for a
wide range of loss functions. The infinite dimensional Lagrange multipliers are
handled through a Double Representer Theorem, and algorithms for
$\epsilon$-insensitive losses and the Huber loss are thoroughly detailed.
Robustness benefits are emphasized by a theoretical stability analysis, as well
as empirical improvements on structured data applications."
216,"['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",Inspector Gadget: A Data Programming-based Labeling System for Industrial Images,"As machine learning for images becomes democratized in the Software 2.0 era,
one of the serious bottlenecks is securing enough labeled data for training.
This problem is especially critical in a manufacturing setting where smart
factories rely on machine learning for product quality control by analyzing
industrial images. Such images are typically large and may only need to be
partially analyzed where only a small portion is problematic (e.g., identifying
defects on a surface). Since manual labeling these images is expensive, weak
supervision is an attractive alternative where the idea is to generate weak
labels that are not perfect, but can be produced at scale. Data programming is
a recent paradigm in this category where it uses human knowledge in the form of
labeling functions and combines them into a generative model. Data programming
has been successful in applications based on text or structured data and can
also be applied to images usually if one can find a way to convert them into
structured data. In this work, we expand the horizon of data programming by
directly applying it to images without this conversion, which is a common
scenario for industrial applications. We propose Inspector Gadget, an image
labeling system that combines crowdsourcing, data augmentation, and data
programming to produce weak labels at scale for image classification. We
perform experiments on real industrial image datasets and show that Inspector
Gadget obtains better performance than other weak-labeling techniques: Snuba,
GOGGLES, and self-learning baselines using convolutional neural networks (CNNs)
without pre-training."
217,"['cs.LG', 'cs.CL', 'stat.ML']",A Capsule Network-based Model for Learning Node Embeddings,"In this paper, we focus on learning low-dimensional embeddings for nodes in
graph-structured data. To achieve this, we propose Caps2NE -- a new
unsupervised embedding model leveraging a network of two capsule layers.
Caps2NE induces a routing process to aggregate feature vectors of context
neighbors of a given target node at the first capsule layer, then feed these
features into the second capsule layer to infer a plausible embedding for the
target node. Experimental results show that our proposed Caps2NE obtains
state-of-the-art performances on benchmark datasets for the node classification
task. Our code is available at: \url{https://github.com/daiquocnguyen/Caps2NE}."
218,"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices,"Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. However, applying the same
methods on 3D data still poses challenges due to the heavy memory requirements
and the lack of structured data. Here, we propose LatticeNet, a novel approach
for 3D semantic segmentation, which takes as input raw point clouds. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on various datasets where our method
achieves state-of-the-art performance."
219,"['cs.LG', 'stat.ML']",Chart Auto-Encoders for Manifold Structured Data,"Deep generative models have made tremendous advances in image and signal
representation learning and generation. These models employ the full Euclidean
space or a bounded subset as the latent space, whose flat geometry, however, is
often too simplistic to meaningfully reflect the manifold structure of the
data. In this work, we advocate the use of a multi-chart latent space for
better data representation. Inspired by differential geometry, we propose a
\textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem
on its representation capability. We show that the training data size and the
network size scale exponentially in approximation error with an exponent
depending on the intrinsic dimension of the data manifold. CAE admits desirable
manifold properties that auto-encoders with a flat latent space fail to obey,
predominantly proximity of data. We conduct extensive experimentation with
synthetic and real-life examples to demonstrate that CAE provides
reconstruction with high fidelity, preserves proximity in the latent space, and
generates new data remaining near the manifold. These experiments show that CAE
is advantageous over existing auto-encoders and variants by preserving the
topology of the data manifold as well as its geometry."
220,"['cs.LG', 'stat.ML']",Tensor Decompositions in Recursive Neural Networks for Tree-Structured Data,"The paper introduces two new aggregation functions to encode structural
knowledge from tree-structured data. They leverage the Canonical and
Tensor-Train decompositions to yield expressive context aggregation while
limiting the number of model parameters. Finally, we define two novel neural
recursive models for trees leveraging such aggregation functions, and we test
them on two tree classification tasks, showing the advantage of proposed models
when tree outdegree increases."
221,"['cs.LG', 'stat.ML']",Incidence Networks for Geometric Deep Learning,"Sparse incidence tensors can represent a variety of structured data. For
example, we may represent attributed graphs using their node-node, node-edge,
or edge-edge incidence matrices. In higher dimensions, incidence tensors can
represent simplicial complexes and polytopes. In this paper, we formalize
incidence tensors, analyze their structure, and present the family of
equivariant networks that operate on them. We show that any incidence tensor
decomposes into invariant subsets. This decomposition, in turn, leads to a
decomposition of the corresponding equivariant linear maps, for which we prove
an efficient pooling-and-broadcasting implementation."
222,"['cs.LG', 'stat.ML']",Epitomic Variational Graph Autoencoder,"Variational autoencoder (VAE) is a widely used generative model for learning
latent representations. Burda et al. in their seminal paper showed that
learning capacity of VAE is limited by over-pruning. It is a phenomenon where a
significant number of latent variables fail to capture any information about
the input data and the corresponding hidden units become inactive. This
adversely affects learning diverse and interpretable latent representations. As
variational graph autoencoder (VGAE) extends VAE for graph-structured data, it
inherits the over-pruning problem. In this paper, we adopt a model based
approach and propose epitomic VGAE (EVGAE),a generative variational framework
for graph datasets which successfully mitigates the over-pruning problem and
also boosts the generative ability of VGAE. We consider EVGAE to consist of
multiple sparse VGAE models, called epitomes, that are groups of latent
variables sharing the latent space. This approach aids in increasing active
units as epitomes compete to learn better representation of the graph data. We
verify our claims via experiments on three benchmark datasets. Our experiments
show that EVGAE has a better generative ability than VGAE. Moreover, EVGAE
outperforms VGAE on link prediction task in citation networks."
223,"['cs.LG', 'stat.ML']",SEAL: Semi-supervised Adversarial Active Learning on Attributed Graphs,"Active learning (AL) on attributed graphs has received increasing attention
with the prevalence of graph-structured data. Although AL has been widely
studied for alleviating label sparsity issues with the conventional non-related
data, how to make it effective over attributed graphs remains an open research
question. Existing AL algorithms on graphs attempt to reuse the classic AL
query strategies designed for non-related data. However, they suffer from two
major limitations. First, different AL query strategies calculated in distinct
scoring spaces are often naively combined to determine which nodes to be
labelled. Second, the AL query engine and the learning of the classifier are
treated as two separating processes, resulting in unsatisfactory performance.
In this paper, we propose a SEmi-supervised Adversarial active Learning (SEAL)
framework on attributed graphs, which fully leverages the representation power
of deep neural networks and devises a novel AL query strategy in an adversarial
way. Our framework learns two adversarial components: a graph embedding network
that encodes both the unlabelled and labelled nodes into a latent space,
expecting to trick the discriminator to regard all nodes as already labelled,
and a semi-supervised discriminator network that distinguishes the unlabelled
from the existing labelled nodes in the latent space. The divergence score,
generated by the discriminator in a unified latent space, serves as the
informativeness measure to actively select the most informative node to be
labelled by an oracle. The two adversarial components form a closed loop to
mutually and simultaneously reinforce each other towards enhancing the active
learning performance. Extensive experiments on four real-world networks
validate the effectiveness of the SEAL framework with superior performance
improvements to state-of-the-art baselines."
224,['cs.CV'],Mix Dimension in Poincar Geometry for 3D Skeleton-based Action Recognition,"Graph Convolutional Networks (GCNs) have already demonstrated their powerful
ability to model the irregular data, e.g., skeletal data in human action
recognition, providing an exciting new way to fuse rich structural information
for nodes residing in different parts of a graph. In human action recognition,
current works introduce a dynamic graph generation mechanism to better capture
the underlying semantic skeleton connections and thus improves the performance.
In this paper, we provide an orthogonal way to explore the underlying
connections. Instead of introducing an expensive dynamic graph generation
paradigm, we build a more efficient GCN on a Riemann manifold, which we think
is a more suitable space to model the graph data, to make the extracted
representations fit the embedding matrix. Specifically, we present a novel
spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\'e
geometry such that it is able to better model the latent anatomy of the
structure data. To further explore the optimal projection dimension in the
Riemann space, we mix different dimensions on the manifold and provide an
efficient way to explore the dimension for each ST-GCN layer. With the final
resulted architecture, we evaluate our method on two current largest scale 3D
datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that
the model could achieve a superior performance under any given evaluation
metrics with only 40\% model size when compared with the previous best GCN
method, which proves the effectiveness of our model."
225,"['cs.LG', 'cs.SI', 'eess.SP', 'stat.ML']",Graph signal processing for machine learning: A review and new perspectives,"The effective representation, processing, analysis, and visualization of
large-scale structured data, especially those related to complex domains such
as networks and graphs, are one of the key questions in modern machine
learning. Graph signal processing (GSP), a vibrant branch of signal processing
models and algorithms that aims at handling data supported on graphs, opens new
paths of research to address this challenge. In this article, we review a few
important contributions made by GSP concepts and tools, such as graph filters
and transforms, to the development of novel machine learning algorithms. In
particular, our discussion focuses on the following three aspects: exploiting
data structure and relational priors, improving data and computational
efficiency, and enhancing model interpretability. Furthermore, we provide new
perspectives on future development of GSP techniques that may serve as a bridge
between applied mathematics and signal processing on one side, and machine
learning and network science on the other. Cross-fertilization across these
different disciplines may help unlock the numerous challenges of complex data
analysis in the modern age."
226,"['cs.CV', 'cs.LG', 'stat.ML']",Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis,"Understanding how certain brain regions relate to a specific neurological
disorder has been an important area of neuroimaging research. A promising
approach to identify the salient regions is using Graph Neural Networks (GNNs),
which can be used to analyze graph structured data, e.g. brain networks
constructed by functional magnetic resonance imaging (fMRI). We propose an
interpretable GNN framework with a novel salient region selection mechanism to
determine neurological brain biomarkers associated with disorders.
Specifically, we design novel regularized pooling layers that highlight salient
regions of interests (ROIs) so that we can infer which ROIs are important to
identify a certain disease based on the node pooling scores calculated by the
pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN),
encourages reasonable ROI-selection and provides flexibility to preserve either
individual- or group-level patterns. We apply the PR-GNN framework on a
Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different
choices of the hyperparameters and show that PR-GNN outperforms baseline
methods in terms of classification accuracy. The salient ROI detection results
show high correspondence with the previous neuroimaging-derived biomarkers for
ASD."
227,"['stat.ML', 'cs.LG', 'eess.IV']",Additive Tensor Decomposition Considering Structural Data Information,"Tensor data with rich structural information becomes increasingly important
in process modeling, monitoring, and diagnosis. Here structural information is
referred to structural properties such as sparsity, smoothness, low-rank, and
piecewise constancy. To reveal useful information from tensor data, we propose
to decompose the tensor into the summation of multiple components based on
different structural information of them. In this paper, we provide a new
definition of structural information in tensor data. Based on it, we propose an
additive tensor decomposition (ATD) framework to extract useful information
from tensor data. This framework specifies a high dimensional optimization
problem to obtain the components with distinct structural information. An
alternating direction method of multipliers (ADMM) algorithm is proposed to
solve it, which is highly parallelable and thus suitable for the proposed
optimization problem. Two simulation examples and a real case study in medical
image analysis illustrate the versatility and effectiveness of the ATD
framework."
228,"['cs.LG', 'cs.IR', 'stat.ML']",Hierarchical BiGraph Neural Network as Recommendation Systems,"Graph neural networks emerge as a promising modeling method for applications
dealing with datasets that are best represented in the graph domain. In
specific, developing recommendation systems often require addressing sparse
structured data which often lacks the feature richness in either the user
and/or item side and requires processing within the correct context for optimal
performance. These datasets intuitively can be mapped to and represented as
networks or graphs. In this paper, we propose the Hierarchical BiGraph Neural
Network (HBGNN), a hierarchical approach of using GNNs as recommendation
systems and structuring the user-item features using a bigraph framework. Our
experimental results show competitive performance with current recommendation
system methods and transferability."
229,"['cs.LG', 'cs.SI']",Graph Convolutional Networks using Heat Kernel for Semi-supervised Learning,"Graph convolutional networks gain remarkable success in semi-supervised
learning on graph structured data. The key to graph-based semisupervised
learning is capturing the smoothness of labels or features over nodes exerted
by graph structure. Previous methods, spectral methods and spatial methods,
devote to defining graph convolution as a weighted average over neighboring
nodes, and then learn graph convolution kernels to leverage the smoothness to
improve the performance of graph-based semi-supervised learning. One open
challenge is how to determine appropriate neighborhood that reflects relevant
information of smoothness manifested in graph structure. In this paper, we
propose GraphHeat, leveraging heat kernel to enhance low-frequency filters and
enforce smoothness in the signal variation on the graph. GraphHeat leverages
the local structure of target node under heat diffusion to determine its
neighboring nodes flexibly, without the constraint of order suffered by
previous methods. GraphHeat achieves state-of-the-art results in the task of
graph-based semi-supervised classification across three benchmark datasets:
Cora, Citeseer and Pubmed."
230,"['stat.ML', 'cs.LG']",MetAL: Active Semi-Supervised Learning on Graphs via Meta Learning,"The objective of active learning (AL) is to train classification models with
less number of labeled instances by selecting only the most informative
instances for labeling. The AL algorithms designed for other data types such as
images and text do not perform well on graph-structured data. Although a few
heuristics-based AL algorithms have been proposed for graphs, a principled
approach is lacking. In this paper, we propose MetAL, an AL approach that
selects unlabeled instances that directly improve the future performance of a
classification model. For a semi-supervised learning problem, we formulate the
AL task as a bilevel optimization problem. Based on recent work in
meta-learning, we use the meta-gradients to approximate the impact of
retraining the model with any unlabeled instance on the model performance.
Using multiple graph datasets belonging to different domains, we demonstrate
that MetAL efficiently outperforms existing state-of-the-art AL algorithms."
231,['cs.LG'],Tensor Regression Networks,"Convolutional neural networks typically consist of many convolutional layers
followed by one or more fully connected layers. While convolutional layers map
between high-order activation tensors, the fully connected layers operate on
flattened activation vectors. Despite empirical success, this approach has
notable drawbacks. Flattening followed by fully connected layers discards
multilinear structure in the activations and requires many parameters. We
address these problems by incorporating tensor algebraic operations that
preserve multilinear structure at every layer. First, we introduce Tensor
Contraction Layers (TCLs) that reduce the dimensionality of their input while
preserving their multilinear structure using tensor contraction. Next, we
introduce Tensor Regression Layers (TRLs), which express outputs through a
low-rank multilinear mapping from a high-order activation tensor to an output
tensor of arbitrary order. We learn the contraction and regression factors
end-to-end, and produce accurate nets with fewer parameters. Additionally, our
layers regularize networks by imposing low-rank constraints on the activations
(TCL) and regression weights (TRL). Experiments on ImageNet show that, applied
to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters
compared to fully connected layers by more than 65% while maintaining or
increasing accuracy. In addition to the space savings, our approach's ability
to leverage topological structure can be crucial for structured data such as
MRI. In particular, we demonstrate significant performance improvements over
comparable architectures on three tasks associated with the UK Biobank dataset."
232,"['cs.LG', 'q-bio.QM', 'stat.ML']",A Hierarchical Approach to Scaling Batch Active Search Over Structured Data,"Active search is the process of identifying high-value data points in a large
and often high-dimensional parameter space that can be expensive to evaluate.
Traditional active search techniques like Bayesian optimization trade off
exploration and exploitation over consecutive evaluations, and have
historically focused on single or small (<5) numbers of examples evaluated per
round. As modern data sets grow, so does the need to scale active search to
large data sets and batch sizes. In this paper, we present a general
hierarchical framework based on bandit algorithms to scale active search to
large batch sizes by maximizing information derived from the unique structure
of each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search
(HBBS), strategically distributes batch selection across a learned embedding
space by facilitating wide exploration of different structural elements within
a dataset. We focus our application of HBBS on modern biology, where large
batch experimentation is often fundamental to the research process, and
demonstrate batch design of biological sequences (protein and DNA). We also
present a new Gym environment to easily simulate diverse biological sequences
and to enable more comprehensive evaluation of active search methods across
heterogeneous data sets. The HBBS framework improves upon standard performance,
wall-clock, and scalability benchmarks for batch search by using a broad
exploration strategy across coarse partitions and fine-grained exploitation
within each partition of structured data."
233,"['cs.LG', 'stat.ML']",Few-shot link prediction via graph neural networks for Covid-19 drug-repurposing,"Predicting interactions among heterogenous graph structured data has numerous
applications such as knowledge graph completion, recommendation systems and
drug discovery. Often times, the links to be predicted belong to rare types
such as the case in repurposing drugs for novel diseases. This motivates the
task of few-shot link prediction. Typically, GCNs are ill-equipped in learning
such rare link types since the relation embedding is not learned in an
inductive fashion. This paper proposes an inductive RGCN for learning
informative relation embeddings even in the few-shot learning regime. The
proposed inductive model significantly outperforms the RGCN and
state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply
our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs
for Covid-19. We pose the drug discovery task as link prediction and learn
embeddings for the biological entities that partake in the DRKG. Our initial
results corroborate that several drugs used in clinical trials were identified
as possible drug candidates. The method in this paper are implemented using the
efficient deep graph learning (DGL)"
234,['cs.CV'],Deep Representation Learning For Multimodal Brain Networks,"Applying network science approaches to investigate the functions and anatomy
of the human brain is prevalent in modern medical imaging analysis. Due to the
complex network topology, for an individual brain, mining a discriminative
network representation from the multimodal brain networks is non-trivial. The
recent success of deep learning techniques on graph-structured data suggests a
new way to model the non-linear cross-modality relationship. However, current
deep brain network methods either ignore the intrinsic graph topology or
require a network basis shared within a group. To address these challenges, we
propose a novel end-to-end deep graph representation learning (Deep Multimodal
Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we
decipher the cross-modality relationship through a graph encoding and decoding
process. The higher-order network mappings from brain structural networks to
functional networks are learned in the node domain. The learned network
representation is a set of node features that are informative to induce brain
saliency maps in a supervised manner. We test our framework in both synthetic
and real image data. The experimental results show the superiority of the
proposed method over some other state-of-the-art deep brain network models."
235,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",Extracting Structured Data from Physician-Patient Conversations By Predicting Noteworthy Utterances,"Despite diverse efforts to mine various modalities of medical data, the
conversations between physicians and patients at the time of care remain an
untapped source of insights. In this paper, we leverage this data to extract
structured information that might assist physicians with post-visit
documentation in electronic health records, potentially lightening the clerical
burden. In this exploratory study, we describe a new dataset consisting of
conversation transcripts, post-visit summaries, corresponding supporting
evidence (in the transcript), and structured labels. We focus on the tasks of
recognizing relevant diagnoses and abnormalities in the review of organ systems
(RoS). One methodological challenge is that the conversations are long (around
1500 words), making it difficult for modern deep-learning models to use them as
input. To address this challenge, we extract noteworthy utterances---parts of
the conversation likely to be cited as evidence supporting some summary
sentence. We find that by first filtering for (predicted) noteworthy
utterances, we can significantly boost predictive performance for recognizing
both diagnoses and RoS abnormalities."
236,"['cs.LG', 'stat.ML']",Deep Graph Contrastive Representation Learning,"Graph representation learning nowadays becomes fundamental in analyzing
graph-structured data. Inspired by recent success of contrastive methods, in
this paper, we propose a novel framework for unsupervised graph representation
learning by leveraging a contrastive objective at the node level. Specifically,
we generate two graph views by corruption and learn node representations by
maximizing the agreement of node representations in these two views. To provide
diverse node contexts for the contrastive objective, we propose a hybrid scheme
for generating graph views on both structure and attribute levels. Besides, we
provide theoretical justification behind our motivation from two perspectives,
mutual information and the classical triplet loss. We perform empirical
experiments on both transductive and inductive learning tasks using a variety
of real-world datasets. Experimental experiments demonstrate that despite its
simplicity, our proposed method consistently outperforms existing
state-of-the-art methods by large margins. Moreover, our unsupervised method
even surpasses its supervised counterparts on transductive tasks, demonstrating
its great potential in real-world applications."
237,"['cs.CV', 'cs.AI', 'cs.DB', 'cs.MM']",Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing,"Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds."
238,"['cs.LG', 'stat.ML']",Learning distributed representations of graphs with Geo2DR,"We present Geo2DR (Geometric to Distributed Representations), a GPU ready
Python library for unsupervised learning on graph-structured data using
discrete substructure patterns and neural language models. It contains
efficient implementations of popular graph decomposition algorithms and neural
language models in PyTorch which can be combined to learn representations of
graphs using the distributive hypothesis. Furthermore, Geo2DR comes with
general data processing and loading methods to bring substantial speed-up in
the training of the neural language models. Through this we provide a modular
set of tools and methods to quickly construct systems capable of learning
distributed representations of graphs. This is useful for replication of
existing methods, modification, or development of completely new methods. This
paper serves to present the Geo2DR library and perform a comprehensive
comparative analysis of existing methods re-implemented using Geo2DR across
widely used graph classification benchmarks. Geo2DR displays a high
reproducibility of results in published methods and interoperability with other
libraries useful for distributive language modelling."
239,"['cs.LG', 'cond-mat.dis-nn', 'cs.NI', 'physics.data-an', 'stat.ML']",Path Integral Based Convolution and Pooling for Graph Neural Networks,"Graph neural networks (GNNs) extends the functionality of traditional neural
networks to graph-structured data. Similar to CNNs, an optimized design of
graph convolution and pooling is key to success. Borrowing ideas from physics,
we propose a path integral based graph neural networks (PAN) for classification
and regression tasks on graphs. Specifically, we consider a convolution
operation that involves every path linking the message sender and receiver with
learnable weights depending on the path length, which corresponds to the
maximal entropy random walk. It generalizes the graph Laplacian to a new
transition matrix we call maximal entropy transition (MET) matrix derived from
a path integral formalism. Importantly, the diagonal entries of the MET matrix
are directly related to the subgraph centrality, thus providing a natural and
adaptive pooling mechanism. PAN provides a versatile framework that can be
tailored for different graph data with varying sizes and structures. We can
view most existing GNN architectures as special cases of PAN. Experimental
results show that PAN achieves state-of-the-art performance on various graph
classification/regression tasks, including a new benchmark dataset from
statistical mechanics we propose to boost applications of GNN in physical
sciences."
240,"['cs.LG', 'stat.ML']",Simple and Deep Graph Convolutional Networks,"Graph convolutional networks (GCNs) are a powerful deep learning approach for
graph-structured data. Recently, GCNs and subsequent variants have shown
superior performance in various application areas on real-world datasets.
Despite their success, most of the current GCN models are shallow, due to the
{\em over-smoothing} problem. In this paper, we study the problem of designing
and analyzing deep graph convolutional networks. We propose the GCNII, an
extension of the vanilla GCN model with two simple yet effective techniques:
{\em Initial residual} and {\em Identity mapping}. We provide theoretical and
empirical evidence that the two techniques effectively relieves the problem of
over-smoothing. Our experiments show that the deep GCNII model outperforms the
state-of-the-art methods on various semi- and full-supervised tasks. Code is
available at https://github.com/chennnM/GCNII ."
241,"['cs.LG', 'stat.ML']",Spatial Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have recently become the primary choice
for learning from graph-structured data, superseding hash fingerprints in
representing chemical compounds. However, GCNs lack the ability to take into
account the ordering of node neighbors, even when there is a geometric
interpretation of the graph vertices that provides an order based on their
spatial positions. To remedy this issue, we propose Spatial Graph Convolutional
Network (SGCN) which uses spatial features to efficiently learn from graphs
that can be naturally located in space. Our contribution is threefold: we
propose a GCN-inspired architecture which (i) leverages node positions, (ii) is
a proper generalization of both GCNs and Convolutional Neural Networks (CNNs),
(iii) benefits from augmentation which further improves the performance and
assures invariance with respect to the desired properties. Empirically, SGCN
outperforms state-of-the-art graph-based methods on image classification and
chemical tasks."
242,"['cs.LG', 'cs.AI', 'cs.IR']",Coupling Learning of Complex Interactions,"Complex applications such as big data analytics involve different forms of
coupling relationships that reflect interactions between factors related to
technical, business (domain-specific) and environmental (including
socio-cultural and economic) aspects. There are diverse forms of couplings
embedded in poor-structured and ill-structured data. Such couplings are
ubiquitous, implicit and/or explicit, objective and/or subjective,
heterogeneous and/or homogeneous, presenting complexities to existing learning
systems in statistics, mathematics and computer sciences, such as typical
dependency, association and correlation relationships. Modeling and learning
such couplings thus is fundamental but challenging. This paper discusses the
concept of coupling learning, focusing on the involvement of coupling
relationships in learning systems. Coupling learning has great potential for
building a deep understanding of the essence of business problems and handling
challenges that have not been addressed well by existing learning theories and
tools. This argument is verified by several case studies on coupling learning,
including handling coupling in recommender systems, incorporating couplings
into coupled clustering, coupling document clustering, coupled recommender
algorithms and coupled behavior analysis for groups."
243,"['stat.ML', 'cs.LG']",Convolutional Kernel Networks for Graph-Structured Data,"We introduce a family of multilayer graph kernels and establish new links
between graph convolutional neural networks and kernel methods. Our approach
generalizes convolutional kernel networks to graph-structured data, by
representing graphs as a sequence of kernel feature maps, where each node
carries information about local graph substructures. On the one hand, the
kernel point of view offers an unsupervised, expressive, and easy-to-regularize
data representation, which is useful when limited samples are available. On the
other hand, our model can also be trained end-to-end on large-scale data,
leading to new types of graph convolutional neural networks. We show that our
method achieves competitive performance on several graph classification
benchmarks, while offering simple model interpretation. Our code is freely
available at https://github.com/claying/GCKN."
244,"['cs.LG', 'stat.ML']",Beyond Textual Data: Predicting Drug-Drug Interactions from Molecular Structure Images using Siamese Neural Networks,"Predicting and discovering drug-drug interactions (DDIs) is an important
problem and has been studied extensively both from medical and machine learning
point of view. Almost all of the machine learning approaches have focused on
text data or textual representation of the structural data of drugs. We present
the first work that uses drug structure images as the input and utilizes a
Siamese convolutional network architecture to predict DDIs."
245,"['cs.LG', 'stat.ML']",Structural Landmarking and Interaction Modelling: on Resolution Dilemmas in Graph Classification,"Graph neural networks are promising architecture for learning and inference
with graph-structured data. Yet difficulties in modelling the ``parts'' and
their ``interactions'' still persist in terms of graph classification, where
graph-level representations are usually obtained by squeezing the whole graph
into a single vector through graph pooling. From complex systems point of view,
mixing all the parts of a system together can affect both model
interpretability and predictive performance, because properties of a complex
system arise largely from the interaction among its components. We analyze the
intrinsic difficulty in graph classification under the unified concept of
``resolution dilemmas'' with learning theoretic recovery guarantees, and
propose ``SLIM'', an inductive neural network model for Structural Landmarking
and Interaction Modelling. It turns out, that by solving the resolution
dilemmas, and leveraging explicit interacting relation between component parts
of a graph to explain its complexity, SLIM is more interpretable, accurate, and
offers new insight in graph representation learning."
246,"['cs.LG', 'cs.SI', 'stat.ML']",GPT-GNN: Generative Pre-Training of Graph Neural Networks,"Graph neural networks (GNNs) have been demonstrated to be powerful in
modeling graph-structured data. However, training GNNs usually requires
abundant task-specific labeled data, which is often arduously expensive to
obtain. One effective way to reduce the labeling effort is to pre-train an
expressive GNN model on unlabeled data with self-supervision and then transfer
the learned model to downstream tasks with only a few labels. In this paper, we
present the GPT-GNN framework to initialize GNNs by generative pre-training.
GPT-GNN introduces a self-supervised attributed graph generation task to
pre-train a GNN so that it can capture the structural and semantic properties
of the graph. We factorize the likelihood of the graph generation into two
components: 1) Attribute Generation and 2) Edge Generation. By modeling both
components, GPT-GNN captures the inherent dependency between node attributes
and graph structure during the generative process. Comprehensive experiments on
the billion-scale Open Academic Graph and Amazon recommendation data
demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models
without pre-training by up to 9.1% across various downstream tasks."
247,"['cs.LG', 'eess.SP', 'math.OC', 'stat.ML']",Fast Learning of Graph Neural Networks with Guaranteed Generalizability: One-hidden-layer Case,"Although graph neural networks (GNNs) have made great progress recently on
learning from graph-structured data in practice, their theoretical guarantee on
generalizability remains elusive in the literature. In this paper, we provide a
theoretically-grounded generalizability analysis of GNNs with one hidden layer
for both regression and binary classification problems. Under the assumption
that there exists a ground-truth GNN model (with zero generalization error),
the objective of GNN learning is to estimate the ground-truth GNN parameters
from the training data. To achieve this objective, we propose a learning
algorithm that is built on tensor initialization and accelerated gradient
descent. We then show that the proposed learning algorithm converges to the
ground-truth GNN model for the regression problem, and to a model sufficiently
close to the ground-truth for the binary classification problem. Moreover, for
both cases, the convergence rate of the proposed learning algorithm is proven
to be linear and faster than the vanilla gradient descent algorithm. We further
explore the relationship between the sample complexity of GNNs and their
underlying graph properties. Lastly, we provide numerical experiments to
demonstrate the validity of our analysis and the effectiveness of the proposed
learning algorithm for GNNs."
248,"['stat.ML', 'cs.LG']",Non-Parametric Graph Learning for Bayesian Graph Neural Networks,"Graphs are ubiquitous in modelling relational structures. Recent endeavours
in machine learning for graph-structured data have led to many architectures
and learning algorithms. However, the graph used by these algorithms is often
constructed based on inaccurate modelling assumptions and/or noisy data. As a
result, it fails to represent the true relationships between nodes. A Bayesian
framework which targets posterior inference of the graph by considering it as a
random quantity can be beneficial. In this paper, we propose a novel
non-parametric graph model for constructing the posterior distribution of graph
adjacency matrices. The proposed model is flexible in the sense that it can
effectively take into account the output of graph-based learning algorithms
that target specific tasks. In addition, model inference scales well to large
graphs. We demonstrate the advantages of this model in three different problem
settings: node classification, link prediction and recommendation."
249,"['cs.LG', 'stat.ML']",A Note on Over-Smoothing for Graph Neural Networks,"Graph Neural Networks (GNNs) have achieved a lot of success on
graph-structured data. However, it is observed that the performance of graph
neural networks does not improve as the number of layers increases. This
effect, known as over-smoothing, has been analyzed mostly in linear cases. In
this paper, we build upon previous results \cite{oono2019graph} to further
analyze the over-smoothing effect in the general graph neural network
architecture. We show when the weight matrix satisfies the conditions
determined by the spectrum of augmented normalized Laplacian, the Dirichlet
energy of embeddings will converge to zero, resulting in the loss of
discriminative power. Using Dirichlet energy to measure ""expressiveness"" of
embedding is conceptually clean; it leads to simpler proofs than
\cite{oono2019graph} and can handle more non-linearities."
250,"['cs.LG', 'stat.ML']",Adaptive-Step Graph Meta-Learner for Few-Shot Graph Classification,"Graph classification aims to extract accurate information from
graph-structured data for classification and is becoming more and more
important in graph learning community. Although Graph Neural Networks (GNNs)
have been successfully applied to graph classification tasks, most of them
overlook the scarcity of labeled graph data in many applications. For example,
in bioinformatics, obtaining protein graph labels usually needs laborious
experiments. Recently, few-shot learning has been explored to alleviate this
problem with only given a few labeled graph samples of test classes. The shared
sub-structures between training classes and test classes are essential in
few-shot graph classification. Exiting methods assume that the test classes
belong to the same set of super-classes clustered from training classes.
However, according to our observations, the label spaces of training classes
and test classes usually do not overlap in real-world scenario. As a result,
the existing methods don't well capture the local structures of unseen test
classes. To overcome the limitation, in this paper, we propose a direct method
to capture the sub-structures with well initialized meta-learner within a few
adaptation steps. More specifically, (1) we propose a novel framework
consisting of a graph meta-learner, which uses GNNs based modules for fast
adaptation on graph data, and a step controller for the robustness and
generalization of meta-learner; (2) we provide quantitative analysis for the
framework and give a graph-dependent upper bound of the generalization error
based on our framework; (3) the extensive experiments on real-world datasets
demonstrate that our framework gets state-of-the-art results on several
few-shot graph classification tasks compared to baselines."
251,"['cs.LG', 'stat.ML']",Graph Neural Networks for Node-Level Predictions,"The success of deep learning has revolutionized many fields of research
including areas of computer vision, text and speech processing. Enormous
research efforts have led to numerous methods that are capable of efficiently
analyzing data, especially in the Euclidean space. However, many problems are
posed in non-Euclidean domains modeled as general graphs with complex
connection patterns. Increased problem complexity and computational power
constraints have limited early approaches to static and small-sized graphs. In
recent years, a rising interest in machine learning on graph-structured data
has been accompanied by improved methods that overcome the limitations of their
predecessors. These methods paved the way for dealing with large-scale and
time-dynamic graphs. This work aims to provide an overview of early and modern
graph neural network based machine learning methods for node-level prediction
tasks. Under the umbrella of taxonomies already established in the literature,
we explain the core concepts and provide detailed explanations for
convolutional methods that have had strong impact. In addition, we introduce
common benchmarks and present selected applications from various areas.
Finally, we discuss open problems for further research."
252,"['cs.LG', 'cs.AI', 'stat.ML']",Verifying Individual Fairness in Machine Learning Models,"We consider the problem of whether a given decision model, working with
structured data, has individual fairness. Following the work of Dwork, a model
is individually biased (or unfair) if there is a pair of valid inputs which are
close to each other (according to an appropriate metric) but are treated
differently by the model (different class label, or large difference in
output), and it is unbiased (or fair) if no such pair exists. Our objective is
to construct verifiers for proving individual fairness of a given model, and we
do so by considering appropriate relaxations of the problem. We construct
verifiers which are sound but not complete for linear classifiers, and
kernelized polynomial/radial basis function classifiers. We also report the
experimental results of evaluating our proposed algorithms on publicly
available datasets."
253,"['cs.LG', 'cs.NE', 'stat.ML']",Generalising Recursive Neural Models by Tensor Decomposition,"Most machine learning models for structured data encode the structural
knowledge of a node by leveraging simple aggregation functions (in neural
models, typically a weighted sum) of the information in the node's
neighbourhood. Nevertheless, the choice of simple context aggregation
functions, such as the sum, can be widely sub-optimal. In this work we
introduce a general approach to model aggregation of structural context
leveraging a tensor-based formulation. We show how the exponential growth in
the size of the parameter space can be controlled through an approximation
based on the Tucker tensor decomposition. This approximation allows limiting
the parameters space size, decoupling it from its strict relation with the size
of the hidden encoding space. By this means, we can effectively regulate the
trade-off between expressivity of the encoding, controlled by the hidden size,
computational complexity and model generalisation, influenced by
parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an
instance of our framework and we use it to experimentally assess our working
hypotheses on tree classification scenarios."
254,"['cs.LG', 'stat.ML']",Differentiable Graph Module (DGM) for Graph Convolutional Networks,"Graph deep learning has recently emerged as a powerful ML concept allowing to
generalize successful deep neural architectures to non-Euclidean structured
data. Such methods have shown promising results on a broad spectrum of
applications ranging from social science, biomedicine, and particle physics to
computer vision, graphics, and chemistry. One of the limitations of the
majority of the current graph neural network architectures is that they are
often restricted to the transductive setting and rely on the assumption that
the underlying graph is known and fixed. In many settings, such as those
arising in medical and healthcare applications, this assumption is not
necessarily true since the graph may be noisy, partially- or even completely
unknown, and one is thus interested in inferring it from the data. This is
especially important in inductive settings when dealing with nodes not present
in the graph at training time. Furthermore, sometimes such a graph itself may
convey insights that are even more important than the downstream task. In this
paper, we introduce Differentiable Graph Module (DGM), a learnable function
predicting the edge probability in the graph relevant for the task, that can be
combined with convolutional graph neural network layers and trained in an
end-to-end fashion. We provide an extensive evaluation of applications from the
domains of healthcare (disease prediction), brain imaging (gender and age
prediction), computer graphics (3D point cloud segmentation), and computer
vision (zero-shot learning). We show that our model provides a significant
improvement over baselines both in transductive and inductive settings and
achieves state-of-the-art results."
255,"['cs.LG', 'stat.ML']",Cryptotree: fast and accurate predictions on encrypted structured data,"Applying machine learning algorithms to private data, such as financial or
medical data, while preserving their confidentiality, is a difficult task.
Homomorphic Encryption (HE) is acknowledged for its ability to allow
computation on encrypted data, where both the input and output are encrypted,
which therefore enables secure inference on private data. Nonetheless, because
of the constraints of HE, such as its inability to evaluate non-polynomial
functions or to perform arbitrary matrix multiplication efficiently, only
inference of linear models seem usable in practice in the HE paradigm so far.
  In this paper, we propose Cryptotree, a framework that enables the use of
Random Forests (RF), a very powerful learning procedure compared to linear
regression, in the context of HE. To this aim, we first convert a regular RF to
a Neural RF, then adapt this to fit the HE scheme CKKS, which allows HE
operations on real values. Through SIMD operations, we are able to have quick
inference and prediction results better than the original RF on encrypted data."
256,"['cs.LG', 'stat.ML']",Graph Representation Learning Network via Adaptive Sampling,"Graph Attention Network (GAT) and GraphSAGE are neural network architectures
that operate on graph-structured data and have been widely studied for link
prediction and node classification. One challenge raised by GraphSAGE is how to
smartly combine neighbour features based on graph structure. GAT handles this
problem through attention, however the challenge with GAT is its scalability
over large and dense graphs. In this work, we proposed a new architecture to
address these issues that is more efficient and is capable of incorporating
different edge type information. It generates node representations by attending
to neighbours sampled from weighted multi-step transition probabilities. We
conduct experiments on both transductive and inductive settings. Experiments
achieved comparable or better results on several graph benchmarks, including
the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets."
257,"['cs.LG', 'stat.ML']",One-Class Graph Neural Networks for Anomaly Detection in Attributed Networks,"Nowadays, graph-structured data are increasingly used to model complex
systems. Meanwhile, detecting anomalies from graph has become a vital research
problem of pressing societal concerns. Anomaly detection is an unsupervised
learning task of identifying rare data that differ from the majority. As one of
the dominant anomaly detection algorithms, One Class Support Vector Machine has
been widely used to detect outliers. However, those traditional anomaly
detection methods lost their effectiveness in graph data. Since traditional
anomaly detection methods are stable, robust and easy to use, it is vitally
important to generalize them to graph data. In this work, we propose One Class
Graph Neural Network (OCGNN), a one-class classification framework for graph
anomaly detection. OCGNN is designed to combine the powerful representation
ability of Graph Neural Networks along with the classical one-class objective.
Compared with other baselines, OCGNN achieves significant improvements in
extensive experiments."
258,"['cs.CV', 'cs.LG', 'stat.ML']","Location, location, location: Satellite image-based real-estate appraisal","Buying a home is one of the most important buying decisions people have to
make in their life. The latest research on real-estate appraisal focuses on
incorporating image data in addition to structured data into the modeling
process. This research measures the prediction performance of satellite images
and structured data by using convolutional neural networks. The resulting CNN
model trained performs 7% better in MAE than the advanced baseline of a neural
network trained on structured data. Moreover, sliding-window heatmap provides
visual interpretability of satellite images, revealing that neighborhood
structures are essential in the price estimation."
259,['cs.CV'],Self-supervised Training of Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have been successfully applied to analyze
non-grid data, where the classical convolutional neural networks (CNNs) cannot
be directly used. One similarity shared by GCNs and CNNs is the requirement of
massive amount of labeled data for network training. In addition, GCNs need the
adjacency matrix as input to define the relationship between those non-grid
data, which leads to all of data including training, validation and test data
typically forms only one graph structures data for training. Furthermore, the
adjacency matrix is usually pre-defined and stationary, which makes the data
augmentation strategies cannot be employed on the constructed graph structures
data to augment the amount of training data. To further improve the learning
capacity and model performance under the limited training data, in this paper,
we propose two types of self-supervised learning strategies to exploit
available information from the input graph structure data itself. Our proposed
self-supervised learning strategies are examined on two representative GCN
models with three public citation network datasets - Citeseer, Cora and Pubmed.
The experimental results demonstrate the generalization ability as well as the
portability of our proposed strategies, which can significantly improve the
performance of GCNs with the power of self-supervised learning in improving
feature learning."
260,"['cs.LG', 'stat.ML']",Graph Random Neural Features for Distance-Preserving Graph Representations,"We present Graph Random Neural Features (GRNF), a novel embedding method from
graph-structured data to real vectors based on a family of graph neural
networks. The embedding naturally deals with graph isomorphism and preserves
the metric structure of the graph domain, in probability. In addition to being
an explicit embedding method, it also allows us to efficiently and effectively
approximate graph metric distances (as well as complete kernel functions); a
criterion to select the embedding dimension trading off the approximation
accuracy with the computational cost is also provided. GRNF can be used within
traditional processing methods or as a training-free input layer of a graph
neural network. The theoretical guarantees that accompany GRNF ensure that the
considered graph distance is metric, hence allowing to distinguish any pair of
non-isomorphic graphs."
261,"['cs.LG', 'stat.ML']",From Variational to Deterministic Autoencoders,"Variational Autoencoders (VAEs) provide a theoretically-backed and popular
framework for deep generative models. However, learning a VAE from data poses
still unanswered theoretical questions and considerable practical challenges.
In this work, we propose an alternative framework for generative modeling that
is simpler, easier to train, and deterministic, yet has many of the advantages
of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be
interpreted as simply injecting noise into the input of a deterministic
decoder. We investigate how substituting this kind of stochasticity, with other
explicit and implicit regularization schemes, can lead to an equally smooth and
meaningful latent space without forcing it to conform to an arbitrarily chosen
prior. To retrieve a generative mechanism to sample new data, we introduce an
ex-post density estimation step that can be readily applied also to existing
VAEs, improving their sample quality. We show, in a rigorous empirical study,
that the proposed regularized deterministic autoencoders are able to generate
samples that are comparable to, or better than, those of VAEs and more powerful
alternatives when applied to images as well as to structured data such as
molecules. \footnote{An implementation is available at:
\url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}}"
262,"['cs.LG', 'stat.ML']",Non-IID Graph Neural Networks,"Graph classification is an important task on graph-structured data with many
real-world applications. The goal of graph classification task is to train a
classifier using a set of training graphs. Recently, Graph Neural Networks
(GNNs) have greatly advanced the task of graph classification. When building a
GNN model for graph classification, the graphs in the training set are usually
assumed to be identically distributed. However, in many real-world
applications, graphs in the same dataset could have dramatically different
structures, which indicates that these graphs are likely non-identically
distributed. Therefore, in this paper, we aim to develop graph neural networks
for graphs that are not non-identically distributed. Specifically, we propose a
general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a
graph, Non-IID-GNN can adapt any existing graph neural network model to
generate a sample-specific model for this graph. Comprehensive experiments on
various graph classification benchmarks demonstrate the effectiveness of the
proposed framework. We will release the code of the proposed framework upon the
acceptance of the paper."
263,"['cs.LG', 'cs.CL', 'cs.SI', 'stat.ML']",HighwayGraph: Modelling Long-distance Node Relations for Improving General Graph Neural Network,"Graph Neural Networks (GNNs) are efficient approaches to process
graph-structured data. Modelling long-distance node relations is essential for
GNN training and applications. However, conventional GNNs suffer from bad
performance in modelling long-distance node relations due to limited-layer
information propagation. Existing studies focus on building deep GNN
architectures, which face the over-smoothing issue and cannot model node
relations in particularly long distance. To address this issue, we propose to
model long-distance node relations by simply relying on shallow GNN
architectures with two solutions: (1) Implicitly modelling by learning to
predict node pair relations (2) Explicitly modelling by adding edges between
nodes that potentially have the same label. To combine our two solutions, we
propose a model-agnostic training framework named HighwayGraph, which overcomes
the challenge of insufficient labeled nodes by sampling node pairs from the
training set and adopting the self-training method. Extensive experimental
results show that our HighwayGraph achieves consistent and significant
improvements over four representative GNNs on three benchmark datasets."
264,"['cs.LG', 'stat.ML']",Graph Neural Networks with Composite Kernels,"Learning on graph structured data has drawn increasing interest in recent
years. Frameworks like Graph Convolutional Networks (GCNs) have demonstrated
their ability to capture structural information and obtain good performance in
various tasks. In these frameworks, node aggregation schemes are typically used
to capture structural information: a node's feature vector is recursively
computed by aggregating features of its neighboring nodes. However, most of
aggregation schemes treat all connections in a graph equally, ignoring node
feature similarities. In this paper, we re-interpret node aggregation from the
perspective of kernel weighting, and present a framework to consider feature
similarity in an aggregation scheme. Specifically, we show that normalized
adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein
Space. We then propose feature aggregation as the composition of the original
neighbor-based kernel and a learnable kernel to encode feature similarities in
a feature space. We further show how the proposed method can be extended to
Graph Attention Network (GAT). Experimental results demonstrate better
performance of our proposed framework in several real-world applications."
265,"['cs.LG', 'stat.ML']",Improving Attention Mechanism in Graph Neural Networks via Cardinality Preservation,"Graph Neural Networks (GNNs) are powerful to learn the representation of
graph-structured data. Most of the GNNs use the message-passing scheme, where
the embedding of a node is iteratively updated by aggregating the information
of its neighbors. To achieve a better expressive capability of node influences,
attention mechanism has grown to be popular to assign trainable weights to the
nodes in aggregation. Though the attention-based GNNs have achieved remarkable
results in various tasks, a clear understanding of their discriminative
capacities is missing. In this work, we present a theoretical analysis of the
representational properties of the GNN that adopts the attention mechanism as
an aggregator. Our analysis determines all cases when those attention-based
GNNs can always fail to distinguish certain distinct structures. Those cases
appear due to the ignorance of cardinality information in attention-based
aggregation. To improve the performance of attention-based GNNs, we propose
cardinality preserved attention (CPA) models that can be applied to any kind of
attention mechanisms. Our experiments on node and graph classification confirm
our theoretical analysis and show the competitive performance of our CPA
models."
266,"['cs.LG', 'stat.ML']",Directed Graph Convolutional Network,"Graph Convolutional Networks (GCNs) have been widely used due to their
outstanding performance in processing graph-structured data. However, the
undirected graphs limit their application scope. In this paper, we extend
spectral-based graph convolution to directed graphs by using first- and
second-order proximity, which can not only retain the connection properties of
the directed graph, but also expand the receptive field of the convolution
operation. A new GCN model, called DGCN, is then designed to learn
representations on the directed graph, leveraging both the first- and
second-order proximity information. We empirically show the fact that GCNs
working only with DGCNs can encode more useful information from graph and help
achieve better performance when generalized to other models. Moreover,
extensive experiments on citation networks and co-purchase datasets demonstrate
the superiority of our model against the state-of-the-art methods."
267,"['cs.LG', 'stat.ML']",Variational Graph Recurrent Neural Networks,"Representation learning over graph structured data has been mostly studied in
static graph settings while efforts for modeling dynamic graphs are still
scant. In this paper, we develop a novel hierarchical variational model that
introduces additional latent random variables to jointly model the hidden
states of a graph recurrent neural network (GRNN) to capture both topology and
node attribute changes in dynamic graphs. We argue that the use of high-level
latent random variables in this variational GRNN (VGRNN) can better capture
potential variability observed in dynamic graphs as well as the uncertainty of
node latent representation. With semi-implicit variational inference developed
for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian
latent representations can further help dynamic graph analytic tasks. Our
experiments with multiple real-world dynamic graph datasets demonstrate that
SI-VGRNN and VGRNN consistently outperform the existing baseline and
state-of-the-art methods by a significant margin in dynamic link prediction."
268,"['cs.CV', 'eess.IV']",CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries,"Automated anatomical labeling plays a vital role in coronary artery disease
diagnosing procedure. The main challenge in this problem is the large
individual variability inherited in human anatomy. Existing methods usually
rely on the position information and the prior knowledge of the topology of the
coronary artery tree, which may lead to unsatisfactory performance when the
main branches are confusing. Motivated by the wide application of the graph
neural network in structured data, in this paper, we propose a conditional
partial-residual graph convolutional network (CPR-GCN), which takes both
position and CT image into consideration, since CT image contains abundant
information such as branch size and spanning direction. Two majority parts, a
Partial-Residual GCN and a conditions extractor, are included in CPR-GCN. The
conditions extractor is a hybrid model containing the 3D CNN and the LSTM,
which can extract 3D spatial image features along the branches. On the
technical side, the Partial-Residual GCN takes the position features of the
branches, with the 3D spatial image features as conditions, to predict the
label for each branches. While on the mathematical side, our approach twists
the partial differential equation (PDE) into the graph modeling. A dataset with
511 subjects is collected from the clinic and annotated by two experts with a
two-phase annotation process. According to the five-fold cross-validation, our
CPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which
outperforms state-of-the-art approaches."
269,"['stat.ML', 'cs.CV', 'cs.LG']",TensorProjection Layer: A Tensor-Based Dimensionality Reduction Method in CNN,"In this paper, we propose a dimensionality reduction method applied to
tensor-structured data as a hidden layer (we call it TensorProjection Layer) in
a convolutional neural network. Our proposed method transforms input tensors
into ones with a smaller dimension by projection. The directions of projection
are viewed as training parameters associated with our proposed layer and
trained via a supervised learning criterion such as minimization of the
cross-entropy loss function. We discuss the gradients of the loss function with
respect to the parameters associated with our proposed layer. We also implement
simple numerical experiments to evaluate the performance of the
TensorProjection Layer."
270,"['cs.CV', 'cs.GR']",Deep Manifold Prior,"We present a prior for manifold structured data, such as surfaces of 3D
shapes, where deep neural networks are adopted to reconstruct a target shape
using gradient descent starting from a random initialization. We show that
surfaces generated this way are smooth, with limiting behavior characterized by
Gaussian processes, and we mathematically derive such properties for
fully-connected as well as convolutional networks. We demonstrate our method in
a variety of manifold reconstruction applications, such as point cloud
denoising and interpolation, achieving considerably better results against
competitive baselines while requiring no training data. We also show that when
training data is available, our method allows developing alternate
parametrizations of surfaces under the framework of AtlasNet, leading to a
compact network architecture and better reconstruction results on standard
image to shape reconstruction benchmarks."
271,"['cs.LG', 'stat.ML']",DeepMap: Learning Deep Representations for Graph Classification,"Graph-structured data arise in many scenarios. A fundamental problem is to
quantify the similarities of graphs for tasks such as classification. Graph
kernels are positive-semidefinite functions that decompose graphs into
substructures and compare them. One problem in the effective implementation of
this idea is that the substructures are not independent, which leads to
high-dimensional feature space. In addition, graph kernels cannot capture the
high-order complex interactions between vertices. To mitigate these two
problems, we propose a framework called DeepMap to learn deep representations
for graph feature maps. The learnt deep representation for a graph is a dense
and low-dimensional vector that captures complex high-order interactions in a
vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to
arbitrary graphs by aligning vertices across graphs and building the receptive
field for each vertex. We empirically validate DeepMap on various graph
classification benchmarks and demonstrate that it achieves state-of-the-art
performance."
272,['cs.CV'],Hierarchical Data Generator based on Tree-Structured Stick Breaking Process for Benchmarking Clustering Methods,"Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis
that gains interest in the field of Machine Learning. Being still at an early
stage of development, the lack of tools for systematic analysis of Object
Cluster Hierarchies inhibits its further improvement. In this paper we address
this issue by proposing a generator of synthetic hierarchical data that can be
used for benchmarking Object Cluster Hierarchy methods. The article presents a
thorough empirical and theoretical analysis of the generator and provides
guidance on how to control its parameters. Conducted experiments show the
usefulness of the data generator that is capable of producing a wide range of
differently structured data. Further, benchmarking datasets that mirror the
most common types of hierarchies are generated and made available to the
public, together with the developed generator
(http://kio.pwr.edu.pl/?page\_id=396)."
273,"['cs.LG', 'cs.DB', 'cs.DM', 'stat.ML']","word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data","Vector representations of graphs and relational structures, whether
hand-crafted feature vectors or learned representations, enable us to apply
standard data analysis and machine learning techniques to the structures. A
wide range of methods for generating such embeddings have been studied in the
machine learning and knowledge representation literature. However, vector
embeddings have received relatively little attention from a theoretical point
of view.
  Starting with a survey of embedding techniques that have been used in
practice, in this paper we propose two theoretical approaches that we see as
central for understanding the foundations of vector embeddings. We draw
connections between the various approaches and suggest directions for future
research."
274,"['cs.LG', 'stat.ML']",A Survey of Deep Learning for Scientific Discovery,"Over the past few years, we have seen fundamental breakthroughs in core
problems in machine learning, largely driven by advances in deep neural
networks. At the same time, the amount of data collected in a wide array of
scientific domains is dramatically increasing in both size and complexity.
Taken together, this suggests many exciting opportunities for deep learning
applications in scientific settings. But a significant challenge to this is
simply knowing where to start. The sheer breadth and diversity of different
deep learning techniques makes it difficult to determine what scientific
problems might be most amenable to these methods, or which specific combination
of methods might offer the most promising first approach. In this survey, we
focus on addressing this central issue, providing an overview of many widely
used deep learning models, spanning visual, sequential and graph structured
data, associated tasks and different training methods, along with techniques to
use deep learning with less data and better interpret these complex models ---
two central considerations for many scientific use cases. We also include
overviews of the full design process, implementation tips, and links to a
plethora of tutorials, research summaries and open-sourced deep learning
pipelines and pretrained models, developed by the community. We hope that this
survey will help accelerate the use of deep learning across different
scientific domains."
275,"['cs.CV', 'cs.CL']",Visual Question Answering for Cultural Heritage,"Technology and the fruition of cultural heritage are becoming increasingly
more entwined, especially with the advent of smart audio guides, virtual and
augmented reality, and interactive installations. Machine learning and computer
vision are important components of this ongoing integration, enabling new
interaction modalities between user and museum. Nonetheless, the most frequent
way of interacting with paintings and statues still remains taking pictures.
Yet images alone can only convey the aesthetics of the artwork, lacking is
information which is often required to fully understand and appreciate it.
Usually this additional knowledge comes both from the artwork itself (and
therefore the image depicting it) and from an external source of knowledge,
such as an information sheet. While the former can be inferred by computer
vision algorithms, the latter needs more structured data to pair visual content
with relevant information. Regardless of its source, this information still
must be be effectively transmitted to the user. A popular emerging trend in
computer vision is Visual Question Answering (VQA), in which users can interact
with a neural network by posing questions in natural language and receiving
answers about the visual content. We believe that this will be the evolution of
smart audio guides for museum visits and simple image browsing on personal
smartphones. This will turn the classic audio guide into a smart personal
instructor with which the visitor can interact by asking for explanations
focused on specific interests. The advantages are twofold: on the one hand the
cognitive burden of the visitor will decrease, limiting the flow of information
to what the user actually wants to hear; and on the other hand it proposes the
most natural way of interacting with a guide, favoring engagement."
276,"['cs.LG', 'stat.ML']",Unified Multi-Domain Learning and Data Imputation using Adversarial Autoencoder,"We present a novel framework that can combine multi-domain learning (MDL),
data imputation (DI) and multi-task learning (MTL) to improve performance for
classification and regression tasks in different domains. The core of our
method is an adversarial autoencoder that can: (1) learn to produce
domain-invariant embeddings to reduce the difference between domains; (2) learn
the data distribution for each domain and correctly perform data imputation on
missing data. For MDL, we use the Maximum Mean Discrepancy (MMD) measure to
align the domain distributions. For DI, we use an adversarial approach where a
generator fill in information for missing data and a discriminator tries to
distinguish between real and imputed values. Finally, using the universal
feature representation in the embeddings, we train a classifier using MTL that
given input from any domain, can predict labels for all domains. We demonstrate
the superior performance of our approach compared to other state-of-art methods
in three distinct settings, DG-DI in image recognition with unstructured data,
MTL-DI in grade estimation with structured data and MDMTL-DI in a selection
process using mixed data."
277,"['stat.ML', 'cs.LG']",AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data,"We introduce AutoGluon-Tabular, an open-source AutoML framework that requires
only a single line of Python to train highly accurate machine learning models
on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML
frameworks that primarily focus on model/hyperparameter selection,
AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in
multiple layers. Experiments reveal that our multi-layer combination of many
models offers better use of allocated training time than seeking out the best.
A second contribution is an extensive evaluation of public and commercial
AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and
Google AutoML Tables. Tests on a suite of 50 classification and regression
tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is
faster, more robust, and much more accurate. We find that AutoGluon often even
outperforms the best-in-hindsight combination of all of its competitors. In two
popular Kaggle competitions, AutoGluon beat 99% of the participating data
scientists after merely 4h of training on the raw data."
278,"['cs.LG', 'stat.ML']",Metric Learning for Ordered Labeled Trees with pq-grams,"Computing the similarity between two data points plays a vital role in many
machine learning algorithms. Metric learning has the aim of learning a good
metric automatically from data. Most existing studies on metric learning for
tree-structured data have adopted the approach of learning the tree edit
distance. However, the edit distance is not amenable for big data analysis
because it incurs high computation cost. In this paper, we propose a new metric
learning approach for tree-structured data with pq-grams. The pq-gram distance
is a distance for ordered labeled trees, and has much lower computation cost
than the tree edit distance. In order to perform metric learning based on
pq-grams, we propose a new differentiable parameterized distance, weighted
pq-gram distance. We also propose a way to learn the proposed distance based on
Large Margin Nearest Neighbors (LMNN), which is a well-studied and practical
metric learning scheme. We formulate the metric learning problem as an
optimization problem and use the gradient descent technique to perform metric
learning. We empirically show that the proposed approach not only achieves
competitive results with the state-of-the-art edit distance-based methods in
various classification problems, but also solves the classification problems
much more rapidly than the edit distance-based methods."
279,['cs.CV'],Semantic Graph Convolutional Networks for 3D Human Pose Regression,"In this paper, we study the problem of learning Graph Convolutional Networks
(GCNs) for regression. Current architectures of GCNs are limited to the small
receptive field of convolution filters and shared transformation matrix for
each node. To address these limitations, we propose Semantic Graph
Convolutional Networks (SemGCN), a novel neural network architecture that
operates on regression tasks with graph-structured data. SemGCN learns to
capture semantic information such as local and global node relationships, which
is not explicitly represented in the graph. These semantic relationships can be
learned through end-to-end training from the ground truth without additional
supervision or hand-crafted rules. We further investigate applying SemGCN to 3D
human pose regression. Our formulation is intuitive and sufficient since both
2D and 3D human poses can be represented as a structured graph encoding the
relationships between joints in the skeleton of a human body. We carry out
comprehensive studies to validate our method. The results prove that SemGCN
outperforms state of the art while using 90% fewer parameters."
280,"['cs.LG', 'cs.CV', 'eess.IV']",GFCN: A New Graph Convolutional Network Based on Parallel Flows,"In view of the huge success of convolution neural networks (CNN) for image
classification and object recognition, there have been attempts to generalize
the method to general graph-structured data. One major direction is based on
spectral graph theory and graph signal processing. In this paper, we study the
problem from a completely different perspective, by introducing parallel flow
decomposition of graphs. The essential idea is to decompose a graph into
families of non-intersecting one dimensional (1D) paths, after which, we may
apply a 1D CNN along each family of paths. We demonstrate that the our method,
which we call GraphFlow, is able to transfer CNN architectures to general
graphs. To show the effectiveness of our approach, we test our method on the
classical MNIST dataset, synthetic datasets on network information propagation
and a news article classification dataset."
281,"['cs.LG', 'cs.SI', 'stat.ML']",Heterogeneous Graph Transformer,"Recent years have witnessed the emerging success of graph neural networks
(GNNs) for modeling structured data. However, most GNNs are designed for
homogeneous graphs, in which all nodes and edges belong to the same types,
making them infeasible to represent heterogeneous structures. In this paper, we
present the Heterogeneous Graph Transformer (HGT) architecture for modeling
Web-scale heterogeneous graphs. To model heterogeneity, we design node- and
edge-type dependent parameters to characterize the heterogeneous attention over
each edge, empowering HGT to maintain dedicated representations for different
types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce
the relative temporal encoding technique into HGT, which is able to capture the
dynamic structural dependency with arbitrary durations. To handle Web-scale
graph data, we design the heterogeneous mini-batch graph sampling
algorithm---HGSampling---for efficient and scalable training. Extensive
experiments on the Open Academic Graph of 179 million nodes and 2 billion edges
show that the proposed HGT model consistently outperforms all the
state-of-the-art GNN baselines by 9%--21% on various downstream tasks."
282,"['cs.LG', 'cs.AI', 'cs.SI', 'stat.ML']",EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs,"Neural networks for structured data like graphs have been studied extensively
in recent years. To date, the bulk of research activity has focused mainly on
static graphs. However, most real-world networks are dynamic since their
topology tends to change over time. Predicting the evolution of dynamic graphs
is a task of high significance in the area of graph mining. Despite its
practical importance, the task has not been explored in depth so far, mainly
due to its challenging nature. In this paper, we propose a model that predicts
the evolution of dynamic graphs. Specifically, we use a graph neural network
along with a recurrent architecture to capture the temporal evolution patterns
of dynamic graphs. Then, we employ a generative model which predicts the
topology of the graph at the next time step and constructs a graph instance
that corresponds to that topology. We evaluate the proposed model on several
artificial datasets following common network evolving dynamics, as well as on
real-world datasets. Results demonstrate the effectiveness of the proposed
model."
283,"['cs.LG', 'stat.ML']",Permutation Invariant Graph Generation via Score-Based Generative Modeling,"Learning generative models for graph-structured data is challenging because
graphs are discrete, combinatorial, and the underlying data distribution is
invariant to the ordering of nodes. However, most of the existing generative
models for graphs are not invariant to the chosen ordering, which might lead to
an undesirable bias in the learned distribution. To address this difficulty, we
propose a permutation invariant approach to modeling graphs, using the recent
framework of score-based generative modeling. In particular, we design a
permutation equivariant, multi-channel graph neural network to model the
gradient of the data distribution at the input graph (a.k.a., the score
function). This permutation equivariant model of gradients implicitly defines a
permutation invariant distribution for graphs. We train this graph neural
network with score matching and sample from it with annealed Langevin dynamics.
In our experiments, we first demonstrate the capacity of this new architecture
in learning discrete graph algorithms. For graph generation, we find that our
learning approach achieves better or comparable results to existing models on
benchmark datasets."
284,"['cs.LG', 'stat.ML']",From Node Embedding To Community Embedding : A Hyperbolic Approach,"Detecting communities on graphs has received significant interest in recent
literature. Current state-of-the-art community embedding approach called
\textit{ComE} tackles this problem by coupling graph embedding with community
detection. Considering the success of hyperbolic representations of
graph-structured data in last years, an ongoing challenge is to set up a
hyperbolic approach for the community detection problem. The present paper
meets this challenge by introducing a Riemannian equivalent of \textit{ComE}.
Our proposed approach combines hyperbolic embeddings with Riemannian K-means or
Riemannian mixture models to perform community detection. We illustrate the
usefulness of this framework through several experiments on real-world social
networks and comparisons with \textit{ComE} and recent hyperbolic-based
classification approaches."
285,"['cs.LG', 'cs.SI', 'stat.ML']",Tree++: Truncated Tree Based Graph Kernels,"Graph-structured data arise ubiquitously in many application domains. A
fundamental problem is to quantify their similarities. Graph kernels are often
used for this purpose, which decompose graphs into substructures and compare
these substructures. However, most of the existing graph kernels do not have
the property of scale-adaptivity, i.e., they cannot compare graphs at multiple
levels of granularities. Many real-world graphs such as molecules exhibit
structure at varying levels of granularities. To tackle this problem, we
propose a new graph kernel called Tree++ in this paper. At the heart of Tree++
is a graph kernel called the path-pattern graph kernel. The path-pattern graph
kernel first builds a truncated BFS tree rooted at each vertex and then uses
paths from the root to every vertex in the truncated BFS tree as features to
represent graphs. The path-pattern graph kernel can only capture graph
similarity at fine granularities. In order to capture graph similarity at
coarse granularities, we incorporate a new concept called super path into it.
The super path contains truncated BFS trees rooted at the vertices in a path.
Our evaluation on a variety of real-world graphs demonstrates that Tree++
achieves the best classification accuracy compared with previous graph kernels."
286,"['cs.LG', 'cs.CV', 'stat.ML']",Geom-GCN: Geometric Graph Convolutional Networks,"Message-passing neural networks (MPNNs) have been successfully applied to
representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability
to represent graph-structured data: losing the structural information of nodes
in neighborhoods and lacking the ability to capture long-range dependencies in
disassortative graphs. Few studies have noticed the weaknesses from different
perspectives. From the observations on classical neural network and network
geometry, we propose a novel geometric aggregation scheme for graph neural
networks to overcome the two weaknesses. The behind basic idea is the
aggregation on a graph can benefit from a continuous space underlying the
graph. The proposed aggregation scheme is permutation-invariant and consists of
three modules, node embedding, structural neighborhood, and bi-level
aggregation. We also present an implementation of the scheme in graph
convolutional networks, termed Geom-GCN (Geometric Graph Convolutional
Networks), to perform transductive learning on graphs. Experimental results
show the proposed Geom-GCN achieved state-of-the-art performance on a wide
range of open datasets of graphs. Code is available at
https://github.com/graphdml-uiuc-jlu/geom-gcn."
287,"['cs.LG', 'math.MG', 'stat.AP', 'stat.ML']",Multiple Metric Learning for Structured Data,"We address the problem of merging graph and feature-space information while
learning a metric from structured data. Existing algorithms tackle the problem
in an asymmetric way, by either extracting vectorized summaries of the graph
structure or adding hard constraints to feature-space algorithms. Following a
different path, we define a metric regression scheme where we train
metric-constrained linear combinations of dissimilarity matrices. The idea is
that the input matrices can be pre-computed dissimilarity measures obtained
from any kind of available data (e.g. node attributes or edge structure). As
the model inputs are distance measures, we do not need to assume the existence
of any underlying feature space. Main challenge is that metric constraints
(especially positive-definiteness and sub-additivity), are not automatically
respected if, for example, the coefficients of the linear combination are
allowed to be negative. Both positive and sub-additive constraints are linear
inequalities, but the computational complexity of imposing them scales as
O(D3), where D is the size of the input matrices (i.e. the size of the data
set). This becomes quickly prohibitive, even when D is relatively small. We
propose a new graph-based technique for optimizing under such constraints and
show that, in some cases, our approach may reduce the original computational
complexity of the optimization process by one order of magnitude. Contrarily to
existing methods, our scheme applies to any (possibly non-convex)
metric-constrained objective function."
288,"['cs.LG', 'stat.ML']",A Framework for End-to-End Learning on Semantic Tree-Structured Data,"While learning models are typically studied for inputs in the form of a fixed
dimensional feature vector, real world data is rarely found in this form. In
order to meet the basic requirement of traditional learning models, structural
data generally have to be converted into fix-length vectors in a handcrafted
manner, which is tedious and may even incur information loss. A common form of
structured data is what we term ""semantic tree-structures"", corresponding to
data where rich semantic information is encoded in a compositional manner, such
as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup
Language (XML). For tree-structured data, several learning models have been
studied to allow for working directly on raw tree-structure data, However such
learning models are limited to either a specific tree-topology or a specific
tree-structured data format, e.g., synthetic parse trees. In this paper, we
propose a novel framework for end-to-end learning on generic semantic
tree-structured data of arbitrary topology and heterogeneous data types, such
as data expressed in JSON, XML and so on. Motivated by the works in recursive
and recurrent neural networks, we develop exemplar neural implementations of
our framework for the JSON format. We evaluate our approach on several UCI
benchmark datasets, including ablation and data-efficiency studies, and on a
toy reinforcement learning task. Experimental results suggest that our
framework yields comparable performance to use of standard models with
dedicated feature-vectors in general, and even exceeds baseline performance in
cases where compositional nature of the data is particularly important.
  The source code for a JSON-based implementation of our framework along with
experiments can be downloaded at https://github.com/EndingCredits/json2vec."
289,"['cs.CV', 'cs.LG']",A Machine Learning Framework for Data Ingestion in Document Images,"Paper documents are widely used as an irreplaceable channel of information in
many fields, especially in financial industry, fostering a great amount of
demand for systems which can convert document images into structured data
representations. In this paper, we present a machine learning framework for
data ingestion in document images, which processes the images uploaded by users
and return fine-grained data in JSON format. Details of model architectures,
design strategies, distinctions with existing solutions and lessons learned
during development are elaborated. We conduct abundant experiments on both
synthetic and real-world data in State Street. The experimental results
indicate the effectiveness and efficiency of our methods."
290,"['cs.LG', 'stat.ML']",A Regularized Attention Mechanism for Graph Attention Networks,"Machine learning models that can exploit the inherent structure in data have
gained prominence. In particular, there is a surge in deep learning solutions
for graph-structured data, due to its wide-spread applicability in several
fields. Graph attention networks (GAT), a recent addition to the broad class of
feature learning models in graphs, utilizes the attention mechanism to
efficiently learn continuous vector representations for semi-supervised
learning problems. In this paper, we perform a detailed analysis of GAT models,
and present interesting insights into their behavior. In particular, we show
that the models are vulnerable to heterogeneous rogue nodes and hence propose
novel regularization strategies to improve the robustness of GAT models. Using
benchmark datasets, we demonstrate performance improvements on semi-supervised
learning, using the proposed robust variant of GAT."
291,"['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'stat.ML']",Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias,"Despite the phenomenal success of deep neural networks in a broad range of
learning tasks, there is a lack of theory to understand the way they work. In
particular, Convolutional Neural Networks (CNNs) are known to perform much
better than Fully-Connected Networks (FCNs) on spatially structured data: the
architectural structure of CNNs benefits from prior knowledge on the features
of the data, for instance their translation invariance. The aim of this work is
to understand this fact through the lens of dynamics in the loss landscape.
  We introduce a method that maps a CNN to its equivalent FCN (denoted as
eFCN). Such an embedding enables the comparison of CNN and FCN training
dynamics directly in the FCN space. We use this method to test a new training
protocol, which consists in training a CNN, embedding it to FCN space at a
certain ``relax time'', then resuming the training in FCN space. We observe
that for all relax times, the deviation from the CNN subspace is small, and the
final performance reached by the eFCN is higher than that reachable by a
standard FCN of same architecture. More surprisingly, for some intermediate
relax times, the eFCN outperforms the CNN it stemmed, by combining the prior
information of the CNN and the expressivity of the FCN in a complementary way.
The practical interest of our protocol is limited by the very large size of the
highly sparse eFCN. However, it offers interesting insights into the
persistence of architectural bias under stochastic gradient dynamics. It shows
the existence of some rare basins in the FCN loss landscape associated with
very good generalization. These can only be accessed thanks to the CNN prior,
which helps navigate the landscape during the early stages of optimization."
292,"['cs.LG', 'cs.AI', 'stat.ML']",Graph Representation Learning via Graphical Mutual Information Maximization,"The richness in the content of various information networks such as social
networks and communication networks provides the unprecedented potential for
learning high-quality expressive representations without external supervision.
This paper investigates how to preserve and extract the abundant information
from graph-structured data into embedding space in an unsupervised manner. To
this end, we propose a novel concept, Graphical Mutual Information (GMI), to
measure the correlation between input graphs and high-level hidden
representations. GMI generalizes the idea of conventional mutual information
computations from vector space to the graph domain where measuring mutual
information from two aspects of node features and topological structure is
indispensable. GMI exhibits several benefits: First, it is invariant to the
isomorphic transformation of input graphs---an inevitable constraint in many
existing graph representation learning algorithms; Besides, it can be
efficiently estimated and maximized by current mutual information estimation
methods such as MINE; Finally, our theoretical analysis confirms its
correctness and rationality. With the aid of GMI, we develop an unsupervised
learning model trained by maximizing GMI between the input and output of a
graph neural encoder. Considerable experiments on transductive as well as
inductive node classification and link prediction demonstrate that our method
outperforms state-of-the-art unsupervised counterparts, and even sometimes
exceeds the performance of supervised ones."
293,['cs.CV'],Effect of top-down connections in Hierarchical Sparse Coding,"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent
multi-dimensional, structured data such as images. The simplest solution to
solve this computationally hard problem is to decompose it into independent
layer-wise subproblems. However, neuroscientific evidence would suggest
inter-connecting these subproblems as in the Predictive Coding (PC) theory,
which adds top-down connections between consecutive layers. In this study, a
new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to
assess the impact of this inter-layer feedback connection. In particular, the
2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a
sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La
networks are trained on 4 different databases and with different sparsity
parameters on each layer. First, we show that the overall prediction error
generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers
prediction error between layers. Second, we demonstrate that the inference
stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we
show that the 2L-SPC also accelerates the learning process. Finally, the
qualitative analysis of both models dictionaries, supported by their activation
probability, show that the 2L-SPC features are more generic and informative."
294,"['cs.LG', 'stat.ML']",ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations,"Graph Neural Networks (GNN) have been shown to work effectively for modeling
graph structured data to solve tasks such as node classification, link
prediction and graph classification. There has been some recent progress in
defining the notion of pooling in graphs whereby the model tries to generate a
graph level representation by downsampling and summarizing the information
present in the nodes. Existing pooling methods either fail to effectively
capture the graph substructure or do not easily scale to large graphs. In this
work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and
differentiable pooling method that addresses the limitations of previous graph
pooling architectures. ASAP utilizes a novel self-attention network along with
a modified GNN formulation to capture the importance of each node in a given
graph. It also learns a sparse soft cluster assignment for nodes at each layer
to effectively pool the subgraphs to form the pooled graph. Through extensive
experiments on multiple datasets and theoretical analysis, we motivate our
choice of the components used in ASAP. Our experimental results show that
combining existing GNN architectures with ASAP leads to state-of-the-art
results on multiple graph classification benchmarks. ASAP has an average
improvement of 4%, compared to current sparse hierarchical state-of-the-art
method."
295,"['stat.ML', 'cs.LG']",Sharp Rate of Convergence for Deep Neural Network Classifiers under the Teacher-Student Setting,"Classifiers built with neural networks handle large-scale high dimensional
data, such as facial images from computer vision, extremely well while
traditional statistical methods often fail miserably. In this paper, we attempt
to understand this empirical success in high dimensional classification by
deriving the convergence rates of excess risk. In particular, a teacher-student
framework is proposed that assumes the Bayes classifier to be expressed as ReLU
neural networks. In this setup, we obtain a sharp rate of convergence, i.e.,
$\tilde{O}_d(n^{-2/3})$, for classifiers trained using either 0-1 loss or hinge
loss. This rate can be further improved to $\tilde{O}_d(n^{-1})$ when the data
distribution is separable. Here, $n$ denotes the sample size. An interesting
observation is that the data dimension only contributes to the $\log(n)$ term
in the above rates. This may provide one theoretical explanation for the
empirical successes of deep neural networks in high dimensional classification,
particularly for structured data."
296,"['cs.LG', 'stat.ML']",Composition-based Multi-Relational Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have recently been shown to be quite
successful in modeling graph-structured data. However, the primary focus has
been on handling simple undirected graphs. Multi-relational graphs are a more
general and prevalent form of graphs where each edge has a label and direction
associated with it. Most of the existing approaches to handle such graphs
suffer from over-parameterization and are restricted to learning
representations of nodes only. In this paper, we propose CompGCN, a novel Graph
Convolutional framework which jointly embeds both nodes and relations in a
relational graph. CompGCN leverages a variety of entity-relation composition
operations from Knowledge Graph Embedding techniques and scales with the number
of relations. It also generalizes several of the existing multi-relational GCN
methods. We evaluate our proposed method on multiple tasks such as node
classification, link prediction, and graph classification, and achieve
demonstrably superior results. We make the source code of CompGCN available to
foster reproducible research."
297,"['cs.LG', 'stat.ML']",DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters,"We propose a novel spectral convolutional neural network (CNN) model on graph
structured data, namely Distributed Feedback-Looped Networks (DFNets). This
model is incorporated with a robust class of spectral graph filters, called
feedback-looped filters, to provide better localization on vertices, while
still attaining fast convergence and linear memory requirements. Theoretically,
feedback-looped filters can guarantee convergence w.r.t. a specified error
bound, and be applied universally to any graph without knowing its structure.
Furthermore, the propagation rule of this model can diversify features from the
preceding layers to produce strong gradient flows. We have evaluated our model
using two benchmark tasks: semi-supervised document classification on citation
networks and semi-supervised entity classification on a knowledge graph. The
experimental results show that our model considerably outperforms the
state-of-the-art methods in both benchmark tasks over all datasets."
298,"['cs.LG', 'cs.AI', 'stat.ML']",Spectral Inference Networks: Unifying Deep and Spectral Learning,"We present Spectral Inference Networks, a framework for learning
eigenfunctions of linear operators by stochastic optimization. Spectral
Inference Networks generalize Slow Feature Analysis to generic symmetric
operators, and are closely related to Variational Monte Carlo methods from
computational physics. As such, they can be a powerful tool for unsupervised
representation learning from video or graph-structured data. We cast training
Spectral Inference Networks as a bilevel optimization problem, which allows for
online learning of multiple eigenfunctions. We show results of training
Spectral Inference Networks on problems in quantum mechanics and feature
learning for videos on synthetic datasets. Our results demonstrate that
Spectral Inference Networks accurately recover eigenfunctions of linear
operators and can discover interpretable representations from video in a fully
unsupervised manner."
299,['cs.LG'],Discriminative Embeddings of Latent Variable Models for Structured Data,"Kernel classifiers and regressors designed for structured data, such as
sequences, trees and graphs, have significantly advanced a number of
interdisciplinary areas such as computational biology and drug design.
Typically, kernels are designed beforehand for a data type which either exploit
statistics of the structures or make use of probabilistic generative models,
and then a discriminative classifier is learned based on the kernels via convex
optimization. However, such an elegant two-stage approach also limited kernel
methods from scaling up to millions of data points, and exploiting
discriminative information to learn feature representations.
  We propose, structure2vec, an effective and scalable approach for structured
data representation based on the idea of embedding latent variable models into
feature spaces, and learning such feature spaces using discriminative
information. Interestingly, structure2vec extracts features by performing a
sequence of function mappings in a way similar to graphical model inference
procedures, such as mean field and belief propagation. In applications
involving millions of data points, we showed that structure2vec runs 2 times
faster, produces models which are $10,000$ times smaller, while at the same
time achieving the state-of-the-art predictive performance."
300,['cs.CV'],Graph-FCN for image semantic segmentation,"Semantic segmentation with deep learning has achieved great progress in
classifying the pixels in the image. However, the local location information is
usually ignored in the high-level feature extraction by the deep learning,
which is important for image semantic segmentation. To avoid this problem, we
propose a graph model initialized by a fully convolutional network (FCN) named
Graph-FCN for image semantic segmentation. Firstly, the image grid data is
extended to graph structure data by a convolutional network, which transforms
the semantic segmentation problem into a graph node classification problem.
Then we apply graph convolutional network to solve this graph node
classification problem. As far as we know, it is the first time that we apply
the graph convolutional network in image semantic segmentation. Our method
achieves competitive performance in mean intersection over union (mIOU) on the
VOC dataset(about 1.34% improvement), compared to the original FCN model."
301,"['cs.LG', 'stat.ML']",Hierarchical Graph Pooling with Structure Learning,"Graph Neural Networks (GNNs), which generalize deep neural networks to
graph-structured data, have drawn considerable attention and achieved
state-of-the-art performance in numerous graph related tasks. However, existing
GNN models mainly focus on designing graph convolution operations. The graph
pooling (or downsampling) operations, that play an important role in learning
hierarchical representations, are usually overlooked. In this paper, we propose
a novel graph pooling operator, called Hierarchical Graph Pooling with
Structure Learning (HGP-SL), which can be integrated into various graph neural
network architectures. HGP-SL incorporates graph pooling and structure learning
into a unified module to generate hierarchical representations of graphs. More
specifically, the graph pooling operation adaptively selects a subset of nodes
to form an induced subgraph for the subsequent layers. To preserve the
integrity of graph's topological information, we further introduce a structure
learning mechanism to learn a refined graph structure for the pooled graph at
each layer. By combining HGP-SL operator with graph neural networks, we perform
graph level representation learning with focus on graph classification task.
Experimental results on six widely used benchmarks demonstrate the
effectiveness of our proposed model."
302,"['cs.LG', 'stat.ML']",How Robust Are Graph Neural Networks to Structural Noise?,"Graph neural networks (GNNs) are an emerging model for learning graph
embeddings and making predictions on graph structured data. However, robustness
of graph neural networks is not yet well-understood. In this work, we focus on
node structural identity predictions, where a representative GNN model is able
to achieve near-perfect accuracy. We also show that the same GNN model is not
robust to addition of structural noise, through a controlled dataset and set of
experiments. Finally, we show that under the right conditions, graph-augmented
training is capable of significantly improving robustness to structural noise."
303,"['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient,"In the last decade, many diverse advances have occurred in the field of
information extraction from data. Information extraction in its simplest form
takes place in computing environments, where structured data can be extracted
through a series of queries. The continuous expansion of quantities of data
have therefore provided an opportunity for knowledge extraction (KE) from a
textual document (TD). A typical problem of this kind is the extraction of
common characteristics and knowledge from a group of TDs, with the possibility
to group such similar TDs in a process known as clustering. In this paper we
present a technique for such KE among a group of TDs related to the common
characteristics and meaning of their content. Our technique is based on the
Spearman's Rank Correlation Coefficient (SRCC), for which the conducted
experiments have proven to be comprehensive measure to achieve a high-quality
KE."
304,"['cs.LG', 'stat.ML']",Effective Decoding in Graph Auto-Encoder using Triadic Closure,"The (variational) graph auto-encoder and its variants have been popularly
used for representation learning on graph-structured data. While the encoder is
often a powerful graph convolutional network, the decoder reconstructs the
graph structure by only considering two nodes at a time, thus ignoring possible
interactions among edges. On the other hand, structured prediction, which
considers the whole graph simultaneously, is computationally expensive. In this
paper, we utilize the well-known triadic closure property which is exhibited in
many real-world networks. We propose the triad decoder, which considers and
predicts the three edges involved in a local triad together. The triad decoder
can be readily used in any graph-based auto-encoder. In particular, we
incorporate this to the (variational) graph auto-encoder. Experiments on link
prediction, node clustering and graph generation show that the use of triads
leads to more accurate prediction, clustering and better preservation of the
graph characteristics."
305,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",Set Aggregation Network as a Trainable Pooling Layer,"Global pooling, such as max- or sum-pooling, is one of the key ingredients in
deep neural networks used for processing images, texts, graphs and other types
of structured data. Based on the recent DeepSets architecture proposed by
Zaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an
alternative global pooling layer. In contrast to typical pooling operators, SAN
allows to embed a given set of features to a vector representation of arbitrary
size. We show that by adjusting the size of embedding, SAN is capable of
preserving the whole information from the input. In experiments, we demonstrate
that replacing global pooling layer by SAN leads to the improvement of
classification accuracy. Moreover, it is less prone to overfitting and can be
used as a regularizer."
306,"['stat.ML', 'cs.LG']",Deductron -- A Recurrent Neural Network,"The current paper is a study in Recurrent Neural Networks (RNN), motivated by
the lack of examples simple enough so that they can be thoroughly understood
theoretically, but complex enough to be realistic. We constructed an example of
structured data, motivated by problems from image-to-text conversion (OCR),
which requires long-term memory to decode. Our data is a simple writing system,
encoding characters 'X' and 'O' as their upper halves, which is possible due to
symmetry of the two characters. The characters can be connected, as in some
languages using cursive, such as Arabic (abjad). The string 'XOOXXO' may be
encoded as
'${\vee}{\wedge}\kern-1.5pt{\wedge}{\vee}\kern-1.5pt{\vee}{\wedge}$'. It
follows that we may need to know arbitrarily long past to decode a current
character, thus requiring long-term memory. Subsequently we constructed an RNN
capable of decoding sequences encoded in this manner. Rather than by training,
we constructed our RNN ""by inspection"", i.e. we guessed its weights. This
involved a sequence of steps. We wrote a conventional program which decodes the
sequences as the example above. Subsequently, we interpreted the program as a
neural network (the only example of this kind known to us). Finally, we
generalized this neural network to discover a new RNN architecture whose
instance is our handcrafted RNN. It turns out to be a 3 layer network, where
the middle layer is capable of performing simple logical inferences; thus the
name ""deductron"". It is demonstrated that it is possible to train our network
by simulated annealing. Also, known variants of stochastic gradient descent
(SGD) methods are shown to work."
307,"['cs.LG', 'q-bio.QM', 'stat.ML']",CASTER: Predicting Drug Interactions with Chemical Substructure Representation,"Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and
mortality. Identifying potential DDIs during the drug design process is
critical for patients and society. Although several computational models have
been proposed for DDI prediction, there are still limitations: (1) specialized
design of drug representation for DDI predictions is lacking; (2) predictions
are based on limited labelled data and do not generalize well to unseen drugs
or DDIs; and (3) models are characterized by a large number of parameters, thus
are hard to interpret. In this work, we develop a ChemicAl SubstrucTurE
Representation (CASTER) framework that predicts DDIs given chemical structures
of drugs.CASTER aims to mitigate these limitations via (1) a sequential pattern
mining module rooted in the DDI mechanism to efficiently characterize
functional sub-structures of drugs; (2) an auto-encoding module that leverages
both labelled and unlabelled chemical structure data to improve predictive
accuracy and generalizability; and (3) a dictionary learning module that
explains the prediction via a small set of coefficients which measure the
relevance of each input sub-structures to the DDI outcome. We evaluated CASTER
on two real-world DDI datasets and showed that it performed better than
state-of-the-art baselines and provided interpretable predictions."
308,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",Towards unstructured mortality prediction with free-text clinical notes,"Healthcare data continues to flourish yet a relatively small portion, mostly
structured, is being utilized effectively for predicting clinical outcomes. The
rich subjective information available in unstructured clinical notes can
possibly facilitate higher discrimination but tends to be under-utilized in
mortality prediction. This work attempts to assess the gain in performance when
multiple notes that have been minimally preprocessed are used as an input for
prediction. A hierarchical architecture consisting of both convolutional and
recurrent layers is used to concurrently model the different notes compiled in
an individual hospital stay. This approach is evaluated on predicting
in-hospital mortality on the MIMIC-III dataset. On comparison to approaches
utilizing structured data, it achieved higher metrics despite requiring less
cleaning and preprocessing. This demonstrates the potential of unstructured
data in enhancing mortality prediction and signifies the need to incorporate
more raw unstructured data into current clinical prediction methods."
309,"['stat.ML', 'cs.LG']",Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms,"The scattering transform is a multilayered wavelet-based deep learning
architecture that acts as a model of convolutional neural networks. Recently,
several works have introduced generalizations of the scattering transform for
non-Euclidean settings such as graphs. Our work builds upon these constructions
by introducing windowed and non-windowed graph scattering transforms based upon
a very general class of asymmetric wavelets. We show that these asymmetric
graph scattering transforms have many of the same theoretical guarantees as
their symmetric counterparts. This work helps bridge the gap between scattering
and other graph neural networks by introducing a large family of networks with
provable stability and invariance guarantees. This lays the groundwork for
future deep learning architectures for graph-structured data that have learned
filters and also provably have desirable theoretical properties."
310,['cs.LG'],Few-Features Attack to Fool Machine Learning Models through Mask-Based GAN,"GAN is a deep-learning based generative approach to generate contents such as
images, languages and speeches. Recently, studies have shown that GAN can also
be applied to generative adversarial attack examples to fool the
machine-learning models. In comparison with the previous non-learning
adversarial example attack approaches, the GAN-based adversarial attack example
approach can generate the adversarial samples quickly using the GAN
architecture every time facing a new sample after training, but meanwhile needs
to perturb the attack samples in great quantities, which results in the
unpractical application in reality. To address this issue, we propose a new
approach, named Few-Feature-Attack-GAN (FFA-GAN). FFA-GAN has a significant
time-consuming advantage than the non-learning adversarial samples approaches
and a better non-zero-features performance than the GANbased adversarial sample
approaches. FFA-GAN can automatically generate the attack samples in the
black-box attack through the GAN architecture instead of the evolutional
algorithms or the other non-learning approaches. Besides, we introduce the mask
mechanism into the generator network of the GAN architecture to optimize the
constraint issue, which can also be regarded as the sparsity problem of the
important features. During the training, the different weights of losses of the
generator are set in the different training phases to ensure the divergence of
the two above mentioned parallel networks of the generator. Experiments are
made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in
which the dimensions of the data are relatively low, and also on the
unstructured data sets MNIST and CIFAR-10 with the data of the relatively high
dimensions. The results of the experiments demonstrate the effectiveness and
the robustness of our proposed approach."
311,['cs.CV'],Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching,"Human action recognition from skeleton data, fueled by the Graph
Convolutional Network (GCN), has attracted lots of attention, due to its
powerful capability of modeling non-Euclidean structure data. However, many
existing GCN methods provide a pre-defined graph and fix it through the entire
network, which can loss implicit joint correlations. Besides, the mainstream
spectral GCN is approximated by one-order hop, thus higher-order connections
are not well involved. Therefore, huge efforts are required to explore a better
GCN architecture. To address these problems, we turn to Neural Architecture
Search (NAS) and propose the first automatically designed GCN for
skeleton-based action recognition. Specifically, we enrich the search space by
providing multiple dynamic graph modules after fully exploring the
spatial-temporal correlations between nodes. Besides, we introduce multiple-hop
modules and expect to break the limitation of representational capacity caused
by one-order approximation. Moreover, a sampling- and memory-efficient
evolution strategy is proposed to search an optimal architecture for this task.
The resulted architecture proves the effectiveness of the higher-order
approximation and the dynamic graph modeling mechanism with temporal
interactions, which is barely discussed before. To evaluate the performance of
the searched model, we conduct extensive experiments on two very large scaled
datasets and the results show that our model gets the state-of-the-art results."
312,"['cs.LG', 'cs.AI', 'stat.ML']",Graph Representation Learning via Multi-task Knowledge Distillation,"Machine learning on graph structured data has attracted much research
interest due to its ubiquity in real world data. However, how to efficiently
represent graph data in a general way is still an open problem. Traditional
methods use handcraft graph features in a tabular form but suffer from the
defects of domain expertise requirement and information loss. Graph
representation learning overcomes these defects by automatically learning the
continuous representations from graph structures, but they require abundant
training labels, which are often hard to fulfill for graph-level prediction
problems. In this work, we demonstrate that, if available, the domain expertise
used for designing handcraft graph features can improve the graph-level
representation learning when training labels are scarce. Specifically, we
proposed a multi-task knowledge distillation method. By incorporating
network-theory-based graph metrics as auxiliary tasks, we show on both
synthetic and real datasets that the proposed multi-task learning method can
improve the prediction performance of the original learning task, especially
when the training data size is small."
313,"['cs.LG', 'cs.IT', 'cs.NE', 'eess.SP', 'math.IT', 'stat.ML']",Inference with Deep Generative Priors in High Dimensions,"Deep generative priors offer powerful models for complex-structured data,
such as images, audio, and text. Using these priors in inverse problems
typically requires estimating the input and/or hidden signals in a multi-layer
deep neural network from observation of its output. While these approaches have
been successful in practice, rigorous performance analysis is complicated by
the non-convex nature of the underlying optimization problems. This paper
presents a novel algorithm, Multi-Layer Vector Approximate Message Passing
(ML-VAMP), for inference in multi-layer stochastic neural networks. ML-VAMP can
be configured to compute maximum a priori (MAP) or approximate minimum
mean-squared error (MMSE) estimates for these networks. We show that the
performance of ML-VAMP can be exactly predicted in a certain high-dimensional
random limit. Furthermore, under certain conditions, ML-VAMP yields estimates
that achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica
method. In this way, ML-VAMP provides a computationally efficient method for
multi-layer inference with an exact performance characterization and testable
conditions for optimality in the large-system limit."
314,"['cs.LG', 'stat.ML']",Study of Constrained Network Structures for WGANs on Numeric Data Generation,"Some recent studies have suggested using GANs for numeric data generation
such as to generate data for completing the imbalanced numeric data.
Considering the significant difference between the dimensions of the numeric
data and images, as well as the strong correlations between features of numeric
data, the conventional GANs normally face an overfitting problem, consequently
leads to an ill-conditioning problem in generating numeric and structured data.
This paper studies the constrained network structures between generator G and
discriminator D in WGAN, designs several structures including isomorphic,
mirror and self-symmetric structures. We evaluates the performances of the
constrained WGANs in data augmentations, taking the non-constrained GANs and
WGANs as the baselines. Experiments prove the constrained structures have been
improved in 17/20 groups of experiments. In twenty experiments on four UCI
Machine Learning Repository datasets, Australian Credit Approval data, German
Credit data, Pima Indians Diabetes data and SPECT heart data facing five
conventional classifiers. Especially, Isomorphic WGAN is the best in 15/20
experiments. Finally, we theoretically proves that the effectiveness of
constrained structures by the directed graphic model (DGM) analysis."
315,"['cs.LG', 'q-bio.QM']",bigMap: Big Data Mapping with Parallelized t-SNE,"We introduce an improved unsupervised clustering protocol specially suited
for large-scale structured data. The protocol follows three steps: a
dimensionality reduction of the data, a density estimation over the low
dimensional representation of the data, and a final segmentation of the density
landscape. For the dimensionality reduction step we introduce a parallelized
implementation of the well-known t-Stochastic Neighbouring Embedding (t-SNE)
algorithm that significantly alleviates some inherent limitations, while
improving its suitability for large datasets. We also introduce a new adaptive
Kernel Density Estimation particularly coupled with the t-SNE framework in
order to get accurate density estimates out of the embedded data, and a variant
of the rainfalling watershed algorithm to identify clusters within the density
landscape. The whole mapping protocol is wrapped in the bigMap R package,
together with visualization and analysis tools to ease the qualitative and
quantitative assessment of the clustering."
316,"['cs.LG', 'stat.ML']",Semi-supervised Learning in Network-Structured Data via Total Variation Minimization,"We propose and analyze a method for semi-supervised learning from
partially-labeled network-structured data. Our approach is based on a graph
signal recovery interpretation under a clustering hypothesis that labels of
data points belonging to the same well-connected subset (cluster) are similar
valued. This lends naturally to learning the labels by total variation (TV)
minimization, which we solve by applying a recently proposed primal-dual method
for non-smooth convex optimization. The resulting algorithm allows for a highly
scalable implementation using message passing over the underlying empirical
graph, which renders the algorithm suitable for big data applications. By
applying tools of compressed sensing, we derive a sufficient condition on the
underlying network structure such that TV minimization recovers clusters in the
empirical graph of the data. In particular, we show that the proposed
primal-dual method amounts to maximizing network flows over the empirical graph
of the dataset. Moreover, the learning accuracy of the proposed algorithm is
linked to the set of network flows between data points having known labels. The
effectiveness and scalability of our approach is verified by numerical
experiments."
317,"['cs.LG', 'stat.ML']",A Flexible Generative Framework for Graph-based Semi-supervised Learning,"We consider a family of problems that are concerned about making predictions
for the majority of unlabeled, graph-structured data samples based on a small
proportion of labeled samples. Relational information among the data samples,
often encoded in the graph/network structure, is shown to be helpful for these
semi-supervised learning tasks. However, conventional graph-based
regularization methods and recent graph neural networks do not fully leverage
the interrelations between the features, the graph, and the labels. In this
work, we propose a flexible generative framework for graph-based
semi-supervised learning, which approaches the joint distribution of the node
features, labels, and the graph structure. Borrowing insights from random graph
models in network science literature, this joint distribution can be
instantiated using various distribution families. For the inference of missing
labels, we exploit recent advances of scalable variational inference techniques
to approximate the Bayesian posterior. We conduct thorough experiments on
benchmark datasets for graph-based semi-supervised learning. Results show that
the proposed methods outperform the state-of-the-art models in most settings."
318,"['cs.LG', 'cs.SI', 'stat.ML']",Understanding Isomorphism Bias in Graph Data Sets,"In recent years there has been a rapid increase in classification methods on
graph structured data. Both in graph kernels and graph neural networks, one of
the implicit assumptions of successful state-of-the-art models was that
incorporating graph isomorphism features into the architecture leads to better
empirical performance. However, as we discover in this work, commonly used data
sets for graph classification have repeating instances which cause the problem
of isomorphism bias, i.e. artificially increasing the accuracy of the models by
memorizing target information from the training set. This prevents fair
competition of the algorithms and raises a question of the validity of the
obtained results. We analyze 54 data sets, previously extensively used for
graph-related tasks, on the existence of isomorphism bias, give a set of
recommendations to machine learning practitioners to properly set up their
models, and open source new data sets for the future experiments."
319,"['cs.LG', 'stat.ML', '42C40, 05C85, 11Y16']",Fast Haar Transforms for Graph Neural Networks,"Graph Neural Networks (GNNs) have become a topic of intense research recently
due to their powerful capability in high-dimensional classification and
regression tasks for graph-structured data. However, as GNNs typically define
the graph convolution by the orthonormal basis for the graph Laplacian, they
suffer from high computational cost when the graph size is large. This paper
introduces Haar basis which is a sparse and localized orthonormal system for a
coarse-grained chain on graph. The graph convolution under Haar basis, called
Haar convolution, can be defined accordingly for GNNs. The sparsity and
locality of the Haar basis allow Fast Haar Transforms (FHTs) on graph, by which
a fast evaluation of Haar convolution between graph data and filters can be
achieved. We conduct experiments on GNNs equipped with Haar convolution, which
demonstrates state-of-the-art results on graph-based regression and node
classification tasks."
320,"['cs.LG', 'stat.ML']",D-VAE: A Variational Autoencoder for Directed Acyclic Graphs,"Graph structured data are abundant in the real world. Among different graph
types, directed acyclic graphs (DAGs) are of particular interest to machine
learning researchers, as many machine learning models are realized as
computations on DAGs, including neural networks and Bayesian networks. In this
paper, we study deep generative models for DAGs, and propose a novel DAG
variational autoencoder (D-VAE). To encode DAGs into the latent space, we
leverage graph neural networks. We propose an asynchronous message passing
scheme that allows encoding the computations on DAGs, rather than using
existing simultaneous message passing schemes to encode local graph structures.
We demonstrate the effectiveness of our proposed DVAE through two tasks: neural
architecture search and Bayesian network structure learning. Experiments show
that our model not only generates novel and valid DAGs, but also produces a
smooth latent space that facilitates searching for DAGs with better performance
through Bayesian optimization."
321,"['cs.LG', 'stat.ML']",Hyperbolic Graph Neural Networks,"Learning from graph-structured data is an important task in machine learning
and artificial intelligence, for which Graph Neural Networks (GNNs) have shown
great promise. Motivated by recent advances in geometric representation
learning, we propose a novel GNN architecture for learning representations on
Riemannian manifolds with differentiable exponential and logarithmic maps. We
develop a scalable algorithm for modeling the structural properties of graphs,
comparing Euclidean and hyperbolic geometry. In our experiments, we show that
hyperbolic GNNs can lead to substantial improvements on various benchmark
datasets."
322,"['stat.ML', 'cs.LG', 'math.DS', 'math.PR', '62-07, 37H99']",Metric on random dynamical systems with vector-valued reproducing kernel Hilbert spaces,"Development of metrics for structural data-generating mechanisms is
fundamental in machine learning and the related fields. In this paper, we give
a general framework to construct metrics on random nonlinear dynamical systems,
defined with the Perron-Frobenius operators in vector-valued reproducing kernel
Hilbert spaces (vvRKHSs). We employ vvRKHSs to design mathematically manageable
metrics and also to introduce operator-valued kernels, which enables us to
handle randomness in systems. Our metric provides an extension of the existing
metrics for deterministic systems, and gives a specification of the kernel
maximal mean discrepancy of random processes. Moreover, by considering the
time-wise independence of random processes, we clarify a connection between our
metric and the independence criteria with kernels such as Hilbert-Schmidt
independence criteria. We empirically illustrate our metric with synthetic
data, and evaluate it in the context of the independence test for random
processes. We also evaluate the performance with real time seris datas via
clusering tasks."
323,"['cs.LG', 'stat.ML']",Active Learning for Graph Neural Networks via Node Feature Propagation,"Graph Neural Networks (GNNs) for prediction tasks like node classification or
edge prediction have received increasing attention in recent machine learning
from graphically structured data. However, a large quantity of labeled graphs
is difficult to obtain, which significantly limits the true success of GNNs.
Although active learning has been widely studied for addressing label-sparse
issues with other data types like text, images, etc., how to make it effective
over graphs is an open question for research. In this paper, we present an
investigation on active learning with GNNs for node classification tasks.
Specifically, we propose a new method, which uses node feature propagation
followed by K-Medoids clustering of the nodes for instance selection in active
learning. With a theoretical bound analysis we justify the design choice of our
approach. In our experiments on four benchmark datasets, the proposed method
outperforms other representative baseline methods consistently and
significantly."
324,['cs.CV'],Human Action Recognition with Multi-Laplacian Graph Convolutional Networks,"Convolutional neural networks are nowadays witnessing a major success in
different pattern recognition problems. These learning models were basically
designed to handle vectorial data such as images but their extension to
non-vectorial and semi-structured data (namely graphs with variable sizes,
topology, etc.) remains a major challenge, though a few interesting solutions
are currently emerging. In this paper, we introduce MLGCN; a novel spectral
Multi-Laplacian Graph Convolutional Network. The main contribution of this
method resides in a new design principle that learns graph-laplacians as convex
combinations of other elementary laplacians each one dedicated to a particular
topology of the input graphs. We also introduce a novel pooling operator, on
graphs, that proceeds in two steps: context-dependent node expansion is
achieved, followed by a global average pooling; the strength of this two-step
process resides in its ability to preserve the discrimination power of nodes
while achieving permutation invariance. Experiments conducted on SBU and
UCF-101 datasets, show the validity of our method for the challenging task of
action recognition."
325,"['cs.LG', 'math.AT', 'stat.ML']",Shapley Homology: Topological Analysis of Sample Influence for Neural Networks,"Data samples collected for training machine learning models are typically
assumed to be independent and identically distributed (iid). Recent research
has demonstrated that this assumption can be problematic as it simplifies the
manifold of structured data. This has motivated different research areas such
as data poisoning, model improvement, and explanation of machine learning
models. In this work, we study the influence of a sample on determining the
intrinsic topological features of its underlying manifold. We propose the
Shapley Homology framework, which provides a quantitative metric for the
influence of a sample of the homology of a simplicial complex. By interpreting
the influence as a probability measure, we further define an entropy which
reflects the complexity of the data manifold. Our empirical studies show that
when using the 0-dimensional homology, on neighboring graphs, samples with
higher influence scores have more impact on the accuracy of neural networks for
determining the graph connectivity and on several regular grammars whose higher
entropy values imply more difficulty in being learned."
326,"['cs.CV', 'cs.LG']",Dilated Convolutional Neural Networks for Sequential Manifold-valued Data,"Efforts are underway to study ways via which the power of deep neural
networks can be extended to non-standard data types such as structured data
(e.g., graphs) or manifold-valued data (e.g., unit vectors or special
matrices). Often, sizable empirical improvements are possible when the geometry
of such data spaces are incorporated into the design of the model,
architecture, and the algorithms. Motivated by neuroimaging applications, we
study formulations where the data are {\em sequential manifold-valued
measurements}. This case is common in brain imaging, where the samples
correspond to symmetric positive definite matrices or orientation distribution
functions. Instead of a recurrent model which poses computational/technical
issues, and inspired by recent results showing the viability of dilated
convolutional models for sequence prediction, we develop a dilated
convolutional neural network architecture for this task. On the technical side,
we show how the modules needed in our network can be derived while explicitly
taking the Riemannian manifold structure into account. We show how the
operations needed can leverage known results for calculating the weighted
Fr\'{e}chet Mean (wFM). Finally, we present scientific results for group
difference analysis in Alzheimer's disease (AD) where the groups are derived
using AD pathology load: here the model finds several brain fiber bundles that
are related to AD even when the subjects are all still cognitively healthy."
327,"['cs.LG', 'cs.CV', 'stat.ML']",Continuous Graph Flow,"In this paper, we propose Continuous Graph Flow, a generative continuous flow
based method that aims to model complex distributions of graph-structured data.
Once learned, the model can be applied to an arbitrary graph, defining a
probability density over the random variables represented by the graph. It is
formulated as an ordinary differential equation system with shared and reusable
functions that operate over the graphs. This leads to a new type of neural
graph message passing scheme that performs continuous message passing over
time. This class of models offers several advantages: a flexible representation
that can generalize to variable data dimensions; ability to model dependencies
in complex data distributions; reversible and memory-efficient; and exact and
efficient computation of the likelihood of the data. We demonstrate the
effectiveness of our model on a diverse set of generation tasks across
different domains: graph generation, image puzzle generation, and layout
generation from scene graphs. Our proposed model achieves significantly better
performance compared to state-of-the-art models."
328,['cs.CV'],Learning Propagation for Arbitrarily-structured Data,"Processing an input signal that contains arbitrary structures, e.g.,
superpixels and point clouds, remains a big challenge in computer vision.
Linear diffusion, an effective model for image processing, has been recently
integrated with deep learning algorithms. In this paper, we propose to learn
pairwise relations among data points in a global fashion to improve semantic
segmentation with arbitrarily-structured data, through spatial generalized
propagation networks (SGPN). The network propagates information on a group of
graphs, which represent the arbitrarily-structured data, through a learned,
linear diffusion process. The module is flexible to be embedded and jointly
trained with many types of networks, e.g., CNNs. We experiment with semantic
segmentation networks, where we use our propagation module to jointly train on
different data -- images, superpixels and point clouds. We show that SGPN
consistently improves the performance of both pixel and point cloud
segmentation, compared to networks that do not contain this module. Our method
suggests an effective way to model the global pairwise relations for
arbitrarily-structured data."
329,"['cs.CV', 'cs.LG']",Multimodal Multitask Representation Learning for Pathology Biobank Metadata Prediction,"Metadata are general characteristics of the data in a well-curated and
condensed format, and have been proven to be useful for decision making,
knowledge discovery, and also heterogeneous data organization of biobank. Among
all data types in the biobank, pathology is the key component of the biobank
and also serves as the gold standard of diagnosis. To maximize the utility of
biobank and allow the rapid progress of biomedical science, it is essential to
organize the data with well-populated pathology metadata. However, manual
annotation of such information is tedious and time-consuming. In the study, we
develop a multimodal multitask learning framework to predict four major
slide-level metadata of pathology images. The framework learns generalizable
representations across tissue slides, pathology reports, and case-level
structured data. We demonstrate improved performance across all four tasks with
the proposed method compared to a single modal single task baseline on two test
sets, one external test set from a distinct data source (TCGA) and one internal
held-out test set (TTH). In the test sets, the performance improvements on the
averaged area under receiver operating characteristic curve across the four
tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology
metadata prediction system may be adopted to mitigate the effort of expert
annotation and ultimately accelerate the data-driven research by better
utilization of the pathology biobank."
330,"['stat.ML', 'cs.LG']",Attention-based Multi-Input Deep Learning Architecture for Biological Activity Prediction: An Application in EGFR Inhibitors,"Machine learning and deep learning have gained popularity and achieved
immense success in Drug discovery in recent decades. Historically, machine
learning and deep learning models were trained on either structural data or
chemical properties by separated model. In this study, we proposed an
architecture training simultaneously both type of data in order to improve the
overall performance. Given the molecular structure in the form of SMILES
notation and their label, we generated the SMILES-based feature matrix and
molecular descriptors. These data were trained on a deep learning model which
was also integrated with the Attention mechanism to facilitate training and
interpreting. Experiments showed that our model could raise the performance of
prediction comparing to the reference. With the maximum MCC 0.58 and AUC 90% by
cross-validation on EGFR inhibitors dataset, our architecture was outperforming
the referring model. We also successfully integrated Attention mechanism into
our model, which helped to interpret the contribution of chemical structures on
bioactivity."
331,"['cs.LG', 'stat.ML']",Exploring Structure-Adaptive Graph Learning for Robust Semi-Supervised Classification,"Graph Convolutional Neural Networks (GCNNs) are generalizations of CNNs to
graph-structured data, in which convolution is guided by the graph topology. In
many cases where graphs are unavailable, existing methods manually construct
graphs or learn task-driven adaptive graphs. In this paper, we propose Graph
Learning Neural Networks (GLNNs), which exploit the optimization of graphs (the
adjacency matrix in particular) from both data and tasks. Leveraging on
spectral graph theory, we propose the objective of graph learning from a
sparsity constraint, properties of a valid adjacency matrix as well as a graph
Laplacian regularizer via maximum a posteriori estimation. The optimization
objective is then integrated into the loss function of the GCNN, which adapts
the graph topology to not only labels of a specific task but also the input
data. Experimental results show that our proposed GLNN outperforms
state-of-the-art approaches over widely adopted social network datasets and
citation network datasets for semi-supervised classification."
332,"['cs.LG', 'stat.ML']",Auto-GNN: Neural Architecture Search of Graph Neural Networks,"Graph neural networks (GNN) has been successfully applied to operate on the
graph-structured data. Given a specific scenario, rich human expertise and
tremendous laborious trials are usually required to identify a suitable GNN
architecture. It is because the performance of a GNN architecture is
significantly affected by the choice of graph convolution components, such as
aggregate function and hidden dimension. Neural architecture search (NAS) has
shown its potential in discovering effective deep architectures for learning
tasks in image and language modeling. However, existing NAS algorithms cannot
be directly applied to the GNN search problem. First, the search space of GNN
is different from the ones in existing NAS work. Second, the representation
learning capacity of GNN architecture changes obviously with slight
architecture modifications. It affects the search efficiency of traditional
search methods. Third, widely used techniques in NAS such as parameter sharing
might become unstable in GNN.
  To bridge the gap, we propose the automated graph neural networks (AGNN)
framework, which aims to find an optimal GNN architecture within a predefined
search space. A reinforcement learning based controller is designed to greedily
validate architectures via small steps. AGNN has a novel parameter sharing
strategy that enables homogeneous architectures to share parameters, based on a
carefully-designed homogeneity definition. Experiments on real-world benchmark
datasets demonstrate that the GNN architecture identified by AGNN achieves the
best performance, comparing with existing handcrafted models and tradistional
search methods."
333,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",A Non-Negative Factorization approach to node pooling in Graph Convolutional Neural Networks,"The paper discusses a pooling mechanism to induce subsampling in graph
structured data and introduces it as a component of a graph convolutional
neural network. The pooling mechanism builds on the Non-Negative Matrix
Factorization (NMF) of a matrix representing node adjacency and node similarity
as adaptively obtained through the vertices embedding learned by the model.
Such mechanism is applied to obtain an incrementally coarser graph where nodes
are adaptively pooled into communities based on the outcomes of the
non-negative factorization. The empirical analysis on graph classification
benchmarks shows how such coarsening process yields significant improvements in
the predictive performance of the model with respect to its non-pooled
counterpart."
334,"['cs.LG', 'stat.ML']",NEAR: Neighborhood Edge AggregatoR for Graph Classification,"Learning graph-structured data with graph neural networks (GNNs) has been
recently emerging as an important field because of its wide applicability in
bioinformatics, chemoinformatics, social network analysis and data mining.
Recent GNN algorithms are based on neural message passing, which enables GNNs
to integrate local structures and node features recursively. However, past GNN
algorithms based on 1-hop neighborhood neural message passing are exposed to a
risk of loss of information on local structures and relationships. In this
paper, we propose Neighborhood Edge AggregatoR (NEAR), a novel framework that
aggregates relations between the nodes in the neighborhood via edges. NEAR,
which can be orthogonally combined with previous GNN algorithms, gives
integrated information that describes which nodes in the neighborhood are
connected. Therefore, GNNs combined with NEAR reflect each node's local
structure beyond the nodes themselves. Experimental results on multiple graph
classification tasks show that our algorithm achieves state-of-the-art results."
335,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",Deep Loopy Neural Network Model for Graph Structured Data Representation Learning,"Existing deep learning models may encounter great challenges in handling
graph structured data. In this paper, we introduce a new deep learning model
for graph data specifically, namely the deep loopy neural network.
Significantly different from the previous deep models, inside the deep loopy
neural network, there exist a large number of loops created by the extensive
connections among nodes in the input graph data, which makes model learning an
infeasible task. To resolve such a problem, in this paper, we will introduce a
new learning algorithm for the deep loopy neural network specifically. Instead
of learning the model variables based on the original model, in the proposed
learning algorithm, errors will be back-propagated through the edges in a group
of extracted spanning trees. Extensive numerical experiments have been done on
several real-world graph datasets, and the experimental results demonstrate the
effectiveness of both the proposed model and the learning algorithm in handling
graph data."
336,"['cs.LG', 'stat.ML']",A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels,"Non-linear kernel methods can be approximated by fast linear ones using
suitable explicit feature maps allowing their application to large scale
problems. We investigate how convolution kernels for structured data are
composed from base kernels and construct corresponding feature maps. On this
basis we propose exact and approximative feature maps for widely used graph
kernels based on the kernel trick. We analyze for which kernels and graph
properties computation by explicit feature maps is feasible and actually more
efficient. In particular, we derive approximative, explicit feature maps for
state-of-the-art kernels supporting real-valued attributes including the
GraphHopper and graph invariant kernels. In extensive experiments we show that
our approaches often achieve a classification accuracy close to the exact
methods based on the kernel trick, but require only a fraction of their running
time. Moreover, we propose and analyze algorithms for computing random walk,
shortest-path and subgraph matching kernels by explicit and implicit feature
maps. Our theoretical results are confirmed experimentally by observing a phase
transition when comparing running time with respect to label diversity, walk
lengths and subgraph size, respectively."
337,"['cs.LG', 'stat.ML']",Adversarial Edit Attacks for Tree Data,"Many machine learning models can be attacked with adversarial examples, i.e.
inputs close to correctly classified examples that are classified incorrectly.
However, most research on adversarial attacks to date is limited to vectorial
data, in particular image data. In this contribution, we extend the field by
introducing adversarial edit attacks for tree-structured data with potential
applications in medicine and automated program analysis. Our approach solely
relies on the tree edit distance and a logarithmic number of black-box queries
to the attacked classifier without any need for gradient information. We
evaluate our approach on two programming and two biomedical data sets and show
that many established tree classifiers, like tree-kernel-SVMs and recursive
neural networks, can be attacked effectively."
338,"['cs.LG', 'cs.SI', 'stat.ML']",DANE: Domain Adaptive Network Embedding,"Recent works reveal that network embedding techniques enable many machine
learning models to handle diverse downstream tasks on graph structured data.
However, as previous methods usually focus on learning embeddings for a single
network, they can not learn representations transferable on multiple networks.
Hence, it is important to design a network embedding algorithm that supports
downstream model transferring on different networks, known as domain
adaptation. In this paper, we propose a novel Domain Adaptive Network Embedding
framework, which applies graph convolutional network to learn transferable
embeddings. In DANE, nodes from multiple networks are encoded to vectors via a
shared set of learnable parameters so that the vectors share an aligned
embedding space. The distribution of embeddings on different networks are
further aligned by adversarial learning regularization. In addition, DANE's
advantage in learning transferable network embedding can be guaranteed
theoretically. Extensive experiments reflect that the proposed framework
outperforms other state-of-the-art network embedding baselines in cross-network
domain adaptation tasks."
339,"['cs.LG', 'stat.ML']",Deep Weisfeiler-Lehman Assignment Kernels via Multiple Kernel Learning,"Kernels for structured data are commonly obtained by decomposing objects into
their parts and adding up the similarities between all pairs of parts measured
by a base kernel. Assignment kernels are based on an optimal bijection between
the parts and have proven to be an effective alternative to the established
convolution kernels. We explore how the base kernel can be learned as part of
the classification problem. We build on the theory of valid assignment kernels
derived from hierarchies defined on the parts. We show that the weights of this
hierarchy can be optimized via multiple kernel learning. We apply this result
to learn vertex similarities for the Weisfeiler-Lehman optimal assignment
kernel for graph classification. We present first experimental results which
demonstrate the feasibility and effectiveness of the approach."
340,"['cs.LG', 'cs.SI', 'stat.ML']",Graph Node Embeddings using Domain-Aware Biased Random Walks,"The recent proliferation of publicly available graph-structured data has
sparked an interest in machine learning algorithms for graph data. Since most
traditional machine learning algorithms assume data to be tabular, embedding
algorithms for mapping graph data to real-valued vector spaces has become an
active area of research. Existing graph embedding approaches are based purely
on structural information and ignore any semantic information from the
underlying domain. In this paper, we demonstrate that semantic information can
play a useful role in computing graph embeddings. Specifically, we present a
framework for devising embedding strategies aware of domain-specific
interpretations of graph nodes and edges, and use knowledge of downstream
machine learning tasks to identify relevant graph substructures. Using two
real-life domains, we show that our framework yields embeddings that are simple
to implement and yet achieve equal or greater accuracy in machine learning
tasks compared to domain independent approaches."
341,"['cs.LG', 'cs.CV', 'stat.ML']",Structuring Autoencoders,"In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural
networks which learn a low dimensional representation of data which are
additionally enriched with a desired structure in this low dimensional space.
While traditional Autoencoders have proven to structure data naturally they
fail to discover semantic structure that is hard to recognize in the raw data.
The SAE solves the problem by enhancing a traditional Autoencoder using weak
supervision to form a structured latent space. In the experiments we
demonstrate, that the structured latent space allows for a much more efficient
data representation for further tasks such as classification for sparsely
labeled data, an efficient choice of data to label, and morphing between
classes. To demonstrate the general applicability of our method, we show
experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2
and on a dataset of 3D human shapes."
342,"['cs.LG', 'stat.ML']",Sparse hierarchical representation learning on molecular graphs,"Architectures for sparse hierarchical representation learning have recently
been proposed for graph-structured data, but so far assume the absence of edge
features in the graph. We close this gap and propose a method to pool graphs
with edge features, inspired by the hierarchical nature of chemistry. In
particular, we introduce two types of pooling layers compatible with an
edge-feature graph-convolutional architecture and investigate their performance
for molecules relevant to drug discovery on a set of two classification and two
regression benchmark datasets of MoleculeNet. We find that our models
significantly outperform previous benchmarks on three of the datasets and reach
state-of-the-art results on the fourth benchmark, with pooling improving
performance for three out of four tasks, keeping performance stable on the
fourth task, and generally speeding up the training process."
343,"['cs.LG', 'stat.ML']",Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,"Neural networks have many successful applications, while much less
theoretical understanding has been gained. Towards bridging this gap, we study
the problem of learning a two-layer overparameterized ReLU neural network for
multi-class classification via stochastic gradient descent (SGD) from random
initialization. In the overparameterized setting, when the data comes from
mixtures of well-separated distributions, we prove that SGD learns a network
with a small generalization error, albeit the network has enough capacity to
fit arbitrary labels. Furthermore, the analysis provides interesting insights
into several aspects of learning neural networks and can be verified based on
empirical studies on synthetic data and on the MNIST dataset."
344,"['cs.LG', 'cs.CL', 'stat.ML']",Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data,"Many common sequential data sources, such as source code and natural
language, have a natural tree-structured representation. These trees can be
generated by fitting a sequence to a grammar, yielding a hierarchical ordering
of the tokens in the sequence. This structure encodes a high degree of
syntactic information, making it ideal for problems such as grammar correction.
However, little work has been done to develop neural networks that can operate
on and exploit tree-structured data. In this paper we present the
Tree-Transformer \textemdash{} a novel neural network architecture designed to
translate between arbitrary input and output trees. We applied this
architecture to correction tasks in both the source code and natural language
domains. On source code, our model achieved an improvement of $25\%$
$\text{F}0.5$ over the best sequential method. On natural language, we achieved
comparable results to the most complex state of the art systems, obtaining a
$10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to
date $\text{F}0.5$ score on the AESW benchmark of $50.43$."
345,"['stat.ML', 'cs.LG']",Node Attribute Generation on Graphs,"Graph structured data provide two-fold information: graph structures and node
attributes. Numerous graph-based algorithms rely on both information to achieve
success in supervised tasks, such as node classification and link prediction.
However, node attributes could be missing or incomplete, which significantly
deteriorates the performance. The task of node attribute generation aims to
generate attributes for those nodes whose attributes are completely unobserved.
This task benefits many real-world problems like profiling, node classification
and graph data augmentation. To tackle this task, we propose a deep adversarial
learning based method to generate node attributes; called node attribute neural
generator (NANG). NANG learns a unifying latent representation which is shared
by both node attributes and graph structures and can be translated to different
modalities. We thus use this latent representation as a bridge to convert
information from one modality to another. We further introduce practical
applications to quantify the performance of node attribute generation.
Extensive experiments are conducted on four real-world datasets and the
empirical results show that node attributes generated by the proposed method
are high-qualitative and beneficial to other applications. The datasets and
codes are available online."
346,"['cs.CV', 'cs.LG']",Image Classification with Hierarchical Multigraph Networks,"Graph Convolutional Networks (GCNs) are a class of general models that can
learn from graph structured data. Despite being general, GCNs are admittedly
inferior to convolutional neural networks (CNNs) when applied to vision tasks,
mainly due to the lack of domain knowledge that is hardcoded into CNNs, such as
spatially oriented translation invariant filters. However, a great advantage of
GCNs is the ability to work on irregular inputs, such as superpixels of images.
This could significantly reduce the computational cost of image reasoning
tasks. Another key advantage inherent to GCNs is the natural ability to model
multirelational data. Building upon these two promising properties, in this
work, we show best practices for designing GCNs for image classification; in
some cases even outperforming CNNs on the MNIST, CIFAR-10 and PASCAL image
datasets."
347,"['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",Graph Neural Network for Interpreting Task-fMRI Biomarkers,"Finding the biomarkers associated with ASD is helpful for understanding the
underlying roots of the disorder and can lead to earlier diagnosis and more
targeted treatment. A promising approach to identify biomarkers is using Graph
Neural Networks (GNNs), which can be used to analyze graph structured data,
i.e. brain networks constructed by fMRI. One way to interpret important
features is through looking at how the classification probability changes if
the features are occluded or replaced. The major limitation of this approach is
that replacing values may change the distribution of the data and lead to
serious errors. Therefore, we develop a 2-stage pipeline to eliminate the need
to replace features for reliable biomarker interpretation. Specifically, we
propose an inductive GNN to embed the graphs containing different properties of
task-fMRI for identifying ASD and then discover the brain regions/sub-graphs
used as evidence for the GNN classifier. We first show GNN can achieve high
accuracy in identifying ASD. Next, we calculate the feature importance scores
using GNN and compare the interpretation ability with Random Forest. Finally,
we run with different atlases and parameters, proving the robustness of the
proposed method. The detected biomarkers reveal their association with social
behaviors. We also show the potential of discovering new informative
biomarkers. Our pipeline can be generalized to other graph feature importance
interpretation problems."
348,"['stat.ML', 'stat.CO']",Splitting Methods for Convex Bi-Clustering and Co-Clustering,"Co-Clustering, the problem of simultaneously identifying clusters across
multiple aspects of a data set, is a natural generalization of clustering to
higher-order structured data. Recent convex formulations of bi-clustering and
tensor co-clustering, which shrink estimated centroids together using a convex
fusion penalty, allow for global optimality guarantees and precise theoretical
analysis, but their computational properties have been less well studied. In
this note, we present three efficient operator-splitting methods for the convex
co-clustering problem: a standard two-block ADMM, a Generalized ADMM which
avoids an expensive tensor Sylvester equation in the primal update, and a
three-block ADMM based on the operator splitting scheme of Davis and Yin.
Theoretical complexity analysis suggests, and experimental evidence confirms,
that the Generalized ADMM is far more efficient for large problems."
349,['cs.CV'],Mapped Convolutions,"We present a versatile formulation of the convolution operation that we term
a ""mapped convolution."" The standard convolution operation implicitly samples
the pixel grid and computes a weighted sum. Our mapped convolution decouples
these two components, freeing the operation from the confines of the image grid
and allowing the kernel to process any type of structured data. As a test case,
we demonstrate its use by applying it to dense inference on spherical data. We
perform an in-depth study of existing spherical image convolution methods and
propose an improved sampling method for equirectangular images. Then, we
discuss the impact of data discretization when deriving a sampling function,
highlighting drawbacks of the cube map representation for spherical data.
Finally, we illustrate how mapped convolutions enable us to convolve directly
on a mesh by projecting the spherical image onto a geodesic grid and training
on the textured mesh. This method exceeds the state of the art for spherical
depth estimation by nearly 17%. Our findings suggest that mapped convolutions
can be instrumental in expanding the application scope of convolutional neural
networks."
350,"['cs.LG', 'cs.SE', 'stat.ML']",Automatic Source Code Summarization with Extended Tree-LSTM,"Neural machine translation models are used to automatically generate a
document from given source code since this can be regarded as a machine
translation task. Source code summarization is one of the components for
automatic document generation, which generates a summary in natural language
from given source code. This suggests that techniques used in neural machine
translation, such as Long Short-Term Memory (LSTM), can be used for source code
summarization. However, there is a considerable difference between source code
and natural language: Source code is essentially {\em structured}, having loops
and conditional branching, etc. Therefore, there is some obstacle to apply
known machine translation models to source code.
  Abstract syntax trees (ASTs) capture these structural properties and play an
important role in recent machine learning studies on source code. Tree-LSTM is
proposed as a generalization of LSTMs for tree-structured data. However, there
is a critical issue when applying it to ASTs: It cannot handle a tree that
contains nodes having an arbitrary number of children and their order
simultaneously, which ASTs generally have such nodes. To address this issue, we
propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and
apply it for source code summarization. As a result of computational
experiments, our proposal achieved better results when compared with several
state-of-the-art techniques."
351,"['cs.CV', 'cs.AI']",CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor,"Extracting key information from documents, such as receipts or invoices, and
preserving the interested texts to structured data is crucial in the
document-intensive streamline processes of office automation in areas that
includes but not limited to accounting, financial, and taxation areas. To avoid
designing expert rules for each specific type of document, some published works
attempt to tackle the problem by learning a model to explore the semantic
context in text sequences based on the Named Entity Recognition (NER) method in
the NLP field. In this paper, we propose to harness the effective information
from both semantic meaning and spatial distribution of texts in documents.
Specifically, our proposed model, Convolutional Universal Text Information
Extractor (CUTIE), applies convolutional neural networks on gridded texts where
texts are embedded as features with semantical connotations. We further explore
the effect of employing different structures of convolutional neural network
and propose a fast and portable structure. We demonstrate the effectiveness of
the proposed method on a dataset with up to $4,484$ labelled receipts, without
any pre-training or post-processing, achieving state of the art performance
that is much better than the NER based methods in terms of either speed and
accuracy. Experimental results also demonstrate that the proposed CUTIE model
being able to achieve good performance with a much smaller amount of training
data."
352,"['cs.LG', 'cs.AI', 'stat.ML']",On Deep Domain Adaptation: Some Theoretical Understandings,"Compared with shallow domain adaptation, recent progress in deep domain
adaptation has shown that it can achieve higher predictive performance and
stronger capacity to tackle structural data (e.g., image and sequential data).
The underlying idea of deep domain adaptation is to bridge the gap between
source and target domains in a joint space so that a supervised classifier
trained on labeled source data can be nicely transferred to the target domain.
This idea is certainly intuitive and powerful, however, limited theoretical
understandings have been developed to support its underpinning principle. In
this paper, we have provided a rigorous framework to explain why it is possible
to close the gap of the target and source domains in the joint space. More
specifically, we first study the loss incurred when performing transfer
learning from the source to the target domain. This provides a theory that
explains and generalizes existing work in deep domain adaptation which was
mainly empirical. This enables us to further explain why closing the gap in the
joint space can directly minimize the loss incurred for transfer learning
between the two domains. To our knowledge, this offers the first theoretical
result that characterizes a direct bound on the joint space and the gain of
transfer learning via deep domain adaptation"
353,"['cs.LG', 'stat.ML', 'I.2.6']",Self-Attention Graph Pooling,"Advanced methods of applying deep learning to structured data such as graphs
have been proposed in recent years. In particular, studies have focused on
generalizing convolutional neural networks to graph data, which includes
redefining the convolution and the downsampling (pooling) operations for
graphs. The method of generalizing the convolution operation to graphs has been
proven to improve performance and is widely used. However, the method of
applying downsampling to graphs is still difficult to perform and has room for
improvement. In this paper, we propose a graph pooling method based on
self-attention. Self-attention using graph convolution allows our pooling
method to consider both node features and graph topology. To ensure a fair
comparison, the same training procedures and model architectures were used for
the existing pooling methods and our method. The experimental results
demonstrate that our method achieves superior graph classification performance
on the benchmark datasets using a reasonable number of parameters."
354,"['cs.LG', 'cs.CL', 'stat.ML']","Partial Or Complete, That's The Question","For many structured learning tasks, the data annotation process is complex
and costly. Existing annotation schemes usually aim at acquiring completely
annotated structures, under the common perception that partial structures are
of low quality and could hurt the learning process. This paper questions this
common perception, motivated by the fact that structures consist of
interdependent sets of variables. Thus, given a fixed budget, partly annotating
each structure may provide the same level of supervision, while allowing for
more structures to be annotated. We provide an information theoretic
formulation for this perspective and use it, in the context of three diverse
structured learning tasks, to show that learning from partial structures can
sometimes outperform learning from complete ones. Our findings may provide
important insights into structured data annotation schemes and could support
progress in learning protocols for structured tasks."
355,"['cs.LG', 'cs.CV', 'stat.ML']",Novelty Detection via Network Saliency in Visual-based Deep Learning,"Machine-learning driven safety-critical autonomous systems, such as
self-driving cars, must be able to detect situations where its trained model is
not able to make a trustworthy prediction. Often viewed as a black-box, it is
non-obvious to determine when a model will make a safe decision and when it
will make an erroneous, perhaps life-threatening one. Prior work on novelty
detection deal with highly structured data and do not translate well to
dynamic, real-world situations. This paper proposes a multi-step framework for
the detection of novel scenarios in vision-based autonomous systems by
leveraging information learned by the trained prediction model and a new image
similarity metric. We demonstrate the efficacy of this method through
experiments on a real-world driving dataset as well as on our in-house indoor
racing environment."
356,"['cs.LG', 'stat.ML']",DEMO-Net: Degree-specific Graph Neural Networks for Node and Graph Classification,"Graph data widely exist in many high-impact applications. Inspired by the
success of deep learning in grid-structured data, graph neural network models
have been proposed to learn powerful node-level or graph-level representation.
However, most of the existing graph neural networks suffer from the following
limitations: (1) there is limited analysis regarding the graph convolution
properties, such as seed-oriented, degree-aware and order-free; (2) the node's
degree-specific graph structure is not explicitly expressed in graph
convolution for distinguishing structure-aware node neighborhoods; (3) the
theoretical explanation regarding the graph-level pooling schemes is unclear.
  To address these problems, we propose a generic degree-specific graph neural
network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test
that recursively identifies 1-hop neighborhood structures. In order to
explicitly capture the graph topology integrated with node attributes, we argue
that graph convolution should have three properties: seed-oriented,
degree-aware, order-free. To this end, we propose multi-task graph convolution
where each task represents node representation learning for nodes with a
specific degree value, thus leading to preserving the degree-specific graph
structure. In particular, we design two multi-task learning methods:
degree-specific weight and hashing functions for graph convolution. In
addition, we propose a novel graph-level pooling/readout scheme for learning
graph representation provably lying in a degree-specific Hilbert kernel space.
The experimental results on several node and graph classification benchmark
data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net
over state-of-the-art graph neural network models."
357,"['cs.LG', 'stat.ML']",Graph Learning Network: A Structure Learning Algorithm,"Recently, graph neural networks (GNNs) have proved to be suitable in tasks on
unstructured data. Particularly in tasks as community detection, node
classification, and link prediction. However, most GNN models still operate
with static relationships. We propose the Graph Learning Network (GLN), a
simple yet effective process to learn node embeddings and structure prediction
functions. Our model uses graph convolutions to propose expected node features,
and predict the best structure based on them. We repeat these steps recursively
to enhance the prediction and the embeddings."
358,"['cs.LG', 'stat.ML']",Decoding Molecular Graph Embeddings with Reinforcement Learning,"We present RL-VAE, a graph-to-graph variational autoencoder that uses
reinforcement learning to decode molecular graphs from latent embeddings.
Methods have been described previously for graph-to-graph autoencoding, but
these approaches require sophisticated decoders that increase the complexity of
training and evaluation (such as requiring parallel encoders and decoders or
non-trivial graph matching). Here, we repurpose a simple graph generator to
enable efficient decoding and generation of molecular graphs."
359,"['cs.LG', 'cs.CV', 'stat.ML']",Are Graph Neural Networks Miscalibrated?,"Graph Neural Networks (GNNs) have proven to be successful in many
classification tasks, outperforming previous state-of-the-art methods in terms
of accuracy. However, accuracy alone is not enough for high-stakes decision
making. Decision makers want to know the likelihood that a specific GNN
prediction is correct. For this purpose, obtaining calibrated models is
essential. In this work, we perform an empirical evaluation of the calibration
of state-of-the-art GNNs on multiple datasets. Our experiments show that GNNs
can be calibrated in some datasets but also badly miscalibrated in others, and
that state-of-the-art calibration methods are helpful but do not fix the
problem."
360,"['cs.LG', 'stat.ML']",Approximation capability of neural networks on spaces of probability measures and tree-structured domains,"This paper extends the proof of density of neural networks in the space of
continuous (or even measurable) functions on Euclidean spaces to functions on
compact sets of probability measures. By doing so the work parallels a more
then a decade old results on mean-map embedding of probability measures in
reproducing kernel Hilbert spaces. The work has wide practical consequences for
multi-instance learning, where it theoretically justifies some recently
proposed constructions. The result is then extended to Cartesian products,
yielding universal approximation theorem for tree-structured domains, which
naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and
ProtoBuffer. This has important practical implications, as it enables to
automatically create an architecture of neural networks for processing
structured data (AutoML paradigms), as demonstrated by an accompanied library
for JSON format."
361,"['cs.LG', 'stat.ML']",Factor Graph Neural Network,"Most of the successful deep neural network architectures are structured,
often consisting of elements like convolutional neural networks and gated
recurrent neural networks. Recently, graph neural networks have been
successfully applied to graph structured data such as point cloud and molecular
data. These networks often only consider pairwise dependencies, as they operate
on a graph structure. We generalize the graph neural network into a factor
graph neural network (FGNN) in order to capture higher order dependencies. We
show that FGNN is able to represent Max-Product Belief Propagation, an
approximate inference algorithm on probabilistic graphical models; hence it is
able to do well when Max-Product does well. Promising results on both synthetic
and real datasets demonstrate the effectiveness of the proposed model."
362,"['cs.LG', 'stat.ML']",Model Agnostic Contrastive Explanations for Structured Data,"Recently, a method [7] was proposed to generate contrastive explanations for
differentiable models such as deep neural networks, where one has complete
access to the model. In this work, we propose a method, Model Agnostic
Contrastive Explanations Method (MACEM), to generate contrastive explanations
for \emph{any} classification model where one is able to \emph{only} query the
class probabilities for a desired input. This allows us to generate contrastive
explanations for not only neural networks, but models such as random forests,
boosted trees and even arbitrary ensembles that are still amongst the
state-of-the-art when learning on structured data [13]. Moreover, to obtain
meaningful explanations we propose a principled approach to handle real and
categorical features leading to novel formulations for computing pertinent
positives and negatives that form the essence of a contrastive explanation. A
detailed treatment of the different data types of this nature was not performed
in the previous work, which assumed all features to be positive real valued
with zero being indicative of the least interesting value. We part with this
strong implicit assumption and generalize these methods so as to be applicable
across a much wider range of problem settings. We quantitatively and
qualitatively validate our approach over 5 public datasets covering diverse
domains."
363,"['cs.LG', 'stat.ML']",Bayesian Tensor Factorisation for Bottom-up Hidden Tree Markov Models,"Bottom-Up Hidden Tree Markov Model is a highly expressive model for
tree-structured data. Unfortunately, it cannot be used in practice due to the
intractable size of its state-transition matrix. We propose a new approximation
which lies on the Tucker factorisation of tensors. The probabilistic
interpretation of such approximation allows us to define a new probabilistic
model for tree-structured data. Hence, we define the new approximated model and
we derive its learning algorithm. Then, we empirically assess the effective
power of the new model evaluating it on two different tasks. In both cases, our
model outperforms the other approximated model known in the literature."
364,"['cs.LG', 'stat.ML']",Path-Augmented Graph Transformer Network,"Much of the recent work on learning molecular representations has been based
on Graph Convolution Networks (GCN). These models rely on local aggregation
operations and can therefore miss higher-order graph properties. To remedy
this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are
explicitly built on longer-range dependencies in graph-structured data.
Specifically, we use path features in molecular graphs to create global
attention layers. We compare our PAGTN model against the GCN model and show
that our model consistently outperforms GCNs on molecular property prediction
datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL,
Lipophilictiy) and biochemistry (BACE, BBBP)."
365,"['cs.LG', 'stat.ML']",On the equivalence between graph isomorphism testing and function approximation with GNNs,"Graph neural networks (GNNs) have achieved lots of success on
graph-structured data. In the light of this, there has been increasing interest
in studying their representation power. One line of work focuses on the
universal approximation of permutation-invariant functions by certain classes
of GNNs, and another demonstrates the limitation of GNNs via graph isomorphism
tests.
  Our work connects these two perspectives and proves their equivalence. We
further develop a framework of the representation power of GNNs with the
language of sigma-algebra, which incorporates both viewpoints. Using this
framework, we compare the expressive power of different classes of GNNs as well
as other methods on graphs. In particular, we prove that order-2 Graph
G-invariant networks fail to distinguish non-isomorphic regular graphs with the
same degree. We then extend them to a new architecture, Ring-GNNs, which
succeeds on distinguishing these graphs and provides improvements on real-world
social network datasets."
366,"['stat.ML', 'cs.AI', 'cs.LG']",GraphNVP: An Invertible Flow Model for Generating Molecular Graphs,"We propose GraphNVP, the first invertible, normalizing flow-based molecular
graph generation model. We decompose the generation of a graph into two steps:
generation of (i) an adjacency tensor and (ii) node attributes. This
decomposition yields the exact likelihood maximization on graph-structured
data, combined with two novel reversible flows. We empirically demonstrate that
our model efficiently generates valid molecular graphs with almost no
duplicated molecules. In addition, we observe that the learned latent space can
be used to generate molecules with desired chemical properties."
367,"['cs.LG', 'stat.ML']",Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,"Many machine learning tasks such as multiple instance learning, 3D shape
recognition, and few-shot image classification are defined on sets of
instances. Since solutions to such problems do not depend on the order of
elements of the set, models used to address them should be permutation
invariant. We present an attention-based neural network module, the Set
Transformer, specifically designed to model interactions among elements in the
input set. The model consists of an encoder and a decoder, both of which rely
on attention mechanisms. In an effort to reduce computational complexity, we
introduce an attention scheme inspired by inducing point methods from sparse
Gaussian process literature. It reduces the computation time of self-attention
from quadratic to linear in the number of elements in the set. We show that our
model is theoretically attractive and we evaluate it on a range of tasks,
demonstrating the state-of-the-art performance compared to recent methods for
set-structured data."
368,"['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.SP']",Revisiting Graph Neural Networks: All We Have is Low-Pass Filters,"Graph neural networks have become one of the most important techniques to
solve machine learning problems on graph-structured data. Recent work on vertex
classification proposed deep and distributed learning models to achieve high
performance and scalability. However, we find that the feature vectors of
benchmark datasets are already quite informative for the classification task,
and the graph structure only provides a means to denoise the data. In this
paper, we develop a theoretical framework based on graph signal processing for
analyzing graph neural networks. Our results indicate that graph neural
networks only perform low-pass filtering on feature vectors and do not have the
non-linear manifold learning property. We further investigate their resilience
to feature noise and propose some insights on GCN-based graph neural network
design."
369,"['cs.LG', 'cs.SI', 'stat.ML']",Graph Attention Auto-Encoders,"Auto-encoders have emerged as a successful framework for unsupervised
learning. However, conventional auto-encoders are incapable of utilizing
explicit relations in structured data. To take advantage of relations in
graph-structured data, several graph auto-encoders have recently been proposed,
but they neglect to reconstruct either the graph structure or node attributes.
In this paper, we present the graph attention auto-encoder (GATE), a neural
network architecture for unsupervised representation learning on
graph-structured data. Our architecture is able to reconstruct graph-structured
inputs, including both node attributes and the graph structure, through stacked
encoder/decoder layers equipped with self-attention mechanisms. In the encoder,
by considering node attributes as initial node representations, each layer
generates new representations of nodes by attending over their neighbors'
representations. In the decoder, we attempt to reverse the encoding process to
reconstruct node attributes. Moreover, node representations are regularized to
reconstruct the graph structure. Our proposed architecture does not need to
know the graph structure upfront, and thus it can be applied to inductive
learning. Our experiments demonstrate competitive performance on several node
classification benchmark datasets for transductive and inductive tasks, even
exceeding the performance of supervised learning baselines in most cases."
370,"['cs.LG', 'cs.AI', 'stat.ML']",Batch Virtual Adversarial Training for Graph Convolutional Networks,"We present batch virtual adversarial training (BVAT), a novel regularization
method for graph convolutional networks (GCNs). BVAT addresses the shortcoming
of GCNs that do not consider the smoothness of the model's output distribution
against local perturbations around the input. We propose two algorithms,
sample-based BVAT and optimization-based BVAT, which are suitable to promote
the smoothness of the model for graph-structured data by either finding virtual
adversarial perturbations for a subset of nodes far from each other or
generating virtual adversarial perturbations for all nodes with an optimization
process. Extensive experiments on three citation network datasets Cora,
Citeseer and Pubmed and a knowledge graph dataset Nell validate the
effectiveness of the proposed method, which establishes state-of-the-art
results in the semi-supervised node classification tasks."
371,"['cs.LG', 'cs.SI', 'stat.ML']",Learning graphs from data: A signal representation perspective,"The construction of a meaningful graph topology plays a crucial role in the
effective representation, processing, analysis and visualization of structured
data. When a natural choice of the graph is not readily available from the data
sets, it is thus desirable to infer or learn a graph topology from the data. In
this tutorial overview, we survey solutions to the problem of graph learning,
including classical viewpoints from statistics and physics, and more recent
approaches that adopt a graph signal processing (GSP) perspective. We further
emphasize the conceptual similarities and differences between classical and
GSP-based graph inference methods, and highlight the potential advantage of the
latter in a number of theoretical and practical scenarios. We conclude with
several open issues and challenges that are keys to the design of future signal
processing and machine learning algorithms for learning graphs from data."
372,"['cs.LG', 'stat.ML']",Graph Convolutional Networks with EigenPooling,"Graph neural networks, which generalize deep neural network models to graph
structured data, have attracted increasing attention in recent years. They
usually learn node representations by transforming, propagating and aggregating
node features and have been proven to improve the performance of many graph
related tasks such as node classification and link prediction. To apply graph
neural networks for the graph classification task, approaches to generate the
\textit{graph representation} from node representations are demanded. A common
way is to globally combine the node representations. However, rich structural
information is overlooked. Thus a hierarchical pooling procedure is desired to
preserve the graph structure during the graph representation learning. There
are some recent works on hierarchically learning graph representation analogous
to the pooling step in conventional convolutional neural (CNN) networks.
However, the local structural information is still largely neglected during the
pooling process. In this paper, we introduce a pooling operator $\pooling$
based on graph Fourier transform, which can utilize the node features and local
structures during the pooling process. We then design pooling layers based on
the pooling operator, which are further combined with traditional GCN
convolutional layers to form a graph neural network framework $\m$ for graph
classification. Theoretical analysis is provided to understand $\pooling$ from
both local and global perspectives. Experimental results of the graph
classification task on $6$ commonly used benchmarks demonstrate the
effectiveness of the proposed framework."
373,"['cs.LG', 'cs.AI', 'stat.ML']",IPC: A Benchmark Data Set for Learning with Graph-Structured Data,"Benchmark data sets are an indispensable ingredient of the evaluation of
graph-based machine learning methods. We release a new data set, compiled from
International Planning Competitions (IPC), for benchmarking graph
classification, regression, and related tasks. Apart from the graph
construction (based on AI planning problems) that is interesting in its own
right, the data set possesses distinctly different characteristics from
popularly used benchmarks. The data set, named IPC, consists of two
self-contained versions, grounded and lifted, both including graphs of large
and skewedly distributed sizes, posing substantial challenges for the
computation of graph models such as graph kernels and graph neural networks.
The graphs in this data set are directed and the lifted version is acyclic,
offering the opportunity of benchmarking specialized models for directed
(acyclic) structures. Moreover, the graph generator and the labeling are
computer programmed; thus, the data set may be extended easily if a larger
scale is desired. The data set is accessible from
\url{https://github.com/IBM/IPC-graph-data}."
374,"['cs.LG', 'stat.ML']",Embeddings and Representation Learning for Structured Data,"Performing machine learning on structured data is complicated by the fact
that such data does not have vectorial form. Therefore, multiple approaches
have emerged to construct vectorial representations of structured data, from
kernel and distance approaches to recurrent, recursive, and convolutional
neural networks. Recent years have seen heightened attention in this demanding
field of research and several new approaches have emerged, such as metric
learning on structured data, graph convolutional neural networks, and recurrent
decoder networks for structured data. In this contribution, we provide an
high-level overview of the state-of-the-art in representation learning and
embeddings for structured data across a wide range of machine learning fields."
375,"['cs.LG', 'stat.ML']",Hyperbolic Disk Embeddings for Directed Acyclic Graphs,"Obtaining continuous representations of structural data such as directed
acyclic graphs (DAGs) has gained attention in machine learning and artificial
intelligence. However, embedding complex DAGs in which both ancestors and
descendants of nodes are exponentially increasing is difficult. Tackling in
this problem, we develop Disk Embeddings, which is a framework for embedding
DAGs into quasi-metric spaces. Existing state-of-the-art methods, Order
Embeddings and Hyperbolic Entailment Cones, are instances of Disk Embedding in
Euclidean space and spheres respectively. Furthermore, we propose a novel
method Hyperbolic Disk Embeddings to handle exponential growth of relations.
The results of our experiments show that our Disk Embedding models outperform
existing methods especially in complex DAGs other than trees."
376,"['cs.LG', 'cs.SI', 'stat.ML']",Stochastic Blockmodels meet Graph Neural Networks,"Stochastic blockmodels (SBM) and their variants, $e.g.$, mixed-membership and
overlapping stochastic blockmodels, are latent variable based generative models
for graphs. They have proven to be successful for various tasks, such as
discovering the community structure and link prediction on graph-structured
data. Recently, graph neural networks, $e.g.$, graph convolutional networks,
have also emerged as a promising approach to learn powerful representations
(embeddings) for the nodes in the graph, by exploiting graph properties such as
locality and invariance. In this work, we unify these two directions by
developing a \emph{sparse} variational autoencoder for graphs, that retains the
interpretability of SBMs, while also enjoying the excellent predictive
performance of graph neural nets. Moreover, our framework is accompanied by a
fast recognition model that enables fast inference of the node embeddings
(which are of independent interest for inference in SBM and its variants).
Although we develop this framework for a particular type of SBM, namely the
\emph{overlapping} stochastic blockmodel, the proposed framework can be adapted
readily for other types of SBMs. Experimental results on several benchmarks
demonstrate encouraging results on link prediction while learning an
interpretable latent structure that can be used for community discovery."
377,"['stat.ML', 'cs.LG']",Optimal Transport for structured data with application on graphs,"This work considers the problem of computing distances between structured
objects such as undirected graphs, seen as probability distributions in a
specific metric space. We consider a new transportation distance (i.e. that
minimizes a total cost of transporting probability masses) that unveils the
geometric nature of the structured objects space. Unlike Wasserstein or
Gromov-Wasserstein metrics that focus solely and respectively on features (by
considering a metric in the feature space) or structure (by seeing structure as
a metric space), our new distance exploits jointly both information, and is
consequently called Fused Gromov-Wasserstein (FGW). After discussing its
properties and computational aspects, we show results on a graph classification
task, where our method outperforms both graph kernels and deep graph
convolutional networks. Exploiting further on the metric properties of FGW,
interesting geometric objects such as Fr\'echet means or barycenters of graphs
are illustrated and discussed in a clustering context."
378,"['cs.LG', 'stat.ML']",Graph Matching Networks for Learning the Similarity of Graph Structured Objects,"This paper addresses the challenging problem of retrieval and matching of
graph structured objects, and makes two key contributions. First, we
demonstrate how Graph Neural Networks (GNN), which have emerged as an effective
model for various supervised prediction problems defined on structured data,
can be trained to produce embedding of graphs in vector spaces that enables
efficient similarity reasoning. Second, we propose a novel Graph Matching
Network model that, given a pair of graphs as input, computes a similarity
score between them by jointly reasoning on the pair through a new cross-graph
attention-based matching mechanism. We demonstrate the effectiveness of our
models on different domains including the challenging problem of
control-flow-graph based function similarity search that plays an important
role in the detection of vulnerabilities in software systems. The experimental
analysis demonstrates that our models are not only able to exploit structure in
the context of similarity learning but they can also outperform domain-specific
baseline systems that have been carefully hand-engineered for these problems."
379,"['cs.LG', 'stat.ML']","On Graph Classification Networks, Datasets and Baselines","Graph classification receives a great deal of attention from the
non-Euclidean machine learning community. Recent advances in graph coarsening
have enabled the training of deeper networks and produced new state-of-the-art
results in many benchmark tasks. We examine how these architectures train and
find that performance is highly-sensitive to initialisation and depends
strongly on jumping-knowledge structures. We then show that, despite the great
complexity of these models, competitive performance is achieved by the simplest
of models -- structure-blind MLP, single-layer GCN and fixed-weight GCN -- and
propose these be included as baselines in future."
380,"['cs.LG', 'cs.AI', 'stat.ML']",Hybrid Predictive Model: When an Interpretable Model Collaborates with a Black-box Model,"Interpretable machine learning has become a strong competitor for traditional
black-box models. However, the possible loss of the predictive performance for
gaining interpretability is often inevitable, putting practitioners in a
dilemma of choosing between high accuracy (black-box models) and
interpretability (interpretable models). In this work, we propose a novel
framework for building a Hybrid Predictive Model (HPM) that integrates an
interpretable model with any black-box model to combine their strengths. The
interpretable model substitutes the black-box model on a subset of data where
the black-box is overkill or nearly overkill, gaining transparency at no or low
cost of the predictive accuracy. We design a principled objective function that
considers predictive accuracy, model interpretability, and model transparency
(defined as the percentage of data processed by the interpretable substitute.)
Under this framework, we propose two hybrid models, one substituting with
association rules and the other with linear models, and we design customized
training algorithms for both models. We test the hybrid models on structured
data and text data where interpretable models collaborate with various
state-of-the-art black-box models. Results show that hybrid models obtain an
efficient trade-off between transparency and predictive performance,
characterized by our proposed efficient frontiers."
381,"['cs.CV', 'cs.AI', 'cs.LG']",Interactive Image Generation Using Scene Graphs,"Recent years have witnessed some exciting developments in the domain of
generating images from scene-based text descriptions. These approaches have
primarily focused on generating images from a static text description and are
limited to generating images in a single pass. They are unable to generate an
image interactively based on an incrementally additive text description
(something that is more intuitive and similar to the way we describe an image).
We propose a method to generate an image incrementally based on a sequence of
graphs of scene descriptions (scene-graphs). We propose a recurrent network
architecture that preserves the image content generated in previous steps and
modifies the cumulative image as per the newly provided scene information. Our
model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized
scene graphs along with Generative Adversarial image translation networks to
generate realistic multi-object images without needing any intermediate
supervision during training. We experiment with Coco-Stuff dataset which has
multi-object images along with annotations describing the visual scene and show
that our model significantly outperforms other approaches on the same dataset
in generating visually consistent images for incrementally growing scene
graphs."
382,"['cs.CV', 'stat.ML']",Multi-level 3D CNN for Learning Multi-scale Spatial Features,"3D object recognition accuracy can be improved by learning the multi-scale
spatial features from 3D spatial geometric representations of objects such as
point clouds, 3D models, surfaces, and RGB-D data. Current deep learning
approaches learn such features either using structured data representations
(voxel grids and octrees) or from unstructured representations (graphs and
point clouds). Learning features from such structured representations is
limited by the restriction on resolution and tree depth while unstructured
representations creates a challenge due to non-uniformity among data samples.
In this paper, we propose an end-to-end multi-level learning approach on a
multi-level voxel grid to overcome these drawbacks. To demonstrate the utility
of the proposed multi-level learning, we use a multi-level voxel representation
of 3D objects to perform object recognition. The multi-level voxel
representation consists of a coarse voxel grid that contains volumetric
information of the 3D object. In addition, each voxel in the coarse grid that
contains a portion of the object boundary is subdivided into multiple
fine-level voxel grids. The performance of our multi-level learning algorithm
for object recognition is comparable to dense voxel representations while using
significantly lower memory."
383,"['stat.ML', 'cs.LG']",Graph Kernels: A Survey,"Graph kernels have attracted a lot of attention during the last decade, and
have evolved into a rapidly developing branch of learning on structured data.
During the past 20 years, the considerable research activity that occurred in
the field resulted in the development of dozens of graph kernels, each focusing
on specific structural properties of graphs. Graph kernels have proven
successful in a wide range of domains, ranging from social networks to
bioinformatics. The goal of this survey is to provide a unifying view of the
literature on graph kernels. In particular, we present a comprehensive overview
of a wide range of graph kernels. Furthermore, we perform an experimental
evaluation of several of those kernels on publicly available datasets, and
provide a comparative study. Finally, we discuss key applications of graph
kernels, and outline some challenges that remain to be addressed."
384,"['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.SI', 'stat.ML']",PAN: Path Integral Based Convolution for Deep Graph Neural Networks,"Convolution operations designed for graph-structured data usually utilize the
graph Laplacian, which can be seen as message passing between the adjacent
neighbors through a generic random walk. In this paper, we propose PAN, a new
graph convolution framework that involves every path linking the message sender
and receiver with learnable weights depending on the path length, which
corresponds to the maximal entropy random walk. PAN generalizes the graph
Laplacian to a new transition matrix we call \emph{maximal entropy transition}
(MET) matrix derived from a path integral formalism. Most previous graph
convolutional network architectures can be adapted to our framework, and many
variations and derivatives based on the path integral idea can be developed.
Experimental results show that the path integral based graph neural networks
have great learnability and fast convergence rate, and achieve state-of-the-art
performance on benchmark tasks."
385,"['cs.LG', 'stat.ML', 'I.2.6']","Generated Loss, Augmented Training, and Multiscale VAE","The variational autoencoder (VAE) framework remains a popular option for
training unsupervised generative models, especially for discrete data where
generative adversarial networks (GANs) require workaround to create gradient
for the generator. In our work modeling US postal addresses, we show that our
discrete VAE with tree recursive architecture demonstrates limited capability
of capturing field correlations within structured data, even after overcoming
the challenge of posterior collapse with scheduled sampling and tuning of the
KL-divergence weight $\beta$. Worse, VAE seems to have difficulty mapping its
generated samples to the latent space, as their VAE loss lags behind or even
increases during the training process. Motivated by this observation, we show
that augmenting training data with generated variants (augmented training) and
training a VAE with multiple values of $\beta$ simultaneously (multiscale VAE)
both improve the generation quality of VAE. Despite their differences in
motivation and emphasis, we show that augmented training and multiscale VAE are
actually connected and have similar effects on the model."
386,"['cs.LG', 'stat.ML']",GraphTSNE: A Visualization Technique for Graph-Structured Data,"We present GraphTSNE, a novel visualization technique for graph-structured
data based on t-SNE. The growing interest in graph-structured data increases
the importance of gaining human insight into such datasets by means of
visualization. Among the most popular visualization techniques, classical t-SNE
is not suitable on such datasets because it has no mechanism to make use of
information from the graph structure. On the other hand, visualization
techniques which operate on graphs, such as Laplacian Eigenmaps and tsNET, have
no mechanism to make use of information from node features. Our proposed method
GraphTSNE produces visualizations which account for both graph structure and
node features. It is based on scalable and unsupervised training of a graph
convolutional network on a modified t-SNE loss. By assembling a suite of
evaluation metrics, we demonstrate that our method produces desirable
visualizations on three benchmark datasets."
387,"['cs.LG', 'stat.ML']",Feature Grouping as a Stochastic Regularizer for High-Dimensional Structured Data,"In many applications where collecting data is expensive, for example
neuroscience or medical imaging, the sample size is typically small compared to
the feature dimension. It is challenging in this setting to train expressive,
non-linear models without overfitting. These datasets call for intelligent
regularization that exploits known structure, such as correlations between the
features arising from the measurement device. However, existing structured
regularizers need specially crafted solvers, which are difficult to apply to
complex models. We propose a new regularizer specifically designed to leverage
structure in the data in a way that can be applied efficiently to complex
models. Our approach relies on feature grouping, using a fast clustering
algorithm inside a stochastic gradient descent loop: given a family of feature
groupings that capture feature covariations, we randomly select these groups at
each iteration. We show that this approach amounts to enforcing a denoising
regularizer on the solution. The method is easy to implement in many model
architectures, such as fully connected neural networks, and has a linear
computational cost. We apply this regularizer to a real-world fMRI dataset and
the Olivetti Faces datasets. Experiments on both datasets demonstrate that the
proposed approach produces models that generalize better than those trained
with conventional regularizers, and also improves convergence speed."
388,['cs.CV'],Optimized Skeleton-based Action Recognition via Sparsified Graph Regression,"With the prevalence of accessible depth sensors, dynamic human body skeletons
have attracted much attention as a robust modality for action recognition.
Previous methods model skeletons based on RNN or CNN, which has limited
expressive power for irregular skeleton joints. While graph convolutional
networks (GCN) have been proposed to address irregular graph-structured data,
the fundamental graph construction remains challenging. In this paper, we
represent skeletons naturally on graphs, and propose a graph regression based
GCN (GR-GCN) for skeleton-based action recognition, aiming to capture the
spatio-temporal variation in the data. As the graph representation is crucial
to graph convolution, we first propose graph regression to statistically learn
the underlying graph from multiple observations. In particular, we provide
spatio-temporal modeling of skeletons and pose an optimization problem on the
graph structure over consecutive frames, which enforces the sparsity of the
underlying graph for efficient representation. The optimized graph not only
connects each joint to its neighboring joints in the same frame strongly or
weakly, but also links with relevant joints in the previous and subsequent
frames. We then feed the optimized graph into the GCN along with the
coordinates of the skeleton sequence for feature learning, where we deploy
high-order and fast Chebyshev approximation of spectral graph convolution.
Further, we provide analysis of the variation characterization by the Chebyshev
approximation. Experimental results validate the effectiveness of the proposed
graph regression and show that the proposed GR-GCN achieves the
state-of-the-art performance on the widely used NTU RGB+D, UT-Kinect and SYSU
3D datasets."
389,['cs.CV'],"Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation","Generating long and semantic-coherent reports to describe medical images
poses great challenges towards bridging visual and linguistic modalities,
incorporating medical domain knowledge, and generating realistic and accurate
descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase
(KERP) approach which reconciles traditional knowledge- and retrieval-based
methods with modern learning-based methods for accurate and robust medical
report generation. Specifically, KERP decomposes medical report generation into
explicit medical abnormality graph learning and subsequent natural language
modeling. KERP first employs an Encode module that transforms visual features
into a structured abnormality graph by incorporating prior medical knowledge;
then a Retrieve module that retrieves text templates based on the detected
abnormalities; and lastly, a Paraphrase module that rewrites the templates
according to specific cases. The core of KERP is a proposed generic
implementation unit---Graph Transformer (GTR) that dynamically transforms
high-level semantics between graph-structured data of multiple domains such as
knowledge graphs, images and sequences. Experiments show that the proposed
approach generates structured and robust reports supported with accurate
abnormality description and explainable attentive regions, achieving the
state-of-the-art results on two medical report benchmarks, with the best
medical abnormality and disease classification accuracy and improved human
evaluation performance."
390,"['cs.LG', 'stat.ML']",Functional Transparency for Structured Data: a Game-Theoretic Approach,"We provide a new approach to training neural models to exhibit transparency
in a well-defined, functional manner. Our approach naturally operates over
structured data and tailors the predictor, functionally, towards a chosen
family of (local) witnesses. The estimation problem is setup as a co-operative
game between an unrestricted predictor such as a neural network, and a set of
witnesses chosen from the desired transparent family. The goal of the witnesses
is to highlight, locally, how well the predictor conforms to the chosen family
of functions, while the predictor is trained to minimize the highlighted
discrepancy. We emphasize that the predictor remains globally powerful as it is
only encouraged to agree locally with locally adapted witnesses. We analyze the
effect of the proposed approach, provide example formulations in the context of
deep graph and sequence models, and empirically illustrate the idea in chemical
property prediction, temporal modeling, and molecule representation learning."
391,"['cs.LG', 'stat.ML']",Topological Bayesian Optimization with Persistence Diagrams,"Finding an optimal parameter of a black-box function is important for
searching stable material structures and finding optimal neural network
structures, and Bayesian optimization algorithms are widely used for the
purpose. However, most of existing Bayesian optimization algorithms can only
handle vector data and cannot handle complex structured data. In this paper, we
propose the topological Bayesian optimization, which can efficiently find an
optimal solution from structured data using \emph{topological information}.
More specifically, in order to apply Bayesian optimization to structured data,
we extract useful topological information from a structure and measure the
proper similarity between structures. To this end, we utilize persistent
homology, which is a topological data analysis method that was recently applied
in machine learning. Moreover, we propose the Bayesian optimization algorithm
that can handle multiple types of topological information by using a linear
combination of kernels for persistence diagrams. Through experiments, we show
that topological information extracted by persistent homology contributes to a
more efficient search for optimal structures compared to the random search
baseline and the graph Bayesian optimization algorithm."
392,"['cs.LG', 'stat.ML']",Classifying Signals on Irregular Domains via Convolutional Cluster Pooling,"We present a novel and hierarchical approach for supervised classification of
signals spanning over a fixed graph, reflecting shared properties of the
dataset. To this end, we introduce a Convolutional Cluster Pooling layer
exploiting a multi-scale clustering in order to highlight, at different
resolutions, locally connected regions on the input graph. Our proposal
generalises well-established neural models such as Convolutional Neural
Networks (CNNs) on irregular and complex domains, by means of the exploitation
of the weight sharing property in a graph-oriented architecture. In this work,
such property is based on the centrality of each vertex within its
soft-assigned cluster. Extensive experiments on NTU RGB+D, CIFAR-10 and 20NEWS
demonstrate the effectiveness of the proposed technique in capturing both local
and global patterns in graph-structured data out of different domains."
393,"['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",Deep Learning on Attributed Graphs: A Journey from Graphs to Their Embeddings and Back,"A graph is a powerful concept for representation of relations between pairs
of entities. Data with underlying graph structure can be found across many
disciplines and there is a natural desire for understanding such data better.
Deep learning (DL) has achieved significant breakthroughs in a variety of
machine learning tasks in recent years, especially where data is structured on
a grid, such as in text, speech, or image understanding. However, surprisingly
little has been done to explore the applicability of DL on arbitrary
graph-structured data directly.
  The goal of this thesis is to investigate architectures for DL on graphs and
study how to transfer, adapt or generalize concepts that work well on
sequential and image data to this domain. We concentrate on two important
primitives: embedding graphs or their nodes into a continuous vector space
representation (encoding) and, conversely, generating graphs from such vectors
back (decoding). To that end, we make the following contributions.
  First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like
operation on graphs performed in the spatial domain where filters are
dynamically generated based on edge attributes. The method is used to encode
graphs with arbitrary and varying structure.
  Second, we propose SuperPoint Graph, an intermediate point cloud
representation with rich edge attributes encoding the contextual relationship
between object parts. Based on this representation, ECC is employed to segment
large-scale point clouds without major sacrifice in fine details.
  Third, we present GraphVAE, a graph generator allowing us to decode graphs
with variable but upper-bounded number of nodes making use of approximate graph
matching for aligning the predictions of an autoencoder with its inputs. The
method is applied to the task of molecule generation."
394,['cs.CV'],Multiple Graph Adversarial Learning,"Recently, Graph Convolutional Networks (GCNs) have been widely studied for
graph-structured data representation and learning. However, in many real
applications, data are coming with multiple graphs, and it is non-trivial to
adapt GCNs to deal with data representation with multiple graph structures. One
main challenge for multi-graph representation is how to exploit both structure
information of each individual graph and correlation information across
multiple graphs simultaneously. In this paper, we propose a novel Multiple
Graph Adversarial Learning (MGAL) framework for multi-graph representation and
learning. MGAL aims to learn an optimal structure-invariant and consistent
representation for multiple graphs in a common subspace via a novel adversarial
learning framework, which thus incorporates both structure information of
intra-graph and correlation information of inter-graphs simultaneously. Based
on MGAL, we then provide a unified network for semi-supervised learning task.
Promising experimental results demonstrate the effectiveness of MGAL model."
395,['cs.CV'],A Functional Representation for Graph Matching,"Graph matching is an important and persistent problem in computer vision and
pattern recognition for finding node-to-node correspondence between
graph-structured data. However, as widely used, graph matching that
incorporates pairwise constraints can be formulated as a quadratic assignment
problem (QAP), which is NP-complete and results in intrinsic computational
difficulties. In this paper, we present a functional representation for graph
matching (FRGM) that aims to provide more geometric insights on the problem and
reduce the space and time complexities of corresponding algorithms. To achieve
these goals, we represent a graph endowed with edge attributes by a linear
function space equipped with a functional such as inner product or metric, that
has an explicit geometric meaning. Consequently, the correspondence between
graphs can be represented as a linear representation map of that functional.
Specifically, we reformulate the linear functional representation map as a new
parameterization for Euclidean graph matching, which is associative with
geometric parameters for graphs under rigid or nonrigid deformations. This
allows us to estimate the correspondence and geometric deformations
simultaneously. The use of the representation of edge attributes rather than
the affinity matrix enables us to reduce the space complexity by two orders of
magnitudes. Furthermore, we propose an efficient optimization strategy with low
time complexity to optimize the objective function. The experimental results on
both synthetic and real-world datasets demonstrate that the proposed FRGM can
achieve state-of-the-art performance."
396,"['cs.LG', 'stat.ML']",Lovasz Convolutional Networks,"Semi-supervised learning on graph structured data has received significant
attention with the recent introduction of Graph Convolution Networks (GCN).
While traditional methods have focused on optimizing a loss augmented with
Laplacian regularization framework, GCNs perform an implicit Laplacian type
regularization to capture local graph structure. In this work, we propose
Lovasz Convolutional Network (LCNs) which are capable of incorporating global
graph properties. LCNs achieve this by utilizing Lovasz's orthonormal
embeddings of the nodes. We analyse local and global properties of graphs and
demonstrate settings where LCNs tend to work better than GCNs. We validate the
proposed method on standard random graph models such as stochastic block models
(SBM) and certain community structure based graphs where LCNs outperform GCNs
and learn more intuitive embeddings. We also perform extensive binary and
multi-class classification experiments on real world datasets to demonstrate
LCN's effectiveness. In addition to simple graphs, we also demonstrate the use
of LCNs on hyper-graphs by identifying settings where they are expected to work
better than GCNs."
397,"['stat.ML', 'cs.LG']",Analysis of Network Lasso for Semi-Supervised Regression,"We apply network Lasso to semi-supervised regression problems involving
network structured data. This approach lends quite naturally to highly scalable
learning algorithms in the form of message passing over an empirical graph
which represents the network structure of the data. By using a simple
non-parametric regression model, which is motivated by a clustering hypothesis,
we provide an analysis of the estimation error incurred by network Lasso. This
analysis reveals conditions on the the network structure and the available
training data which guarantee network Lasso to be accurate. Remarkably, the
accuracy of network Lasso is related to the existence of sufficiently large
network flows over the empirical graph. Thus, our analysis reveals a connection
between network Lasso and maximum flow problems."
398,"['stat.ML', 'cs.IT', 'cs.LG', 'cs.SI', 'math.IT']",Deep Graph Infomax,"We present Deep Graph Infomax (DGI), a general approach for learning node
representations within graph-structured data in an unsupervised manner. DGI
relies on maximizing mutual information between patch representations and
corresponding high-level summaries of graphs---both derived using established
graph convolutional network architectures. The learnt patch representations
summarize subgraphs centered around nodes of interest, and can thus be reused
for downstream node-wise learning tasks. In contrast to most prior approaches
to unsupervised learning with GCNs, DGI does not rely on random walk
objectives, and is readily applicable to both transductive and inductive
learning setups. We demonstrate competitive performance on a variety of node
classification benchmarks, which at times even exceeds the performance of
supervised learning."
399,"['cs.LG', 'stat.ML']",Approximate Random Dropout,"The training phases of Deep neural network~(DNN) consumes enormous processing
time and energy. Compression techniques utilizing the sparsity of DNNs can
effectively accelerate the inference phase of DNNs. However, it can be hardly
used in the training phase because the training phase involves dense
matrix-multiplication using General Purpose Computation on Graphics Processors
(GPGPU), which endorse regular and structural data layout. In this paper, we
propose the Approximate Random Dropout that replaces the conventional random
dropout of neurons and synapses with a regular and predefined patterns to
eliminate the unnecessary computation and data access. To compensate the
potential performance loss we develop a SGD-based Search Algorithm to produce
the distribution of dropout patterns. We prove our approach is statistically
equivalent to the previous dropout method. Experiments results on MLP and LSTM
using well-known benchmarks show that the proposed Approximate Random Dropout
can reduce the training time by $20\%$-$77\%$ ($19\%$-$60\%$) when dropout rate
is $0.3$-$0.7$ on MLP (LSTM) with marginal accuracy drop."
400,"['cs.LG', 'cs.CV', 'stat.ML']",Graphical Generative Adversarial Networks,"We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model
structured data. Graphical-GAN conjoins the power of Bayesian networks on
compactly representing the dependency structures among random variables and
that of generative adversarial networks on learning expressive dependency
functions. We introduce a structured recognition model to infer the posterior
distribution of latent variables given observations. We generalize the
Expectation Propagation (EP) algorithm to learn the generative model and
recognition model jointly. Finally, we present two important instances of
Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN),
which can successfully learn the discrete and temporal structures on visual
datasets, respectively."
401,"['cs.LG', 'cs.CV', 'stat.ML']",Convolutional Neural Networks with Transformed Input based on Robust Tensor Network Decomposition,"Tensor network decomposition, originated from quantum physics to model
entangled many-particle quantum systems, turns out to be a promising
mathematical technique to efficiently represent and process big data in
parsimonious manner. In this study, we show that tensor networks can
systematically partition structured data, e.g. color images, for distributed
storage and communication in privacy-preserving manner. Leveraging the sea of
big data and metadata privacy, empirical results show that neighbouring
subtensors with implicit information stored in tensor network formats cannot be
identified for data reconstruction. This technique complements the existing
encryption and randomization techniques which store explicit data
representation at one place and highly susceptible to adversarial attacks such
as side-channel attacks and de-anonymization. Furthermore, we propose a theory
for adversarial examples that mislead convolutional neural networks to
misclassification using subspace analysis based on singular value decomposition
(SVD). The theory is extended to analyze higher-order tensors using
tensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of
different datasets to adversarial attacks, the structural similarity of
different adversarial attacks including global and localized attacks, and the
efficacy of different adversarial defenses based on input transformation. An
efficient and adaptive algorithm based on robust TT-SVD is then developed to
detect strong and static adversarial attacks."
402,"['cs.LG', 'stat.ML']",A Tensor-based Structural Health Monitoring Approach for Aeroservoelastic Systems,"Structural health monitoring is a condition-based field of study utilised to
monitor infrastructure, via sensing systems. It is therefore used in the field
of aerospace engineering to assist in monitoring the health of aerospace
structures. A difficulty however is that in structural health monitoring the
data input is usually from sensor arrays, which results in data which are
highly redundant and correlated, an area in which traditional two-way matrix
approaches have had difficulty in deconstructing and interpreting. Newer
methods involving tensor analysis allow us to analyse this multi-way structural
data in a coherent manner. In our approach, we demonstrate the usefulness of
tensor-based learning coupled with for damage detection, on a novel $N$-DoF
Lagrangian aeroservoelastic model."
403,"['cs.CV', 'cs.LG', 'stat.ML']",Stochastic Graphlet Embedding,"Graph-based methods are known to be successful in many machine learning and
pattern classification tasks. These methods consider semi-structured data as
graphs where nodes correspond to primitives (parts, interest points, segments,
etc.) and edges characterize the relationships between these primitives.
However, these non-vectorial graph data cannot be straightforwardly plugged
into off-the-shelf machine learning algorithms without a preliminary step of --
explicit/implicit -- graph vectorization and embedding. This embedding process
should be resilient to intra-class graph variations while being highly
discriminant. In this paper, we propose a novel high-order stochastic graphlet
embedding (SGE) that maps graphs into vector spaces. Our main contribution
includes a new stochastic search procedure that efficiently parses a given
graph and extracts/samples unlimitedly high-order graphlets. We consider these
graphlets, with increasing orders, to model local primitives as well as their
increasingly complex interactions. In order to build our graph representation,
we measure the distribution of these graphlets into a given graph, using
particular hash functions that efficiently assign sampled graphlets into
isomorphic sets with a very low probability of collision. When combined with
maximum margin classifiers, these graphlet-based representations have positive
impact on the performance of pattern comparison and recognition as corroborated
through extensive experiments using standard benchmark databases."
404,"['stat.ML', 'cs.LG']",Bayesian graph convolutional neural networks for semi-supervised classification,"Recently, techniques for applying convolutional neural networks to
graph-structured data have emerged. Graph convolutional neural networks (GCNNs)
have been used to address node and graph classification and matrix completion.
Although the performance has been impressive, the current implementations have
limited capability to incorporate uncertainty in the graph structure. Almost
all GCNNs process a graph as though it is a ground-truth depiction of the
relationship between nodes, but often the graphs employed in applications are
themselves derived from noisy data or modelling assumptions. Spurious edges may
be included; other edges may be missing between nodes that have very strong
relationships. In this paper we adopt a Bayesian approach, viewing the observed
graph as a realization from a parametric family of random graphs. We then
target inference of the joint posterior of the random graph parameters and the
node (or graph) labels. We present the Bayesian GCNN framework and develop an
iterative learning procedure for the case of assortative mixed-membership
stochastic block models. We present the results of experiments that demonstrate
that the Bayesian formulation can provide better performance when there are
very few labels available during the training process."
405,"['cs.LG', 'cs.AI', 'stat.ML']",Spectral Multigraph Networks for Discovering and Fusing Relationships in Molecules,"Spectral Graph Convolutional Networks (GCNs) are a generalization of
convolutional networks to learning on graph-structured data. Applications of
spectral GCNs have been successful, but limited to a few problems where the
graph is fixed, such as shape correspondence and node classification. In this
work, we address this limitation by revisiting a particular family of spectral
graph networks, Chebyshev GCNs, showing its efficacy in solving graph
classification tasks with a variable graph structure and size. Chebyshev GCNs
restrict graphs to have at most one edge between any pair of nodes. To this
end, we propose a novel multigraph network that learns from multi-relational
graphs. We model learned edges with abstract meaning and experiment with
different ways to fuse the representations extracted from annotated and learned
edges, achieving competitive results on a variety of chemical classification
benchmarks."
406,"['cs.LG', 'stat.ML']",Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction
heuristics use some score functions, such as common neighbors and Katz index,
to measure the likelihood of links. They have obtained wide practical uses due
to their simplicity, interpretability, and for some of them, scalability.
However, every heuristic has a strong assumption on when two nodes are likely
to link, which limits their effectiveness on networks where these assumptions
fail. In this regard, a more reasonable way should be learning a suitable
heuristic from a given network instead of using predefined ones. By extracting
a local subgraph around each target link, we aim to learn a function mapping
the subgraph patterns to link existence, thus automatically learning a
`heuristic' that suits the current network. In this paper, we study this
heuristic learning paradigm for link prediction. First, we develop a novel
$\gamma$-decaying heuristic theory. The theory unifies a wide range of
heuristics in a single framework, and proves that all these heuristics can be
well approximated from local subgraphs. Our results show that local subgraphs
reserve rich information related to link existence. Second, based on the
$\gamma$-decaying theory, we propose a new algorithm to learn heuristics from
local subgraphs using a graph neural network (GNN). Its experimental results
show unprecedented performance, working consistently well on a wide range of
problems."
407,"['cs.LG', 'cs.AI', 'stat.ML']",Concept-Oriented Deep Learning: Generative Concept Representations,"Generative concept representations have three major advantages over
discriminative ones: they can represent uncertainty, they support integration
of learning and reasoning, and they are good for unsupervised and
semi-supervised learning. We discuss probabilistic and generative deep
learning, which generative concept representations are based on, and the use of
variational autoencoders and generative adversarial networks for learning
generative concept representations, particularly for concepts whose data are
sequences, structured data or graphs."
408,"['cs.LG', 'cs.AI', 'stat.ML']",Learning Depthwise Separable Graph Convolution from Data Manifold,"Convolution Neural Network (CNN) has gained tremendous success in computer
vision tasks with its outstanding ability to capture the local latent features.
Recently, there has been an increasing interest in extending convolution
operations to the non-Euclidean geometry. Although various types of convolution
operations have been proposed for graphs or manifolds, their connections with
traditional convolution over grid-structured data are not well-understood. In
this paper, we show that depthwise separable convolution can be successfully
generalized for the unification of both graph-based and grid-based convolution
methods. Based on this insight we propose a novel Depthwise Separable Graph
Convolution (DSGC) approach which is compatible with the tradition convolution
network and subsumes existing convolution methods as special cases. It is
equipped with the combined strengths in model expressiveness, compatibility
(relatively small number of parameters), modularity and computational
efficiency in training. Extensive experiments show the outstanding performance
of DSGC in comparison with strong baselines on multi-domain benchmark datasets."
409,"['cs.LG', 'stat.ML']",Streaming Graph Neural Networks,"Graphs are essential representations of many real-world data such as social
networks. Recent years have witnessed the increasing efforts made to extend the
neural network models to graph-structured data. These methods, which are
usually known as the graph neural networks, have been applied to advance many
graphs related tasks such as reasoning dynamics of the physical system, graph
classification, and node classification. Most of the existing graph neural
network models have been designed for static graphs, while many real-world
graphs are inherently dynamic. For example, social networks are naturally
evolving as new users joining and new relations being created. Current graph
neural network models cannot utilize the dynamic information in dynamic graphs.
However, the dynamic information has been proven to enhance the performance of
many graph analytic tasks such as community detection and link prediction.
Hence, it is necessary to design dedicated graph neural networks for dynamic
graphs. In this paper, we propose DGNN, a new {\bf D}ynamic {\bf G}raph {\bf
N}eural {\bf N}etwork model, which can model the dynamic information as the
graph evolving. In particular, the proposed framework can keep updating node
information by capturing the sequential information of edges (interactions),
the time intervals between edges and information propagation coherently.
Experimental results on various dynamic graphs demonstrate the effectiveness of
the proposed framework."
410,"['cs.LG', 'stat.ML']",Effective Learning of Probabilistic Models for Clinical Predictions from Longitudinal Data,"With the expeditious advancement of information technologies, health-related
data presented unprecedented potentials for medical and health discoveries but
at the same time significant challenges for machine learning techniques both in
terms of size and complexity. Those challenges include: the structured data
with various storage formats and value types caused by heterogeneous data
sources; the uncertainty widely existing in every aspect of medical diagnosis
and treatments; the high dimensionality of the feature space; the longitudinal
medical records data with irregular intervals between adjacent observations;
the richness of relations existing among objects with similar genetic factors,
location or socio-demographic background. This thesis aims to develop advanced
Statistical Relational Learning approaches in order to effectively exploit such
health-related data and facilitate the discoveries in medical research. It
presents the work on cost-sensitive statistical relational learning for mining
structured imbalanced data, the first continuous-time probabilistic logic model
for predicting sequential events from longitudinal structured data as well as
hybrid probabilistic relational models for learning from heterogeneous
structured data. It also demonstrates the outstanding performance of these
proposed models as well as other state of the art machine learning models when
applied to medical research problems and other real-world large-scale systems,
reveals the great potential of statistical relational learning for exploring
the structured health-related data to facilitate medical research."
411,"['stat.ML', 'cs.LG', 'math.DS', 'math.FA', '37N99, 46E22, 47B32']",Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators,"The development of a metric for structural data is a long-term problem in
pattern recognition and machine learning. In this paper, we develop a general
metric for comparing nonlinear dynamical systems that is defined with
Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric
includes the existing fundamental metrics for dynamical systems, which are
basically defined with principal angles between some appropriately-chosen
subspaces, as its special cases. We also describe the estimation of our metric
from finite data. We empirically illustrate our metric with an example of
rotation dynamics in a unit disk in a complex plane, and evaluate the
performance with real-world time-series data."
412,['cs.LG'],CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters,"The rise of graph-structured data such as social networks, regulatory
networks, citation graphs, and functional brain networks, in combination with
resounding success of deep learning in various applications, has brought the
interest in generalizing deep learning models to non-Euclidean domains. In this
paper, we introduce a new spectral domain convolutional architecture for deep
learning on graphs. The core ingredient of our model is a new class of
parametric rational complex functions (Cayley polynomials) allowing to
efficiently compute spectral filters on graphs that specialize on frequency
bands of interest. Our model generates rich spectral filters that are localized
in space, scales linearly with the size of the input data for
sparsely-connected graphs, and can handle different constructions of Laplacian
operators. Extensive experimental results show the superior performance of our
approach, in comparison to other spectral domain convolutional architectures,
on spectral image classification, community detection, vertex classification
and matrix completion tasks."
413,"['stat.ML', 'cs.LG']",Bootstrapping Graph Convolutional Neural Networks for Autism Spectrum Disorder Classification,"Using predictive models to identify patterns that can act as biomarkers for
different neuropathoglogical conditions is becoming highly prevalent. In this
paper, we consider the problem of Autism Spectrum Disorder (ASD) classification
where previous work has shown that it can be beneficial to incorporate a wide
variety of meta features, such as socio-cultural traits, into predictive
modeling. A graph-based approach naturally suits these scenarios, where a
contextual graph captures traits that characterize a population, while the
specific brain activity patterns are utilized as a multivariate signal at the
nodes. Graph neural networks have shown improvements in inferencing with
graph-structured data. Though the underlying graph strongly dictates the
overall performance, there exists no systematic way of choosing an appropriate
graph in practice, thus making predictive models non-robust. To address this,
we propose a bootstrapped version of graph convolutional neural networks
(G-CNNs) that utilizes an ensemble of weakly trained G-CNNs, and reduce the
sensitivity of models on the choice of graph construction. We demonstrate its
effectiveness on the challenging Autism Brain Imaging Data Exchange (ABIDE)
dataset and show that our approach improves upon recently proposed graph-based
neural networks. We also show that our method remains more robust to noisy
graphs."
414,"['stat.ML', 'cs.LG', 'math.AT']",Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams,"Algebraic topology methods have recently played an important role for
statistical analysis with complicated geometric structured data such as shapes,
linked twist maps, and material data. Among them, \textit{persistent homology}
is a well-known tool to extract robust topological features, and outputs as
\textit{persistence diagrams} (PDs). However, PDs are point multi-sets which
can not be used in machine learning algorithms for vector data. To deal with
it, an emerged approach is to use kernel methods, and an appropriate geometry
for PDs is an important factor to measure the similarity of PDs. A popular
geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein
distance is not \textit{negative definite}. Thus, it is limited to build
positive definite kernels upon the Wasserstein distance \textit{without
approximation}. In this work, we rely upon the alternative \textit{Fisher
information geometry} to propose a positive definite kernel for PDs
\textit{without approximation}, namely the Persistence Fisher (PF) kernel.
Then, we analyze eigensystem of the integral operator induced by the proposed
kernel for kernel machines. Based on that, we derive generalization error
bounds via covering numbers and Rademacher averages for kernel machines with
the PF kernel. Additionally, we show some nice properties such as stability and
infinite divisibility for the proposed kernel. Furthermore, we also propose a
linear time complexity over the number of points in PDs for an approximation of
our proposed kernel with a bounded error. Throughout experiments with many
different tasks on various benchmark datasets, we illustrate that the PF kernel
compares favorably with other baseline kernels for PDs."
415,['cs.LG'],Graph2Seq: Scalable Learning Dynamics for Graphs,"Neural networks have been shown to be an effective tool for learning
algorithms over graph-structured data. However, graph representation
techniques---that convert graphs to real-valued vectors for use with neural
networks---are still in their infancy. Recent works have proposed several
approaches (e.g., graph convolutional networks), but these methods have
difficulty scaling and generalizing to graphs with different sizes and shapes.
We present Graph2Seq, a new technique that represents vertices of graphs as
infinite time-series. By not limiting the representation to a fixed dimension,
Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq
is also reversible, allowing full recovery of the graph structure from the
sequences. By analyzing a formal computational model for graph representation,
we show that an unbounded sequence is necessary for scalability. Our
experimental results with Graph2Seq show strong generalization and new
state-of-the-art performance on a variety of graph combinatorial optimization
problems."
416,['cs.CV'],Graph Diffusion-Embedding Networks,"We present a novel graph diffusion-embedding networks (GDEN) for graph
structured data. GDEN is motivated by our closed-form formulation on
regularized feature diffusion on graph. GDEN integrates both regularized
feature diffusion and low-dimensional embedding simultaneously in a unified
network model. Moreover, based on GDEN, we can naturally deal with structured
data with multiple graph structures. Experiments on semi-supervised learning
tasks on several benchmark datasets demonstrate the better performance of the
proposed GDEN when comparing with the traditional GCN models."
417,['cs.CV'],Graph Laplacian Regularized Graph Convolutional Networks for Semi-supervised Learning,"Recently, graph convolutional network (GCN) has been widely used for
semi-supervised classification and deep feature representation on
graph-structured data. However, existing GCN generally fails to consider the
local invariance constraint in learning and representation process. That is, if
two data points Xi and Xj are close in the intrinsic geometry of the data
distribution, then their labels/representations should also be close to each
other. This is known as local invariance assumption which plays an essential
role in the development of various kinds of traditional algorithms, such as
dimensionality reduction and semi-supervised learning, in machine learning
area. To overcome this limitation, we introduce a graph Laplacian GCN (gLGCN)
approach for graph data representation and semi-supervised classification. The
proposed gLGCN model is capable of encoding both graph structure and node
features together while maintains the local invariance constraint naturally for
robust data representation and semi-supervised classification. Experiments show
the benefit of the benefits the proposed gLGCN network."
418,['cs.CV'],Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds,"Deep learning systems extensively use convolution operations to process input
data. Though convolution is clearly defined for structured data such as 2D
images or 3D volumes, this is not true for other data types such as sparse
point clouds. Previous techniques have developed approximations to convolutions
for restricted conditions. Unfortunately, their applicability is limited and
cannot be used for general point clouds. We propose an efficient and effective
method to learn convolutions for non-uniformly sampled point clouds, as they
are obtained with modern acquisition techniques. Learning is enabled by four
key novelties: first, representing the convolution kernel itself as a
multilayer perceptron; second, phrasing convolution as a Monte Carlo
integration problem, third, using this notion to combine information from
multiple samplings at different levels; and fourth using Poisson disk sampling
as a scalable means of hierarchical point cloud learning. The key idea across
all these contributions is to guarantee adequate consideration of the
underlying non-uniform sample distribution function from a Monte Carlo
perspective. To make the proposed concepts applicable to real-world tasks, we
furthermore propose an efficient implementation which significantly reduces the
GPU memory required during the training process. By employing our method in
hierarchical network architectures we can outperform most of the
state-of-the-art networks on established point cloud segmentation,
classification and normal estimation benchmarks. Furthermore, in contrast to
most existing approaches, we also demonstrate the robustness of our method with
respect to sampling variations, even when training with uniformly sampled data
only. To support the direct application of these concepts, we provide a
ready-to-use TensorFlow implementation of these layers at
https://github.com/viscom-ulm/MCCNN"
419,"['cs.LG', 'astro-ph.IM', 'stat.ML']",Graph Neural Networks for IceCube Signal Classification,"Tasks involving the analysis of geometric (graph- and manifold-structured)
data have recently gained prominence in the machine learning community, giving
birth to a rapidly developing field of geometric deep learning. In this work,
we leverage graph neural networks to improve signal detection in the IceCube
neutrino observatory. The IceCube detector array is modeled as a graph, where
vertices are sensors and edges are a learned function of the sensors' spatial
coordinates. As only a subset of IceCube's sensors is active during a given
observation, we note the adaptive nature of our GNN, wherein computation is
restricted to the input signal support. We demonstrate the effectiveness of our
GNN architecture on a task classifying IceCube events, where it outperforms
both a traditional physics-based method as well as classical 3D convolution
neural networks."
420,"['cs.CV', 'cs.CL', 'cs.LG', 'stat.ML']",Context-Dependent Diffusion Network for Visual Relationship Detection,"Visual relationship detection can bridge the gap between computer vision and
natural language for scene understanding of images. Different from pure object
recognition tasks, the relation triplets of subject-predicate-object lie on an
extreme diversity space, such as \textit{person-behind-person} and
\textit{car-behind-building}, while suffering from the problem of combinatorial
explosion. In this paper, we propose a context-dependent diffusion network
(CDDN) framework to deal with visual relationship detection. To capture the
interactions of different object instances, two types of graphs, word semantic
graph and visual scene graph, are constructed to encode global context
interdependency. The semantic graph is built through language priors to model
semantic correlations across objects, whilst the visual scene graph defines the
connections of scene objects so as to utilize the surrounding scene
information. For the graph-structured data, we design a diffusion network to
adaptively aggregate information from contexts, which can effectively learn
latent representations of visual relationships and well cater to visual
relationship detection in view of its isomorphic invariance to graphs.
Experiments on two widely-used datasets demonstrate that our proposed method is
more effective and achieves the state-of-the-art performance."
421,"['cs.LG', 'stat.ML']",Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture for Probabilistic Spatio-temporal Solar Irradiance Forecasting,"Machine Learning on graph-structured data is an important and omnipresent
task for a vast variety of applications including anomaly detection and dynamic
network analysis. In this paper, a deep generative model is introduced to
capture continuous probability densities corresponding to the nodes of an
arbitrary graph. In contrast to all learning formulations in the area of
discriminative pattern recognition, we propose a scalable generative
optimization/algorithm theoretically proved to capture distributions at the
nodes of a graph. Our model is able to generate samples from the probability
densities learned at each node. This probabilistic data generation model, i.e.
convolutional graph auto-encoder (CGAE), is devised based on the localized
first-order approximation of spectral graph convolutions, deep learning, and
the variational Bayesian inference. We apply our CGAE to a new problem, the
spatio-temporal probabilistic solar irradiance prediction. Multiple solar
radiation measurement sites in a wide area in northern states of the US are
modeled as an undirected graph. Using our proposed model, the distribution of
future irradiance given historical radiation observations is estimated for
every site/node. Numerical results on the National Solar Radiation Database
show state-of-the-art performance for probabilistic radiation prediction on
geographically distributed irradiance data in terms of reliability, sharpness,
and continuous ranked probability score."
422,"['stat.ML', 'cs.LG']",RetGK: Graph Kernels based on Return Probabilities of Random Walks,"Graph-structured data arise in wide applications, such as computer vision,
bioinformatics, and social networks. Quantifying similarities among graphs is a
fundamental problem. In this paper, we develop a framework for computing graph
kernels, based on return probabilities of random walks. The advantages of our
proposed kernels are that they can effectively exploit various node attributes,
while being scalable to large datasets. We conduct extensive graph
classification experiments to evaluate our graph kernels. The experimental
results show that our graph kernels significantly outperform existing
state-of-the-art approaches in both accuracy and computational efficiency."
423,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",Improving the Expressiveness of Deep Learning Frameworks with Recursion,"Recursive neural networks have widely been used by researchers to handle
applications with recursively or hierarchically structured data. However,
embedded control flow deep learning frameworks such as TensorFlow, Theano,
Caffe2, and MXNet fail to efficiently represent and execute such neural
networks, due to lack of support for recursion. In this paper, we add recursion
to the programming model of existing frameworks by complementing their design
with recursive execution of dataflow graphs as well as additional APIs for
recursive definitions. Unlike iterative implementations, which can only
understand the topological index of each node in recursive data structures, our
recursive implementation is able to exploit the recursive relationships between
nodes for efficient execution based on parallel computation. We present an
implementation on TensorFlow and evaluation results with various recursive
neural network models, showing that our recursive implementation not only
conveys the recursive nature of recursive neural networks better than other
implementations, but also uses given resources more effectively to reduce
training and inference time."
424,['cs.LG'],Feature Selection: A Data Perspective,"Feature selection, as a data preprocessing strategy, has been proven to be
effective and efficient in preparing data (especially high-dimensional data)
for various data mining and machine learning problems. The objectives of
feature selection include: building simpler and more comprehensible models,
improving data mining performance, and preparing clean, understandable data.
The recent proliferation of big data has presented some substantial challenges
and opportunities to feature selection. In this survey, we provide a
comprehensive and structured overview of recent advances in feature selection
research. Motivated by current challenges and opportunities in the era of big
data, we revisit feature selection research from a data perspective and review
representative feature selection algorithms for conventional data, structured
data, heterogeneous data and streaming data. Methodologically, to emphasize the
differences and similarities of most existing feature selection algorithms for
conventional data, we categorize them into four main groups: similarity based,
information theoretical based, sparse learning based and statistical based
methods. To facilitate and promote the research in this community, we also
present an open-source feature selection repository that consists of most of
the popular feature selection algorithms
(\url{http://featureselection.asu.edu/}). Also, we use it as an example to show
how to evaluate feature selection algorithms. At the end of the survey, we
present a discussion about some open problems and challenges that require more
attention in future research."
425,"['stat.ML', 'cs.LG', 'eess.SP']",Topology and Prediction Focused Research on Graph Convolutional Neural Networks,"Important advances have been made using convolutional neural network (CNN)
approaches to solve complicated problems in areas that rely on grid structured
data such as image processing and object classification. Recently, research on
graph convolutional neural networks (GCNN) has increased dramatically as
researchers try to replicate the success of CNN for graph structured data.
Unfortunately, traditional CNN methods are not readily transferable to GCNN,
given the irregularity and geometric complexity of graphs. The emerging field
of GCNN is further complicated by research papers that differ greatly in their
scope, detail, and level of academic sophistication needed by the reader.
  The present paper provides a review of some basic properties of GCNN. As a
guide to the interested reader, recent examples of GCNN research are then
grouped according to techniques that attempt to uncover the underlying topology
of the graph model and those that seek to generalize traditional CNN methods on
graph data to improve prediction of class membership. Discrete Signal
Processing on Graphs (DSPg) is used as a theoretical framework to better
understand some of the performance gains and limitations of these recent GCNN
approaches. A brief discussion of Topology Adaptive Graph Convolutional
Networks (TAGCN) is presented as an approach motivated by DSPg and future
research directions using this approach are briefly discussed."
426,"['cs.LG', 'cs.CL', 'stat.ML']",Deep EHR: Chronic Disease Prediction Using Medical Notes,"Early detection of preventable diseases is important for better disease
management, improved inter-ventions, and more efficient health-care resource
allocation. Various machine learning approacheshave been developed to utilize
information in Electronic Health Record (EHR) for this task. Majorityof
previous attempts, however, focus on structured fields and lose the vast amount
of information inthe unstructured notes. In this work we propose a general
multi-task framework for disease onsetprediction that combines both free-text
medical notes and structured information. We compareperformance of different
deep learning architectures including CNN, LSTM and hierarchical models.In
contrast to traditional text-based prediction models, our approach does not
require disease specificfeature engineering, and can handle negations and
numerical values that exist in the text. Ourresults on a cohort of about 1
million patients show that models using text outperform modelsusing just
structured data, and that models capable of using numerical values and
negations in thetext, in addition to the raw text, further improve performance.
Additionally, we compare differentvisualization methods for medical
professionals to interpret model predictions."
427,"['cs.LG', 'stat.ML']",The Logistic Network Lasso,"We apply the network Lasso to solve binary classification and clustering
problems for network-structured data. To this end, we generalize ordinary
logistic regression to non-Euclidean data with an intrinsic network structure.
The resulting ""logistic network Lasso"" amounts to solving a non-smooth convex
regularized empirical risk minimization. The risk is measured using the
logistic loss incurred over a small set of labeled nodes. For the
regularization, we propose to use the total variation of the classifier
requiring it to conform to the underlying network structure. A scalable
implementation of the learning method is obtained using an inexact variant of
the alternating direction methods of multipliers which results in a scalable
learning algorithm"
428,"['cs.LG', 'stat.ML']",L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,"We study instancewise feature importance scoring as a method for model
interpretation. Any such method yields, for each predicted instance, a vector
of importance scores associated with the feature vector. Methods based on the
Shapley score have been proposed as a fair way of computing feature
attributions of this kind, but incur an exponential complexity in the number of
features. This combinatorial explosion arises from the definition of the
Shapley value and prevents these methods from being scalable to large data sets
and complex models. We focus on settings in which the data have a graph
structure, and the contribution of features to the target variable is
well-approximated by a graph-structured factorization. In such settings, we
develop two algorithms with linear complexity for instancewise feature
importance scoring. We establish the relationship of our methods to the Shapley
value and another closely related concept known as the Myerson value from
cooperative game theory. We demonstrate on both language and image data that
our algorithms compare favorably with other methods for model interpretation."
429,"['stat.ML', 'cs.LG']",Small-Variance Asymptotics for Nonparametric Bayesian Overlapping Stochastic Blockmodels,"The latent feature relational model (LFRM) is a generative model for
graph-structured data to learn a binary vector representation for each node in
the graph. The binary vector denotes the node's membership in one or more
communities. At its core, the LFRM miller2009nonparametric is an overlapping
stochastic blockmodel, which defines the link probability between any pair of
nodes as a bilinear function of their community membership vectors. Moreover,
using a nonparametric Bayesian prior (Indian Buffet Process) enables learning
the number of communities automatically from the data. However, despite its
appealing properties, inference in LFRM remains a challenge and is typically
done via MCMC methods. This can be slow and may take a long time to converge.
In this work, we develop a small-variance asymptotics based framework for the
non-parametric Bayesian LFRM. This leads to an objective function that retains
the nonparametric Bayesian flavor of LFRM, while enabling us to design
deterministic inference algorithms for this model, that are easy to implement
(using generic or specialized optimization routines) and are fast in practice.
Our results on several benchmark datasets demonstrate that our algorithm is
competitive to methods such as MCMC, while being much faster."
430,"['cs.LG', 'stat.ML']",When Work Matters: Transforming Classical Network Structures to Graph CNN,"Numerous pattern recognition applications can be formed as learning from
graph-structured data, including social network, protein-interaction network,
the world wide web data, knowledge graph, etc. While convolutional neural
network (CNN) facilitates great advances in gridded image/video understanding
tasks, very limited attention has been devoted to transform these successful
network structures (including Inception net, Residual net, Dense net, etc.) to
establish convolutional networks on graph, due to its irregularity and
complexity geometric topologies (unordered vertices, unfixed number of adjacent
edges/vertices). In this paper, we aim to give a comprehensive analysis of when
work matters by transforming different classical network structures to graph
CNN, particularly in the basic graph recognition problem. Specifically, we
firstly review the general graph CNN methods, especially in its spectral
filtering operation on the irregular graph data. We then introduce the basic
structures of ResNet, Inception and DenseNet into graph CNN and construct these
network structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In
particular, it seeks to help graph CNNs by shedding light on how these
classical network structures work and providing guidelines for choosing
appropriate graph network frameworks. Finally, we comprehensively evaluate the
performance of these different network structures on several public graph
datasets (including social networks and bioinformatic datasets), and
demonstrate how different network structures work on graph CNN in the graph
recognition task."
431,"['cs.LG', 'stat.ML']",BayesGrad: Explaining Predictions of Graph Convolutional Networks,"Recent advances in graph convolutional networks have significantly improved
the performance of chemical predictions, raising a new research question: ""how
do we explain the predictions of graph convolutional networks?"" A possible
approach to answer this question is to visualize evidence substructures
responsible for the predictions. For chemical property prediction tasks, the
sample size of the training data is often small and/or a label imbalance
problem occurs, where a few samples belong to a single class and the majority
of samples belong to the other classes. This can lead to uncertainty related to
the learned parameters of the machine learning model. To address this
uncertainty, we propose BayesGrad, utilizing the Bayesian predictive
distribution, to define the importance of each node in an input graph, which is
computed efficiently using the dropout technique. We demonstrate that BayesGrad
successfully visualizes the substructures responsible for the label prediction
in the artificial experiment, even when the sample size is small. Furthermore,
we use a real dataset to evaluate the effectiveness of the visualization. The
basic idea of BayesGrad is not limited to graph-structured data and can be
applied to other data types."
432,"['cs.CV', 'stat.ML']",Utility in Fashion with implicit feedback,"Fashion preference is a fuzzy concept that depends on customer taste,
prevailing norms in fashion product/style, henceforth used interchangeably, and
a customer's perception of utility or fashionability, yet fashion e-retail
relies on algorithmically generated search and recommendation systems that
process structured data and images to best match customer preference. Retailers
study tastes solely as a function of what sold vs what did not, and take it to
represent customer preference. Such explicit modeling, however, belies the
underlying user preference, which is a complicated interplay of preference and
commercials such as brand, price point, promotions, other sale events, and
competitor push/marketing. It is hard to infer a notion of utility or even
customer preference by looking at sales data.
  In search and recommendation systems for fashion e-retail, customer
preference is implicitly derived by user-user similarity or item-item
similarity. In this work, we aim to derive a metric that separates the buying
preferences of users from the commercials of the merchandise (price,
promotions, etc). We extend our earlier work on explicit signals to gauge
sellability or preference with implicit signals from user behaviour."
433,"['stat.ML', 'cs.CL', 'cs.LG']",Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations,"Knowledge graphs are a versatile framework to encode richly structured data
relationships, but it can be challenging to combine these graphs with
unstructured data. Methods for retrofitting pre-trained entity representations
to the structure of a knowledge graph typically assume that entities are
embedded in a connected space and that relations imply similarity. However,
useful knowledge graphs often contain diverse entities and relations (with
potentially disjoint underlying corpora) which do not accord with these
assumptions. To overcome these limitations, we present Functional Retrofitting,
a framework that generalizes current retrofitting methods by explicitly
modeling pairwise relations. Our framework can directly incorporate a variety
of pairwise penalty functions previously developed for knowledge graph
completion. Further, it allows users to encode, learn, and extract information
about relation semantics. We present both linear and neural instantiations of
the framework. Functional Retrofitting significantly outperforms existing
retrofitting methods on complex knowledge graphs and loses no accuracy on
simpler graphs (in which relations do imply similarity). Finally, we
demonstrate the utility of the framework by predicting new drug--disease
treatment pairs in a large, complex health knowledge graph."
434,"['stat.ML', 'cs.LG']","High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient","High dimensional structured data enriched model describes groups of
observations by shared and per-group individual parameters, each with its own
structure such as sparsity or group sparsity. In this paper, we consider the
general form of data enrichment where data comes in a fixed but arbitrary
number of groups G. Any convex function, e.g., norms, can characterize the
structure of both shared and individual parameters. We propose an estimator for
high dimensional data enriched model and provide conditions under which it
consistently estimates both shared and individual parameters. We also delineate
sample complexity of the estimator and present high probability non-asymptotic
bound on estimation error of all parameters. Interestingly the sample
complexity of our estimator translates to conditions on both per-group sample
sizes and the total number of samples. We propose an iterative estimation
algorithm with linear convergence rate and supplement our theoretical analysis
with synthetic and real experimental results. Particularly, we show the
predictive power of data-enriched model along with its interpretable results in
anticancer drug sensitivity analysis."
435,"['cs.CV', 'cs.LG']",Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification,"Network biology has been successfully used to help reveal complex mechanisms
of disease, especially cancer. On the other hand, network biology requires
in-depth knowledge to construct disease-specific networks, but our current
knowledge is very limited even with the recent advances in human cancer
biology. Deep learning has shown a great potential to address the difficult
situation like this. However, deep learning technologies conventionally use
grid-like structured data, thus application of deep learning technologies to
the classification of human disease subtypes is yet to be explored. Recently,
graph based deep learning techniques have emerged, which becomes an opportunity
to leverage analyses in network biology. In this paper, we proposed a hybrid
model, which integrates two key components 1) graph convolution neural network
(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component
to learn expression patterns of cooperative gene community, and RN as a
component to learn associations between learned patterns. The proposed model is
applied to the PAM50 breast cancer subtype classification task, the standard
breast cancer subtype classification of clinical utility. In experiments of
both subtype classification and patient survival analysis, our proposed method
achieved significantly better performances than existing methods. We believe
that this work is an important starting point to realize the upcoming
personalized medicine."
436,"['cs.LG', 'stat.ML']",Anonymous Walk Embeddings,"The task of representing entire graphs has seen a surge of prominent results,
mainly due to learning convolutional neural networks (CNNs) on graph-structured
data. While CNNs demonstrate state-of-the-art performance in graph
classification task, such methods are supervised and therefore steer away from
the original problem of network representation in task-agnostic manner. Here,
we coherently propose an approach for embedding entire graphs and show that our
feature representations with SVM classifier increase classification accuracy of
CNN algorithms and traditional graph kernels. For this we describe a recently
discovered graph object, anonymous walk, on which we design task-independent
algorithms for learning graph representations in explicit and distributed way.
Overall, our work represents a new scalable unsupervised learning of
state-of-the-art representations of entire graphs."
437,"['cs.LG', 'cs.CR', 'cs.SI', 'stat.ML']",Adversarial Attack on Graph Structured Data,"Deep learning on graph structures has shown exciting results in various
applications. However, few attentions have been paid to the robustness of such
models, in contrast to numerous research work for image or text adversarial
attack and defense. In this paper, we focus on the adversarial attacks that
fool the model by modifying the combinatorial structure of data. We first
propose a reinforcement learning based attack method that learns the
generalizable attack policy, while only requiring prediction labels from the
target classifier. Also, variants of genetic algorithms and gradient methods
are presented in the scenario where prediction confidence or gradients are
available. We use both synthetic and real-world data to show that, a family of
Graph Neural Network models are vulnerable to these attacks, in both
graph-level and node-level classification tasks. We also show such attacks can
be used to diagnose the learned classifiers."
438,"['cs.LG', 'cs.AI', 'stat.ML']",Dual-Primal Graph Convolutional Networks,"In recent years, there has been a surge of interest in developing deep
learning methods for non-Euclidean structured data such as graphs. In this
paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture
that alternates convolution-like operations on the graph and its dual. Our
approach allows to learn both vertex- and edge features and generalizes the
previous graph attention (GAT) model. We provide extensive experimental
validation showing state-of-the-art results on a variety of tasks tested on
established graph benchmarks, including CORA and Citeseer citation networks as
well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender
systems."
439,"['stat.ML', 'cs.LG']",Learning Tree Distributions by Hidden Markov Models,"Hidden tree Markov models allow learning distributions for tree structured
data while being interpretable as nondeterministic automata. We provide a
concise summary of the main approaches in literature, focusing in particular on
the causality assumptions introduced by the choice of a specific tree visit
direction. We will then sketch a novel non-parametric generalization of the
bottom-up hidden tree Markov model with its interpretation as a
nondeterministic tree automaton with infinite states."
440,"['stat.ML', 'cs.LG']",MolGAN: An implicit generative model for small molecular graphs,"Deep generative models for graph-structured data offer a new angle on the
problem of chemical synthesis: by optimizing differentiable models that
directly generate molecular graphs, it is possible to side-step expensive
search procedures in the discrete and vast space of chemical structures. We
introduce MolGAN, an implicit, likelihood-free generative model for small
molecular graphs that circumvents the need for expensive graph matching
procedures or node ordering heuristics of previous likelihood-based methods.
Our method adapts generative adversarial networks (GANs) to operate directly on
graph-structured data. We combine our approach with a reinforcement learning
objective to encourage the generation of molecules with specific desired
chemical properties. In experiments on the QM9 chemical database, we
demonstrate that our model is capable of generating close to 100% valid
compounds. MolGAN compares favorably both to recent proposals that use
string-based (SMILES) representations of molecules and to a likelihood-based
method that directly generates graphs, albeit being susceptible to mode
collapse."
441,"['stat.ML', 'cs.LG']",On The Complexity of Sparse Label Propagation,"This paper investigates the computational complexity of sparse label
propagation which has been proposed recently for processing network structured
data. Sparse label propagation amounts to a convex optimization problem and
might be considered as an extension of basis pursuit from sparse vectors to
network structured datasets. Using a standard first-order oracle model, we
characterize the number of iterations for sparse label propagation to achieve a
prescribed accuracy. In particular, we derive an upper bound on the number of
iterations required to achieve a certain accuracy and show that this upper
bound is sharp for datasets having a chain structure (e.g., time series)."
442,['cs.CV'],Superpixel-guided Two-view Deterministic Geometric Model Fitting,"Geometric model fitting is a fundamental research topic in computer vision
and it aims to fit and segment multiple-structure data. In this paper, we
propose a novel superpixel-guided two-view geometric model fitting method
(called SDF), which can obtain reliable and consistent results for real images.
Specifically, SDF includes three main parts: a deterministic sampling
algorithm, a model hypothesis updating strategy and a novel model selection
algorithm. The proposed deterministic sampling algorithm generates a set of
initial model hypotheses according to the prior information of superpixels.
Then the proposed updating strategy further improves the quality of model
hypotheses. After that, by analyzing the properties of the updated model
hypotheses, the proposed model selection algorithm extends the conventional
""fit-and-remove"" framework to estimate model instances in multiple-structure
data. The three parts are tightly coupled to boost the performance of SDF in
both speed and accuracy, and SDF has the deterministic nature. Experimental
results show that the proposed SDF has significant advantages over several
state-of-the-art fitting methods when it is applied to real images with
single-structure and multiple-structure data."
443,"['cs.LG', 'stat.ML']",Multi Layered-Parallel Graph Convolutional Network (ML-PGCN) for Disease Prediction,"Structural data from Electronic Health Records as complementary information
to imaging data for disease prediction. We incorporate novel weighting layer
into the Graph Convolutional Networks, which weights every element of
structural data by exploring its relation to the underlying disease. We
demonstrate the superiority of our developed technique in terms of
computational speed and obtained encouraging results where our method
outperforms the state-of-the-art methods when applied to two publicly available
datasets ABIDE and Chest X-ray in terms of relative performance for the
accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC
curve by 4.96 % and 10.36 % respectively. Additionally, the model is
lightweight, fast and easily trainable."
444,"['cs.LG', 'stat.ML']",Residual Gated Graph ConvNets,"Graph-structured data such as social networks, functional brain networks,
gene regulatory networks, communications networks have brought the interest in
generalizing deep learning techniques to graph domains. In this paper, we are
interested to design neural networks for graphs with variable length in order
to solve learning problems such as vertex classification, graph classification,
graph regression, and graph generative tasks. Most existing works have focused
on recurrent neural networks (RNNs) to learn meaningful representations of
graphs, and more recently new convolutional neural networks (ConvNets) have
been introduced. In this work, we want to compare rigorously these two
fundamental families of architectures to solve graph learning tasks. We review
existing graph RNN and ConvNet architectures, and propose natural extension of
LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of
analytically controlled experiments on two basic graph problems, i.e. subgraph
matching and graph clustering, to test the different architectures. Numerical
results show that the proposed graph ConvNets are 3-17% more accurate and
1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than
variational (non-learning) techniques. Finally, the most effective graph
ConvNet architecture uses gated edges and residuality. Residuality plays an
essential role to learn multi-layer architectures as they provide a 10% gain of
performance."
445,['stat.ML'],A new class of metrics for learning on real-valued and structured data,"We propose a new class of metrics on sets, vectors, and functions that can be
used in various stages of data mining, including exploratory data analysis,
learning, and result interpretation. These new distance functions unify and
generalize some of the popular metrics, such as the Jaccard and bag distances
on sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance
on integrable functions. We prove that the new metrics are complete and show
useful relationships with $f$-divergences for probability distributions. To
further extend our approach to structured objects such as concept hierarchies
and ontologies, we introduce information-theoretic metrics on directed acyclic
graphs drawn according to a fixed probability distribution. We conduct
empirical investigation to demonstrate intuitive interpretation of the new
metrics and their effectiveness on real-valued, high-dimensional, and
structured data. Extensive comparative evaluation demonstrates that the new
metrics outperformed multiple similarity and dissimilarity functions
traditionally used in data mining, including the Minkowski family, the
fractional $L^p$ family, two $f$-divergences, cosine distance, and two
correlation coefficients. Finally, we argue that the new class of metrics is
particularly appropriate for rapid processing of high-dimensional and
structured data in distance-based learning."
446,['cs.CV'],FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis,"Convolutional neural networks (CNNs) have massively impacted visual
recognition in 2D images, and are now ubiquitous in state-of-the-art
approaches. CNNs do not easily extend, however, to data that are not
represented by regular grids, such as 3D shape meshes or other graph-structured
data, to which traditional local convolution operators do not directly apply.
To address this problem, we propose a novel graph-convolution operator to
establish correspondences between filter weights and graph neighborhoods with
arbitrary connectivity. The key novelty of our approach is that these
correspondences are dynamically computed from features learned by the network,
rather than relying on predefined static coordinates over the graph as in
previous work. We obtain excellent experimental results that significantly
improve over previous state-of-the-art shape correspondence results. This shows
that our approach can learn effective shape representations from raw input
coordinates, without relying on shape descriptors."
447,"['stat.ML', 'cs.LG', 'q-bio.BM']",Visualizing Convolutional Neural Network Protein-Ligand Scoring,"Protein-ligand scoring is an important step in a structure-based drug design
pipeline. Selecting a correct binding pose and predicting the binding affinity
of a protein-ligand complex enables effective virtual screening. Machine
learning techniques can make use of the increasing amounts of structural data
that are becoming publicly available. Convolutional neural network (CNN)
scoring functions in particular have shown promise in pose selection and
affinity prediction for protein-ligand complexes. Neural networks are known for
being difficult to interpret. Understanding the decisions of a particular
network can help tune parameters and training data to maximize performance.
Visualization of neural networks helps decompose complex scoring functions into
pictures that are more easily parsed by humans. Here we present three methods
for visualizing how individual protein-ligand complexes are interpreted by 3D
convolutional neural networks. We also present a visualization of the
convolutional filters and their weights. We describe how the intuition provided
by these visualizations aids in network design."
448,"['cs.LG', 'eess.SP', 'stat.ML']",MIMO Graph Filters for Convolutional Neural Networks,"Superior performance and ease of implementation have fostered the adoption of
Convolutional Neural Networks (CNNs) for a wide array of inference and
reconstruction tasks. CNNs implement three basic blocks: convolution, pooling
and pointwise nonlinearity. Since the two first operations are well-defined
only on regular-structured data such as audio or images, application of CNNs to
contemporary datasets where the information is defined in irregular domains is
challenging. This paper investigates CNNs architectures to operate on signals
whose support can be modeled using a graph. Architectures that replace the
regular convolution with a so-called linear shift-invariant graph filter have
been recently proposed. This paper goes one step further and, under the
framework of multiple-input multiple-output (MIMO) graph filters, imposes
additional structure on the adopted graph filters, to obtain three new (more
parsimonious) architectures. The proposed architectures result in a lower
number of model parameters, reducing the computational complexity, facilitating
the training, and mitigating the risk of overfitting. Simulations show that the
proposed simpler architectures achieve similar performance as more complex
models."
449,"['stat.ML', 'cs.LG']",Stochastic Training of Graph Convolutional Networks with Variance Reduction,"Graph convolutional networks (GCNs) are powerful deep neural networks for
graph-structured data. However, GCN computes the representation of a node
recursively from its neighbors, making the receptive field size grow
exponentially with the number of layers. Previous attempts on reducing the
receptive field size by subsampling neighbors do not have a convergence
guarantee, and their receptive field size per node is still in the order of
hundreds. In this paper, we develop control variate based algorithms which
allow sampling an arbitrarily small neighbor size. Furthermore, we prove new
theoretical guarantee for our algorithms to converge to a local optimum of GCN.
Empirical results show that our algorithms enjoy a similar convergence with the
exact algorithm using only two neighbors per node. The runtime of our
algorithms on a large Reddit dataset is only one seventh of previous neighbor
sampling algorithms."
450,['cs.CV'],Graph Kernels based on High Order Graphlet Parsing and Hashing,"Graph-based methods are known to be successful in many machine learning and
pattern classification tasks. These methods consider semi-structured data as
graphs where nodes correspond to primitives (parts, interest points, segments,
etc.) and edges characterize the relationships between these primitives.
However, these non-vectorial graph data cannot be straightforwardly plugged
into off-the-shelf machine learning algorithms without a preliminary step of --
explicit/implicit -- graph vectorization and embedding. This embedding process
should be resilient to intra-class graph variations while being highly
discriminant. In this paper, we propose a novel high-order stochastic graphlet
embedding (SGE) that maps graphs into vector spaces. Our main contribution
includes a new stochastic search procedure that efficiently parses a given
graph and extracts/samples unlimitedly high-order graphlets. We consider these
graphlets, with increasing orders, to model local primitives as well as their
increasingly complex interactions. In order to build our graph representation,
we measure the distribution of these graphlets into a given graph, using
particular hash functions that efficiently assign sampled graphlets into
isomorphic sets with a very low probability of collision. When combined with
maximum margin classifiers, these graphlet-based representations have positive
impact on the performance of pattern comparison and recognition as corroborated
through extensive experiments using standard benchmark databases."
451,"['cs.LG', 'stat.ML']",Matching Convolutional Neural Networks without Priors about Data,"We propose an extension of Convolutional Neural Networks (CNNs) to
graph-structured data, including strided convolutions and data augmentation on
graphs.
  Our method matches the accuracy of state-of-the-art CNNs when applied on
images, without any prior about their 2D regular structure.
  On fMRI data, we obtain a significant gain in accuracy compared with existing
graph-based alternatives."
452,"['cs.LG', 'cs.SI', 'stat.ML']",N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification,"Graph Convolutional Networks (GCNs) have shown significant improvements in
semi-supervised learning on graph-structured data. Concurrently, unsupervised
learning of graph embeddings has benefited from the information contained in
random walks. In this paper, we propose a model: Network of GCNs (N-GCN), which
marries these two lines of work. At its core, N-GCN trains multiple instances
of GCNs over node pairs discovered at different distances in random walks, and
learns a combination of the instance outputs which optimizes the classification
objective. Our experiments show that our proposed N-GCN model improves
state-of-the-art baselines on all of the challenging node classification tasks
we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method
has other desirable properties, including generalization to recently proposed
semi-supervised learning methods such as GraphSAGE, allowing us to propose
N-SAGE, and resilience to adversarial input perturbations."
453,"['cs.LG', 'cs.CL']",Syntax-Directed Variational Autoencoder for Structured Data,"Deep generative models have been enjoying success in modeling continuous
data. However it remains challenging to capture the representations for
discrete structures with formal grammars and semantics, e.g., computer programs
and molecular structures. How to generate both syntactically and semantically
correct data still remains largely an open problem. Inspired by the theory of
compiler where the syntax and semantics check is done via syntax-directed
translation (SDT), we propose a novel syntax-directed variational autoencoder
(SD-VAE) by introducing stochastic lazy attributes. This approach converts the
offline SDT check into on-the-fly generated guidance for constraining the
decoder. Comparing to the state-of-the-art methods, our approach enforces
constraints on the output space so that the output will be not only
syntactically valid, but also semantically reasonable. We evaluate the proposed
model with applications in programming language and molecules, including
reconstruction and program/molecule optimization. The results demonstrate the
effectiveness in incorporating syntactic and semantic constraints in discrete
generative models, which is significantly better than current state-of-the-art
approaches."
454,"['cs.CV', 'cs.LG']",A Spatial Mapping Algorithm with Applications in Deep Learning-Based Structure Classification,"Convolutional Neural Network (CNN)-based machine learning systems have made
breakthroughs in feature extraction and image recognition tasks in two
dimensions (2D). Although there is significant ongoing work to apply CNN
technology to domains involving complex 3D data, the success of such efforts
has been constrained, in part, by limitations in data representation
techniques. Most current approaches rely upon low-resolution 3D models,
strategic limitation of scope in the 3D space, or the application of lossy
projection techniques to allow for the use of 2D CNNs. To address this issue,
we present a mapping algorithm that converts 3D structures to 2D and 1D data
grids by mapping a traversal of a 3D space-filling curve to the traversal of
corresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs
trained on data encoded with our method versus comparable volumetric CNNs
operating upon raw 3D data from a popular benchmarking dataset. Our experiments
demonstrate that both 2D and 1D representations of 3D data generated via our
method preserve a significant proportion of the 3D data's features in forms
learnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D
data into lower-dimensional representations allows for decreased CNN training
time cost, increased original 3D model rendering resolutions, and supports
increased numbers of data channels when compared to purely volumetric
approaches. This demonstration is accomplished in the context of a structural
biology classification task wherein we train 3D, 2D, and 1D CNNs on examples of
two homologous branches within the Ras protein family. The essential
contribution of this paper is the introduction of a dimensionality-reduction
method that may ease the application of powerful deep learning tools to domains
characterized by complex structural data."
455,"['stat.ML', 'cond-mat.dis-nn', 'cs.CV', 'cs.LG']",Natural data structure extracted from neighborhood-similarity graphs,"'Big' high-dimensional data are commonly analyzed in low-dimensions, after
performing a dimensionality-reduction step that inherently distorts the data
structure. For the same purpose, clustering methods are also often used. These
methods also introduce a bias, either by starting from the assumption of a
particular geometric form of the clusters, or by using iterative schemes to
enhance cluster contours, with uncontrollable consequences. The goal of data
analysis should, however, be to encode and detect structural data features at
all scales and densities simultaneously, without assuming a parametric form of
data point distances, or modifying them. We propose a novel approach that
directly encodes data point neighborhood similarities as a sparse graph. Our
non-iterative framework permits a transparent interpretation of data, without
altering the original data dimension and metric. Several natural and synthetic
data applications demonstrate the efficacy of our novel approach."
456,"['cs.LG', 'stat.ML']",Topology Adaptive Graph Convolutional Networks,"Spectral graph convolutional neural networks (CNNs) require approximation to
the convolution to alleviate the computational complexity, resulting in
performance loss. This paper proposes the topology adaptive graph convolutional
network (TAGCN), a novel graph convolutional network defined in the vertex
domain. We provide a systematic way to design a set of fixed-size learnable
filters to perform convolutions on graphs. The topologies of these filters are
adaptive to the topology of the graph when they scan the graph to perform
convolution. The TAGCN not only inherits the properties of convolutions in CNN
for grid-structured data, but it is also consistent with convolution as defined
in graph signal processing. Since no approximation to the convolution is
needed, TAGCN exhibits better performance than existing spectral CNNs on a
number of data sets and is also computationally simpler than other recent
methods."
457,"['stat.ML', 'cs.AI', 'cs.LG', 'cs.SI']",Graph Attention Networks,"We present graph attention networks (GATs), novel neural network
architectures that operate on graph-structured data, leveraging masked
self-attentional layers to address the shortcomings of prior methods based on
graph convolutions or their approximations. By stacking layers in which nodes
are able to attend over their neighborhoods' features, we enable (implicitly)
specifying different weights to different nodes in a neighborhood, without
requiring any kind of costly matrix operation (such as inversion) or depending
on knowing the graph structure upfront. In this way, we address several key
challenges of spectral-based graph neural networks simultaneously, and make our
model readily applicable to inductive as well as transductive problems. Our GAT
models have achieved or matched state-of-the-art results across four
established transductive and inductive graph benchmarks: the Cora, Citeseer and
Pubmed citation network datasets, as well as a protein-protein interaction
dataset (wherein test graphs remain unseen during training)."
458,['cs.CV'],Searching for Representative Modes on Hypergraphs for Robust Geometric Model Fitting,"In this paper, we propose a simple and effective {geometric} model fitting
method to fit and segment multi-structure data even in the presence of severe
outliers. We cast the task of geometric model fitting as a representative
mode-seeking problem on hypergraphs. Specifically, a hypergraph is firstly
constructed, where the vertices represent model hypotheses and the hyperedges
denote data points. The hypergraph involves higher-order similarities (instead
of pairwise similarities used on a simple graph), and it can characterize
complex relationships between model hypotheses and data points. {In addition,
we develop a hypergraph reduction technique to remove ""insignificant"" vertices
while retaining as many ""significant"" vertices as possible in the hypergraph}.
Based on the {simplified hypergraph, we then propose a novel mode-seeking
algorithm to search for representative modes within reasonable time. Finally,
the} proposed mode-seeking algorithm detects modes according to two key
elements, i.e., the weighting scores of vertices and the similarity analysis
between vertices. Overall, the proposed fitting method is able to efficiently
and effectively estimate the number and the parameters of model instances in
the data simultaneously. Experimental results demonstrate that the proposed
method achieves significant superiority over {several} state-of-the-art model
fitting methods on both synthetic data and real images."
459,['cs.CV'],Building Deep Networks on Grassmann Manifolds,"Learning representations on Grassmann manifolds is popular in quite a few
visual recognition tasks. In order to enable deep learning on Grassmann
manifolds, this paper proposes a deep network architecture by generalizing the
Euclidean network paradigm to Grassmann manifolds. In particular, we design
full rank mapping layers to transform input Grassmannian data to more desirable
ones, exploit re-orthonormalization layers to normalize the resulting matrices,
study projection pooling layers to reduce the model complexity in the
Grassmannian context, and devise projection mapping layers to respect
Grassmannian geometry and meanwhile achieve Euclidean forms for regular output
layers. To train the Grassmann networks, we exploit a stochastic gradient
descent setting on manifolds of the connection weights, and study a matrix
generalization of backpropagation to update the structured data. The
evaluations on three visual recognition tasks show that our Grassmann networks
have clear advantages over existing Grassmann learning methods, and achieve
results comparable with state-of-the-art approaches."
460,['cs.LG'],Graph Memory Networks for Molecular Activity Prediction,"Molecular activity prediction is critical in drug design. Machine learning
techniques such as kernel methods and random forests have been successful for
this task. These models require fixed-size feature vectors as input while the
molecules are variable in size and structure. As a result, fixed-size
fingerprint representation is poor in handling substructures for large
molecules. In addition, molecular activity tests, or a so-called BioAssays, are
relatively small in the number of tested molecules due to its complexity. Here
we approach the problem through deep neural networks as they are flexible in
modeling structured data such as grids, sequences and graphs. We train multiple
BioAssays using a multi-task learning framework, which combines information
from multiple sources to improve the performance of prediction, especially on
small datasets. We propose Graph Memory Network (GraphMem), a memory-augmented
neural network to model the graph structure in molecules. GraphMem consists of
a recurrent controller coupled with an external memory whose cells dynamically
interact and change through a multi-hop reasoning process. Applied to the
molecules, the dynamic interactions enable an iterative refinement of the
representation of molecular graphs with multiple bond types. GraphMem is
capable of jointly training on multiple datasets by using a specific-task query
fed to the controller as an input. We demonstrate the effectiveness of the
proposed model for separately and jointly training on more than 100K
measurements, spanning across 9 BioAssay activity tests."
461,['cs.CV'],Hierarchical Label Inference for Video Classification,"Videos are a rich source of high-dimensional structured data, with a wide
range of interacting components at varying levels of granularity. In order to
improve understanding of unconstrained internet videos, it is important to
consider the role of labels at separate levels of abstraction. In this paper,
we consider the use of the Bidirectional Inference Neural Network (BINN) for
performing graph-based inference in label space for the task of video
classification. We take advantage of the inherent hierarchy between labels at
increasing granularity. The BINN is evaluated on the first and second release
of the YouTube-8M large scale multilabel video dataset. Our results demonstrate
the effectiveness of BINN, achieving significant improvements against baseline
models."
462,"['stat.ML', 'physics.chem-ph']",SchNet: A continuous-filter convolutional neural network for modeling quantum interactions,"Deep learning has the potential to revolutionize quantum chemistry as it is
ideally suited to learn representations for structured data and speed up the
exploration of chemical space. While convolutional neural networks have proven
to be the first choice for images, audio and video data, the atoms in molecules
are not restricted to a grid. Instead, their precise locations contain
essential physical information, that would get lost if discretized. Thus, we
propose to use continuous-filter convolutional layers to be able to model local
correlations without requiring the data to lie on a grid. We apply those layers
in SchNet: a novel deep learning architecture modeling quantum interactions in
molecules. We obtain a joint model for the total energy and interatomic forces
that follows fundamental quantum-chemical principles. This includes
rotationally invariant energy predictions and a smooth, differentiable
potential energy surface. Our architecture achieves state-of-the-art
performance for benchmarks of equilibrium molecules and molecular dynamics
trajectories. Finally, we introduce a more challenging benchmark with chemical
and structural variations that suggests the path for further work."
463,"['cs.LG', 'cs.AI', 'cs.NE']",Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data,"The paper introduces the Hidden Tree Markov Network (HTN), a
neuro-probabilistic hybrid fusing the representation power of generative models
for trees with the incremental and discriminative learning capabilities of
neural networks. We put forward a modular architecture in which multiple
generative models of limited complexity are trained to learn structural feature
detectors whose outputs are then combined and integrated by neural layers at a
later stage. In this respect, the model is both deep, thanks to the unfolding
of the generative models on the input structures, as well as wide, given the
potentially large number of generative modules that can be trained in parallel.
Experimental results show that the proposed approach can outperform
state-of-the-art syntactic kernels as well as generative kernels built on the
same probabilistic model as the HTN."
464,"['cs.LG', 'cs.AI', 'stat.ML']",Hinge-Loss Markov Random Fields and Probabilistic Soft Logic,"A fundamental challenge in developing high-impact machine learning
technologies is balancing the need to model rich, structured domains with the
ability to scale to big data. Many important problem areas are both richly
structured and large scale, from social and biological networks, to knowledge
graphs and the Web, to images, video, and natural language. In this paper, we
introduce two new formalisms for modeling structured data, and show that they
can both capture rich structure and scale to big data. The first, hinge-loss
Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes different approaches to convex inference. We unite three
approaches from the randomized algorithms, probabilistic graphical models, and
fuzzy logic communities, showing that all three lead to the same inference
objective. We then define HL-MRFs by generalizing this unified objective. The
second new formalism, probabilistic soft logic (PSL), is a probabilistic
programming language that makes HL-MRFs easy to define using a syntax based on
first-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than
general-purpose convex optimization methods, because it uses message passing to
take advantage of sparse dependency structures. We then show how to learn the
parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous
discrete models, but much more scalable. Together, these algorithms enable
HL-MRFs and PSL to model rich, structured data at scales not previously
possible."
465,"['stat.ML', 'cs.DB', 'cs.IR', 'cs.LG']",Graph Convolutional Matrix Completion,"We consider matrix completion for recommender systems from the point of view
of link prediction on graphs. Interaction data such as movie ratings can be
represented by a bipartite user-item graph with labeled edges denoting observed
ratings. Building on recent progress in deep learning on graph-structured data,
we propose a graph auto-encoder framework based on differentiable message
passing on the bipartite interaction graph. Our model shows competitive
performance on standard collaborative filtering benchmarks. In settings where
complimentary feature information or structured data such as a social network
is available, our framework outperforms recent state-of-the-art methods."
466,"['cs.LG', 'stat.ML']",Graph Convolution: A High-Order and Adaptive Approach,"In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening."
467,"['stat.ML', 'cs.LG']",A New Family of Near-metrics for Universal Similarity,"We propose a family of near-metrics based on local graph diffusion to capture
similarity for a wide class of data sets. These quasi-metametrics, as their
names suggest, dispense with one or two standard axioms of metric spaces,
specifically distinguishability and symmetry, so that similarity between data
points of arbitrary type and form could be measured broadly and effectively.
The proposed near-metric family includes the forward k-step diffusion and its
reverse, typically on the graph consisting of data objects and their features.
By construction, this family of near-metrics is particularly appropriate for
categorical data, continuous data, and vector representations of images and
text extracted via deep learning approaches. We conduct extensive experiments
to evaluate the performance of this family of similarity measures and compare
and contrast with traditional measures of similarity used for each specific
application and with the ground truth when available. We show that for
structured data including categorical and continuous data, the near-metrics
corresponding to normalized forward k-step diffusion (k small) work as one of
the best performing similarity measures; for vector representations of text and
images including those extracted from deep learning, the near-metrics derived
from normalized and reverse k-step graph diffusion (k very small) exhibit
outstanding ability to distinguish data points from different classes."
468,['cs.LG'],When is Network Lasso Accurate: The Vector Case,"A recently proposed learning algorithm for massive network-structured data
sets (big data over networks) is the network Lasso (nLasso), which extends the
well- known Lasso estimator from sparse models to network-structured datasets.
Efficient implementations of the nLasso have been presented using modern convex
optimization methods. In this paper, we provide sufficient conditions on the
network structure and available label information such that nLasso accurately
learns a vector-valued graph signal (representing label information) from the
information provided by the labels of a few data points."
469,['cs.LG'],Learning Graph Representations with Embedding Propagation,"We propose Embedding Propagation (EP), an unsupervised learning framework for
graph-structured data. EP learns vector representations of graphs by passing
two types of messages between neighboring nodes. Forward messages consist of
label representations such as representations of words and other attributes
associated with the nodes. Backward messages consist of gradients that result
from aggregating the label representations and applying a reconstruction loss.
Node representations are finally computed from the representation of their
labels. With significantly fewer parameters and hyperparameters an instance of
EP is competitive with and often outperforms state of the art unsupervised and
semi-supervised learning methods on a range of benchmark data sets."
470,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",Gated Graph Sequence Neural Networks,"Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures."
471,"['cs.LG', 'stat.ML']",Sales Forecast in E-commerce using Convolutional Neural Network,"Sales forecast is an essential task in E-commerce and has a crucial impact on
making informed business decisions. It can help us to manage the workforce,
cash flow and resources such as optimizing the supply chain of manufacturers
etc. Sales forecast is a challenging problem in that sales is affected by many
factors including promotion activities, price changes, and user preferences
etc. Traditional sales forecast techniques mainly rely on historical sales data
to predict future sales and their accuracies are limited. Some more recent
learning-based methods capture more information in the model to improve the
forecast accuracy. However, these methods require case-by-case manual feature
engineering for specific commercial scenarios, which is usually a difficult,
time-consuming task and requires expert knowledge. To overcome the limitations
of existing methods, we propose a novel approach in this paper to learn
effective features automatically from the structured data using the
Convolutional Neural Network (CNN). When fed with raw log data, our approach
can automatically extract effective features from that and then forecast sales
using those extracted features. We test our method on a large real-world
dataset from CaiNiao.com and the experimental results validate the
effectiveness of our method."
472,"['cs.CV', 'cs.LG', 'cs.NE']",Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,"A number of problems can be formulated as prediction on graph-structured
data. In this work, we generalize the convolution operator from regular grids
to arbitrary graphs while avoiding the spectral domain, which allows us to
handle graphs of varying size and connectivity. To move beyond a simple
diffusion, filter weights are conditioned on the specific edge labels in the
neighborhood of a vertex. Together with the proper choice of graph coarsening,
we explore constructing deep neural networks for graph classification. In
particular, we demonstrate the generality of our formulation in point cloud
classification, where we set the new state of the art, and on a graph
classification dataset, where we outperform other deep learning approaches. The
source code is available at https://github.com/mys007/ecc"
473,['cs.CV'],A Novel data Pre-processing method for multi-dimensional and non-uniform data,"We are in the era of data analytics and data science which is on full bloom.
There is abundance of all kinds of data for example biometrics based data,
satellite images data, chip-seq data, social network data, sensor based data
etc. from a variety of sources. This data abundance is the result of the fact
that storage cost is getting cheaper day by day, so people as well as almost
all business or scientific organizations are storing more and more data. Most
of the real data is multi-dimensional, non-uniform, and big in size, such that
it requires a unique pre-processing before analyzing it. In order to make data
useful for any kind of analysis, pre-processing is a very important step. This
paper presents a unique and novel pre-processing method for multi-dimensional
and non-uniform data with the aim of making it uniform and reduced in size
without losing much of its value. We have chosen biometric signature data to
demonstrate the proposed method as it qualifies for the attributes of being
multi-dimensional, non-uniform and big in size. Biometric signature data does
not only captures the structural characteristics of a signature but also its
behavioral characteristics that are captured using a dynamic signature capture
device. These features like pen pressure, pen tilt angle, time taken to sign a
document when collected in real-time turn out to be of varying dimensions. This
feature data set along with the structural data needs to be pre-processed in
order to use it to train a machine learning based model for signature
verification purposes. We demonstrate the success of the proposed method over
other methods using experimental results for biometric signature data but the
same can be implemented for any other data with similar properties from a
different domain."
474,['cs.LG'],The All-Paths and Cycles Graph Kernel,"With the recent rise in the amount of structured data available, there has
been considerable interest in methods for machine learning with graphs. Many of
these approaches have been kernel methods, which focus on measuring the
similarity between graphs. These generally involving measuring the similarity
of structural elements such as walks or paths. Borgwardt and Kriegel proposed
the all-paths kernel but emphasized that it is NP-hard to compute and
infeasible in practice, favouring instead the shortest-path kernel. In this
paper, we introduce a new algorithm for computing the all-paths kernel which is
very efficient and enrich it further by including the simple cycles as well. We
demonstrate how it is feasible even on large datasets to compute all the paths
and simple cycles up to a moderate length. We show how to count labelled
paths/simple cycles between vertices of a graph and evaluate a labelled path
and simple cycles kernel. Extensive evaluations on a variety of graph datasets
demonstrate that the all-paths and cycles kernel has superior performance to
the shortest-path kernel and state-of-the-art performance overall."
475,"['cs.LG', 'stat.ML']",Deep Over-sampling Framework for Classifying Imbalanced Data,"Class imbalance is a challenging issue in practical classification problems
for deep learning models as well as traditional models. Traditionally
successful countermeasures such as synthetic over-sampling have had limited
success with complex, structured data handled by deep learning models. In this
paper, we propose Deep Over-sampling (DOS), a framework for extending the
synthetic over-sampling method to exploit the deep feature space acquired by a
convolutional neural network (CNN). Its key feature is an explicit, supervised
representation learning, for which the training data presents each raw input
sample with a synthetic embedding target in the deep feature space, which is
sampled from the linear subspace of in-class neighbors. We implement an
iterative process of training the CNN and updating the targets, which induces
smaller in-class variance among the embeddings, to increase the discriminative
power of the deep representation. We present an empirical study using public
benchmarks, which shows that the DOS framework not only counteracts class
imbalance better than the existing method, but also improves the performance of
the CNN in the standard, balanced settings."
476,"['stat.ML', 'cs.IR', 'cs.LG']",Collaborative Filtering with Side Information: a Gaussian Process Perspective,"We tackle the problem of collaborative filtering (CF) with side information,
through the lens of Gaussian Process (GP) regression. Driven by the idea of
using the kernel to explicitly model user-item similarities, we formulate the
GP in a way that allows the incorporation of low-rank matrix factorisation,
arriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP
generalises classical Bayesian matrix factorisation models, and goes beyond
them to give a natural and elegant method for incorporating side information,
giving enhanced predictive performance for CF problems. Moreover we show that
it is a novel model for regression, especially well-suited to grid-structured
data and problems where the dependence on covariates is close to being
separable."
477,['cs.CV'],Geometric Multi-Model Fitting with a Convex Relaxation Algorithm,"We propose a novel method to fit and segment multi-structural data via convex
relaxation. Unlike greedy methods --which maximise the number of inliers-- this
approach efficiently searches for a soft assignment of points to models by
minimising the energy of the overall classification. Our approach is similar to
state-of-the-art energy minimisation techniques which use a global energy.
However, we deal with the scaling factor (as the number of models increases) of
the original combinatorial problem by relaxing the solution. This relaxation
brings two advantages: first, by operating in the continuous domain we can
parallelize the calculations. Second, it allows for the use of different
metrics which results in a more general formulation.
  We demonstrate the versatility of our technique on two different problems of
estimating structure from images: plane extraction from RGB-D data and
homography estimation from pairs of images. In both cases, we report accurate
results on publicly available datasets, in most of the cases outperforming the
state-of-the-art."
478,"['stat.ML', 'cs.LG']",Neural Embeddings of Graphs in Hyperbolic Space,"Neural embeddings have been used with great success in Natural Language
Processing (NLP). They provide compact representations that encapsulate word
similarity and attain state-of-the-art performance in a range of linguistic
tasks. The success of neural embeddings has prompted significant amounts of
research into applications in domains other than language. One such domain is
graph-structured data, where embeddings of vertices can be learned that
encapsulate vertex similarity and improve performance on tasks including edge
prediction and vertex labelling. For both NLP and graph based tasks, embeddings
have been learned in high-dimensional Euclidean spaces. However, recent work
has shown that the appropriate isometric space for embedding complex networks
is not the flat Euclidean space, but negatively curved, hyperbolic space. We
present a new concept that exploits these recent insights and propose learning
neural embeddings of graphs in hyperbolic space. We provide experimental
evidence that embedding graphs in their natural geometry significantly improves
performance on downstream tasks for several real-world public datasets."
479,"['cs.LG', 'stat.ML']",Active Learning for Graph Embedding,"Graph embedding provides an efficient solution for graph analysis by
converting the graph into a low-dimensional space which preserves the structure
information. In contrast to the graph structure data, the i.i.d. node embedding
can be processed efficiently in terms of both time and space. Current
semi-supervised graph embedding algorithms assume the labelled nodes are given,
which may not be always true in the real world. While manually label all
training data is inapplicable, how to select the subset of training data to
label so as to maximize the graph analysis task performance is of great
importance. This motivates our proposed active graph embedding (AGE) framework,
in which we design a general active learning query strategy for any
semi-supervised graph embedding algorithm. AGE selects the most informative
nodes as the training labelled nodes based on the graphical information (i.e.,
node centrality) as well as the learnt node embedding (i.e., node
classification uncertainty and node embedding representativeness). Different
query criteria are combined with the time-sensitive parameters which shift the
focus from graph based query criteria to embedding based criteria as the
learning progresses. Experiments have been conducted on three public data sets
and the results verified the effectiveness of each component of our query
strategy and the power of combining them using time-sensitive parameters. Our
code is available online at: https://github.com/vwz/AGE."
480,['cs.CV'],Deep neural networks on graph signals for brain imaging analysis,"Brain imaging data such as EEG or MEG are high-dimensional spatiotemporal
data often degraded by complex, non-Gaussian noise. For reliable analysis of
brain imaging data, it is important to extract discriminative, low-dimensional
intrinsic representation of the recorded data. This work proposes a new method
to learn the low-dimensional representations from the noise-degraded
measurements. In particular, our work proposes a new deep neural network design
that integrates graph information such as brain connectivity with
fully-connected layers. Our work leverages efficient graph filter design using
Chebyshev polynomial and recent work on convolutional nets on graph-structured
data. Our approach exploits graph structure as the prior side information,
localized graph filter for feature extraction and neural networks for high
capacity learning. Experiments on real MEG datasets show that our approach can
extract more discriminative representations, leading to improved accuracy in a
supervised classification task."
481,"['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",A Generalization of Convolutional Neural Networks to Graph-Structured Data,"This paper introduces a generalization of Convolutional Neural Networks
(CNNs) from low-dimensional grid data, such as images, to graph-structured
data. We propose a novel spatial convolution utilizing a random walk to uncover
the relations within the input, analogous to the way the standard convolution
uses the spatial neighborhood of a pixel on the grid. The convolution has an
intuitive interpretation, is efficient and scalable and can also be used on
data with varying graph structure. Furthermore, this generalization can be
applied to many standard regression or classification problems, by learning the
the underlying graph. We empirically demonstrate the performance of the
proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular
activity data set."
482,"['cs.LG', 'stat.ML']",Dynamic Graph Convolutional Networks,"Many different classification tasks need to manage structured data, which are
usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that
the vertices/edges of each graph may change during time. Our goal is to jointly
exploit structured data and temporal information through the use of a neural
network model. To the best of our knowledge, this task has not been addressed
using these kind of architectures. For this reason, we propose two novel
approaches, which combine Long Short-Term Memory networks and Graph
Convolutional Networks to learn long short-term dependencies together with
graph structure. The quality of our methods is confirmed by the promising
results achieved."
483,['stat.ML'],Interactive Graphics for Visually Diagnosing Forest Classifiers in R,"This paper describes structuring data and constructing plots to explore
forest classification models interactively. A forest classifier is an example
of an ensemble, produced by bagging multiple trees. The process of bagging and
combining results from multiple trees, produces numerous diagnostics which,
with interactive graphics, can provide a lot of insight into class structure in
high dimensions. Various aspects are explored in this paper, to assess model
complexity, individual model contributions, variable importance and dimension
reduction, and uncertainty in prediction associated with individual
observations. The ideas are applied to the random forest algorithm, and to the
projection pursuit forest, but could be more broadly applied to other bagged
ensembles. Interactive graphics are built in R, using the ggplot2, plotly, and
shiny packages."
484,"['cs.LG', 'stat.ML']",Structural Data Recognition with Graph Model Boosting,"This paper presents a novel method for structural data recognition using a
large number of graph models. In general, prevalent methods for structural data
recognition have two shortcomings: 1) Only a single model is used to capture
structural variation. 2) Naive recognition methods are used, such as the
nearest neighbor method. In this paper, we propose strengthening the
recognition performance of these models as well as their ability to capture
structural variation. The proposed method constructs a large number of graph
models and trains decision trees using the models. This paper makes two main
contributions. The first is a novel graph model that can quickly perform
calculations, which allows us to construct several models in a feasible amount
of time. The second contribution is a novel approach to structural data
recognition: graph model boosting. Comprehensive structural variations can be
captured with a large number of graph models constructed in a boosting
framework, and a sophisticated classifier can be formed by aggregating the
decision trees. Consequently, we can carry out structural data recognition with
powerful recognition capability in the face of comprehensive structural
variation. The experiments shows that the proposed method achieves impressive
results and outperforms existing methods on datasets of IAM graph database
repository."
485,"['cs.LG', 'stat.ML']",Semi-Supervised Classification with Graph Convolutional Networks,"We present a scalable approach for semi-supervised learning on
graph-structured data that is based on an efficient variant of convolutional
neural networks which operate directly on graphs. We motivate the choice of our
convolutional architecture via a localized first-order approximation of
spectral graph convolutions. Our model scales linearly in the number of graph
edges and learns hidden layer representations that encode both local graph
structure and features of nodes. In a number of experiments on citation
networks and on a knowledge graph dataset we demonstrate that our approach
outperforms related methods by a significant margin."
486,"['cs.LG', 'stat.ML']",On Valid Optimal Assignment Kernels and Applications to Graph Classification,"The success of kernel methods has initiated the design of novel positive
semidefinite functions, in particular for structured data. A leading design
paradigm for this is the convolution kernel, which decomposes structured
objects into their parts and sums over all pairs of parts. Assignment kernels,
in contrast, are obtained from an optimal bijection between parts, which can
provide a more valid notion of similarity. In general however, optimal
assignments yield indefinite functions, which complicates their use in kernel
methods. We characterize a class of base kernels used to compare parts that
guarantees positive semidefinite optimal assignment kernels. These base kernels
give rise to hierarchies from which the optimal assignment kernels are computed
in linear time by histogram intersection. We apply these results by developing
the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high
classification accuracy on widely-used benchmark data sets improving over the
original Weisfeiler-Lehman kernel."
487,"['stat.ML', 'cs.LG', 'q-bio.BM']",Protein-Ligand Scoring with Convolutional Neural Networks,"Computational approaches to drug discovery can reduce the time and cost
associated with experimental assays and enable the screening of novel
chemotypes. Structure-based drug design methods rely on scoring functions to
rank and predict binding affinities and poses. The ever-expanding amount of
protein-ligand binding and structural data enables the use of deep machine
learning techniques for protein-ligand scoring.
  We describe convolutional neural network (CNN) scoring functions that take as
input a comprehensive 3D representation of a protein-ligand interaction. A CNN
scoring function automatically learns the key features of protein-ligand
interactions that correlate with binding. We train and optimize our CNN scoring
functions to discriminate between correct and incorrect binding poses and known
binders and non-binders. We find that our CNN scoring function outperforms the
AutoDock Vina scoring function when ranking poses both for pose prediction and
virtual screening."
488,['cs.CV'],Geometric deep learning on graphs and manifolds using mixture model CNNs,"Deep learning has achieved a remarkable performance breakthrough in several
fields, most notably in speech recognition, natural language processing, and
computer vision. In particular, convolutional neural network (CNN)
architectures currently produce state-of-the-art performance on a variety of
image analysis tasks such as object detection and recognition. Most of deep
learning research has so far focused on dealing with 1D, 2D, or 3D
Euclidean-structured data such as acoustic signals, images, or videos.
Recently, there has been an increasing interest in geometric deep learning,
attempting to generalize deep learning methods to non-Euclidean structured data
such as graphs and manifolds, with a variety of applications from the domains
of network analysis, computational social science, or computer graphics. In
this paper, we propose a unified framework allowing to generalize CNN
architectures to non-Euclidean domains (graphs and manifolds) and learn local,
stationary, and compositional task-specific features. We show that various
non-Euclidean CNN methods previously proposed in the literature can be
considered as particular instances of our framework. We test the proposed
method on standard tasks from the realms of image-, graph- and 3D shape
analysis and show that it consistently outperforms previous approaches."
489,"['stat.ML', 'cs.LG']",Variational Graph Auto-Encoders,"We introduce the variational graph auto-encoder (VGAE), a framework for
unsupervised learning on graph-structured data based on the variational
auto-encoder (VAE). This model makes use of latent variables and is capable of
learning interpretable latent representations for undirected graphs. We
demonstrate this model using a graph convolutional network (GCN) encoder and a
simple inner product decoder. Our model achieves competitive results on a link
prediction task in citation networks. In contrast to most existing models for
unsupervised learning on graph-structured data and link prediction, our model
can naturally incorporate node features, which significantly improves
predictive performance on a number of benchmark datasets."
490,['cs.CV'],A reliable order-statistics-based approximate nearest neighbor search algorithm,"We propose a new algorithm for fast approximate nearest neighbor search based
on the properties of ordered vectors. Data vectors are classified based on the
index and sign of their largest components, thereby partitioning the space in a
number of cones centered in the origin. The query is itself classified, and the
search starts from the selected cone and proceeds to neighboring ones. Overall,
the proposed algorithm corresponds to locality sensitive hashing in the space
of directions, with hashing based on the order of components. Thanks to the
statistical features emerging through ordering, it deals very well with the
challenging case of unstructured data, and is a valuable building block for
more complex techniques dealing with structured data. Experiments on both
simulated and real-world data prove the proposed algorithm to provide a
state-of-the-art performance."
491,['cs.CV'],The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition,"Current approaches for fine-grained recognition do the following: First,
recruit experts to annotate a dataset of images, optionally also collecting
more structured data in the form of part annotations and bounding boxes.
Second, train a model utilizing this data. Toward the goal of solving
fine-grained recognition, we introduce an alternative approach, leveraging
free, noisy data from the web and simple, generic methods of recognition. This
approach has benefits in both performance and scalability. We demonstrate its
efficacy on four fine-grained datasets, greatly exceeding existing state of the
art without the manual collection of even a single label, and furthermore show
first results at scaling to more than 10,000 fine-grained categories.
Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on
Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using
their annotated training sets. We compare our approach to an active learning
approach for expanding fine-grained datasets."
492,['cs.LG'],Similarity Learning for Time Series Classification,"Multivariate time series naturally exist in many fields, like energy,
bioinformatics, signal processing, and finance. Most of these applications need
to be able to compare these structured data. In this context, dynamic time
warping (DTW) is probably the most common comparison measure. However, not much
research effort has been put into improving it by learning. In this paper, we
propose a novel method for learning similarities based on DTW, in order to
improve time series classification. Making use of the uniform stability
framework, we provide the first theoretical guarantees in the form of a
generalization bound for linear classification. The experimental study shows
that the proposed approach is efficient, while yielding sparse classifiers."
493,"['stat.ML', 'stat.ME']",Post Selection Inference with Kernels,"We propose a novel kernel based post selection inference (PSI) algorithm,
which can not only handle non-linearity in data but also structured output such
as multi-dimensional and multi-label outputs. Specifically, we develop a PSI
algorithm for independence measures, and propose the Hilbert-Schmidt
Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the
proposed algorithm is that it can handle non-linearity and/or structured data
through kernels. Namely, the proposed algorithm can be used for wider range of
applications including nonlinear multi-class classification and multi-variate
regressions, while existing PSI algorithms cannot handle them. Through
synthetic experiments, we show that the proposed approach can find a set of
statistically significant features for both regression and classification
problems. Moreover, we apply the hsicInf algorithm to a real-world data, and
show that hsicInf can successfully identify important features."
494,['cs.CV'],Superpixel-based Two-view Deterministic Fitting for Multiple-structure Data,"This paper proposes a two-view deterministic geometric model fitting method,
termed Superpixel-based Deterministic Fitting (SDF), for multiple-structure
data. SDF starts from superpixel segmentation, which effectively captures prior
information of feature appearances. The feature appearances are beneficial to
reduce the computational complexity for deterministic fitting methods. SDF also
includes two original elements, i.e., a deterministic sampling algorithm and a
novel model selection algorithm. The two algorithms are tightly coupled to
boost the performance of SDF in both speed and accuracy. Specifically, the
proposed sampling algorithm leverages the grouping cues of superpixels to
generate reliable and consistent hypotheses. The proposed model selection
algorithm further makes use of desirable properties of the generated
hypotheses, to improve the conventional fit-and-remove framework for more
efficient and effective performance. The key characteristic of SDF is that it
can efficiently and deterministically estimate the parameters of model
instances in multi-structure data. Experimental results demonstrate that the
proposed SDF shows superiority over several state-of-the-art fitting methods
for real images with single-structure and multiple-structure data."
495,['cs.CV'],Hypergraph Modelling for Geometric Model Fitting,"In this paper, we propose a novel hypergraph based method (called HF) to fit
and segment multi-structural data. The proposed HF formulates the geometric
model fitting problem as a hypergraph partition problem based on a novel
hypergraph model. In the hypergraph model, vertices represent data points and
hyperedges denote model hypotheses. The hypergraph, with large and
""data-determined"" degrees of hyperedges, can express the complex relationships
between model hypotheses and data points. In addition, we develop a robust
hypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF
can effectively and efficiently estimate the number of, and the parameters of,
model instances in multi-structural data heavily corrupted with outliers
simultaneously. Experimental results show the advantages of the proposed method
over previous methods on both synthetic data and real images."
496,['cs.LG'],Diffusion-Convolutional Neural Networks,"We present diffusion-convolutional neural networks (DCNNs), a new model for
graph-structured data. Through the introduction of a diffusion-convolution
operation, we show how diffusion-based representations can be learned from
graph-structured data and used as an effective basis for node classification.
DCNNs have several attractive qualities, including a latent representation for
graphical data that is invariant under isomorphism, as well as polynomial-time
prediction and learning that can be represented as tensor operations and
efficiently implemented on the GPU. Through several experiments with real
structured datasets, we demonstrate that DCNNs are able to outperform
probabilistic relational models and kernel-on-graph methods at relational node
classification tasks."
497,"['stat.ML', 'cs.LG']",Kernel-based Reconstruction of Graph Signals,"A number of applications in engineering, social sciences, physics, and
biology involve inference over networks. In this context, graph signals are
widely encountered as descriptors of vertex attributes or features in
graph-structured data. Estimating such signals in all vertices given noisy
observations of their values on a subset of vertices has been extensively
analyzed in the literature of signal processing on graphs (SPoG). This paper
advocates kernel regression as a framework generalizing popular SPoG modeling
and reconstruction and expanding their capabilities. Formulating signal
reconstruction as a regression task on reproducing kernel Hilbert spaces of
graph signals permeates benefits from statistical learning, offers fresh
insights, and allows for estimators to leverage richer forms of prior
information than existing alternatives. A number of SPoG notions such as
bandlimitedness, graph filters, and the graph Fourier transform are naturally
accommodated in the kernel framework. Additionally, this paper capitalizes on
the so-called representer theorem to devise simpler versions of existing
Thikhonov regularized estimators, and offers a novel probabilistic
interpretation of kernel methods on graphs based on graphical models. Motivated
by the challenges of selecting the bandwidth parameter in SPoG estimators or
the kernel map in kernel-based methods, the present paper further proposes two
multi-kernel approaches with complementary strengths. Whereas the first enables
estimation of the unknown bandwidth of bandlimited signals, the second allows
for efficient graph filter selection. Numerical tests with synthetic as well as
real data demonstrate the merits of the proposed methods relative to
state-of-the-art alternatives."
498,['cs.CV'],Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting,"In this paper, we propose a novel geometric model fitting method, called
Mode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in the
presence of severe outliers. The proposed method formulates geometric model
fitting as a mode seeking problem on a hypergraph in which vertices represent
model hypotheses and hyperedges denote data points. MSH intuitively detects
model instances by a simple and effective mode seeking algorithm. In addition
to the mode seeking algorithm, MSH includes a similarity measure between
vertices on the hypergraph and a weight-aware sampling technique. The proposed
method not only alleviates sensitivity to the data distribution, but also is
scalable to large scale problems. Experimental results further demonstrate that
the proposed method has significant superiority over the state-of-the-art
fitting methods on both synthetic data and real images."
499,['cs.CV'],Semantic Object Parsing with Graph LSTM,"By taking the semantic object parsing task as an exemplar application
scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,
which is the generalization of LSTM from sequential data or multi-dimensional
data to general graph-structured data. Particularly, instead of evenly and
fixedly dividing an image to pixels or patches in existing multi-dimensional
LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each
arbitrary-shaped superpixel as a semantically consistent node, and adaptively
construct an undirected graph for each image, where the spatial relations of
the superpixels are naturally used as edges. Constructed on such an adaptive
graph topology, the Graph LSTM is more naturally aligned with the visual
patterns in the image (e.g., object boundaries or appearance similarities) and
provides a more economical information propagation route. Furthermore, for each
optimization step over Graph LSTM, we propose to use a confidence-driven scheme
to update the hidden and memory states of nodes progressively till all nodes
are updated. In addition, for each node, the forgets gates are adaptively
learned to capture different degrees of semantic correlation with neighboring
nodes. Comprehensive evaluations on four diverse semantic object parsing
datasets well demonstrate the significant superiority of our Graph LSTM over
other state-of-the-art solutions."
